<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>LORA: LOW-RANK ADAPTATION OF LARGE LAN-GUAGE MODELS - ReadPapers</title>
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="The note of Papers">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/pagetoc.css">
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><div>ReadPapers</div></li><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="159.html"><strong aria-hidden="true">2.</strong> Seamless Human Motion Composition with Blended Positional Encodings</a></li><li class="chapter-item expanded "><a href="158.html"><strong aria-hidden="true">3.</strong> FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing</a></li><li class="chapter-item expanded "><a href="157.html"><strong aria-hidden="true">4.</strong> Fg-T2M: Fine-Grained Text-Driven Human Motion Generation via Diffusion Model</a></li><li class="chapter-item expanded "><a href="156.html"><strong aria-hidden="true">5.</strong> Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation</a></li><li class="chapter-item expanded "><a href="155.html"><strong aria-hidden="true">6.</strong> StableMoFusion: Towards Robust and Efficient Diffusion-based Motion Generation Framework</a></li><li class="chapter-item expanded "><a href="154.html"><strong aria-hidden="true">7.</strong> EMDM: Efficient Motion Diffusion Model for Fast and High-Quality Motion Generation</a></li><li class="chapter-item expanded "><a href="153.html"><strong aria-hidden="true">8.</strong> Motion Mamba: Efficient and Long Sequence Motion Generation</a></li><li class="chapter-item expanded "><a href="152.html"><strong aria-hidden="true">9.</strong> M2D2M: Multi-Motion Generation from Text with Discrete Diffusion Models</a></li><li class="chapter-item expanded "><a href="151.html"><strong aria-hidden="true">10.</strong> T2LM: Long-Term 3D Human Motion Generation from Multiple Sentences</a></li><li class="chapter-item expanded "><a href="150.html"><strong aria-hidden="true">11.</strong> AttT2M:Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism</a></li><li class="chapter-item expanded "><a href="149.html"><strong aria-hidden="true">12.</strong> BAD: Bidirectional Auto-Regressive Diffusion for Text-to-Motion Generation</a></li><li class="chapter-item expanded "><a href="148.html"><strong aria-hidden="true">13.</strong> MMM: Generative Masked Motion Model</a></li><li class="chapter-item expanded "><a href="147.html"><strong aria-hidden="true">14.</strong> Priority-Centric Human Motion Generation in Discrete Latent Space</a></li><li class="chapter-item expanded "><a href="146.html"><strong aria-hidden="true">15.</strong> AvatarGPT: All-in-One Framework for Motion Understanding, Planning, Generation and Beyond</a></li><li class="chapter-item expanded "><a href="145.html"><strong aria-hidden="true">16.</strong> MotionGPT: Human Motion as a Foreign Language</a></li><li class="chapter-item expanded "><a href="144.html"><strong aria-hidden="true">17.</strong> Action-GPT: Leveraging Large-scale Language Models for Improved and Generalized Action Generation</a></li><li class="chapter-item expanded "><a href="143.html"><strong aria-hidden="true">18.</strong> PoseGPT: Quantization-based 3D Human Motion Generation and Forecasting</a></li><li class="chapter-item expanded "><a href="142.html"><strong aria-hidden="true">19.</strong> Incorporating Physics Principles for Precise Human Motion Prediction</a></li><li class="chapter-item expanded "><a href="141.html"><strong aria-hidden="true">20.</strong> PIMNet: Physics-infused Neural Network for Human Motion Prediction</a></li><li class="chapter-item expanded "><a href="140.html"><strong aria-hidden="true">21.</strong> PhysDiff: Physics-Guided Human Motion Diffusion Model</a></li><li class="chapter-item expanded "><a href="139.html"><strong aria-hidden="true">22.</strong> NRDF: Neural Riemannian Distance Fields for Learning Articulated Pose Priors</a></li><li class="chapter-item expanded "><a href="138.html"><strong aria-hidden="true">23.</strong> Pose-NDF: Modeling Human Pose Manifolds with Neural Distance Fields</a></li><li class="chapter-item expanded "><a href="137.html"><strong aria-hidden="true">24.</strong> Geometric Neural Distance Fields for Learning Human Motion Priors</a></li><li class="chapter-item expanded "><a href="136.html"><strong aria-hidden="true">25.</strong> Character Controllers Using Motion VAEs</a></li><li class="chapter-item expanded "><a href="135.html"><strong aria-hidden="true">26.</strong> Improving Human Motion Plausibility with Body Momentum</a></li><li class="chapter-item expanded "><a href="134.html"><strong aria-hidden="true">27.</strong> MoGlow: Probabilistic and controllable motion synthesis using normalising flows</a></li><li class="chapter-item expanded "><a href="133.html"><strong aria-hidden="true">28.</strong> Modi: Unconditional motion synthesis from diverse data</a></li><li class="chapter-item expanded "><a href="132.html"><strong aria-hidden="true">29.</strong> MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model</a></li><li class="chapter-item expanded "><a href="131.html"><strong aria-hidden="true">30.</strong> A deep learning framework for character motion synthesis and editing</a></li><li class="chapter-item expanded "><a href="130.html"><strong aria-hidden="true">31.</strong> Multi-Object Sketch Animation with Grouping and Motion Trajectory Priors</a></li><li class="chapter-item expanded "><a href="129.html"><strong aria-hidden="true">32.</strong> TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos</a></li><li class="chapter-item expanded "><a href="128.html"><strong aria-hidden="true">33.</strong> X-MoGen: Unified Motion Generation across Humans and Animals</a></li><li class="chapter-item expanded "><a href="127.html"><strong aria-hidden="true">34.</strong> Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis</a></li><li class="chapter-item expanded "><a href="126.html"><strong aria-hidden="true">35.</strong> MotionShot: Adaptive Motion Transfer across Arbitrary Objects for Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="114.html"><strong aria-hidden="true">36.</strong> Drop: Dynamics responses from human motion prior and projective dynamics</a></li><li class="chapter-item expanded "><a href="112.html"><strong aria-hidden="true">37.</strong> POMP: Physics-constrainable Motion Generative Model through Phase Manifolds</a></li><li class="chapter-item expanded "><a href="111.html"><strong aria-hidden="true">38.</strong> Dreamgaussian4d: Generative 4d gaussian splatting</a></li><li class="chapter-item expanded "><a href="110.html"><strong aria-hidden="true">39.</strong> Drive Any Mesh: 4D Latent Diffusion for Mesh Deformation from Video</a></li><li class="chapter-item expanded "><a href="109.html"><strong aria-hidden="true">40.</strong> AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation</a></li><li class="chapter-item expanded "><a href="108.html"><strong aria-hidden="true">41.</strong> ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction</a></li><li class="chapter-item expanded "><a href="107.html"><strong aria-hidden="true">42.</strong> Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators</a></li><li class="chapter-item expanded "><a href="106.html"><strong aria-hidden="true">43.</strong> Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals</a></li><li class="chapter-item expanded "><a href="105.html"><strong aria-hidden="true">44.</strong> Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation</a></li><li class="chapter-item expanded "><a href="104.html"><strong aria-hidden="true">45.</strong> Generating time-consistent dynamics with discriminator-guided image diffusion models</a></li><li class="chapter-item expanded "><a href="103.html"><strong aria-hidden="true">46.</strong> GENMO:AGENeralist Model for Human MOtion</a></li><li class="chapter-item expanded "><a href="102.html"><strong aria-hidden="true">47.</strong> HGM3: HIERARCHICAL GENERATIVE MASKED MOTION MODELING WITH HARD TOKEN MINING</a></li><li class="chapter-item expanded "><a href="101.html"><strong aria-hidden="true">48.</strong> Towards Robust and Controllable Text-to-Motion via Masked Autoregressive Diffusion</a></li><li class="chapter-item expanded "><a href="100.html"><strong aria-hidden="true">49.</strong> MoCLIP: Motion-Aware Fine-Tuning and Distillation of CLIP for Human Motion Generation</a></li><li class="chapter-item expanded "><a href="99.html"><strong aria-hidden="true">50.</strong> FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance</a></li><li class="chapter-item expanded "><a href="98.html"><strong aria-hidden="true">51.</strong> VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence</a></li><li class="chapter-item expanded "><a href="97.html"><strong aria-hidden="true">52.</strong> DragAnything: Motion Control for Anything using Entity Representation</a></li><li class="chapter-item expanded "><a href="96.html"><strong aria-hidden="true">53.</strong> PhysAnimator: Physics-Guided Generative Cartoon Animation</a></li><li class="chapter-item expanded "><a href="95.html"><strong aria-hidden="true">54.</strong> SOAP: Style-Omniscient Animatable Portraits</a></li><li class="chapter-item expanded "><a href="94.html"><strong aria-hidden="true">55.</strong> Neural Discrete Representation Learning</a></li><li class="chapter-item expanded "><a href="93.html"><strong aria-hidden="true">56.</strong> TSTMotion: Training-free Scene-aware Text-to-motion Generation</a></li><li class="chapter-item expanded "><a href="92.html"><strong aria-hidden="true">57.</strong> Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis</a></li><li class="chapter-item expanded "><a href="91.html"><strong aria-hidden="true">58.</strong> A lip sync expert is all you need for speech to lip generation in the wild</a></li><li class="chapter-item expanded "><a href="90.html"><strong aria-hidden="true">59.</strong> MUSETALK: REAL-TIME HIGH QUALITY LIP SYN-CHRONIZATION WITH LATENT SPACE INPAINTING</a></li><li class="chapter-item expanded "><a href="89.html"><strong aria-hidden="true">60.</strong> LatentSync: Audio Conditioned Latent Diffusion Models for Lip Sync</a></li><li class="chapter-item expanded "><a href="88.html"><strong aria-hidden="true">61.</strong> T2m-gpt: Generating human motion from textual descriptions with discrete representations</a></li><li class="chapter-item expanded "><a href="87.html"><strong aria-hidden="true">62.</strong> Motiongpt: Finetuned llms are general-purpose motion generators</a></li><li class="chapter-item expanded "><a href="86.html"><strong aria-hidden="true">63.</strong> Guided Motion Diffusion for Controllable Human Motion Synthesis</a></li><li class="chapter-item expanded "><a href="85.html"><strong aria-hidden="true">64.</strong> OmniControl: Control Any Joint at Any Time for Human Motion Generation</a></li><li class="chapter-item expanded "><a href="84.html"><strong aria-hidden="true">65.</strong> Learning Long-form Video Prior via Generative Pre-Training</a></li><li class="chapter-item expanded "><a href="83.html"><strong aria-hidden="true">66.</strong> Instant Neural Graphics Primitives with a Multiresolution Hash Encoding</a></li><li class="chapter-item expanded "><a href="82.html"><strong aria-hidden="true">67.</strong> Magic3D: High-Resolution Text-to-3D Content Creation</a></li><li class="chapter-item expanded "><a href="81.html"><strong aria-hidden="true">68.</strong> CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers</a></li><li class="chapter-item expanded "><a href="80.html"><strong aria-hidden="true">69.</strong> One-Minute Video Generation with Test-Time Training</a></li><li class="chapter-item expanded "><a href="79.html"><strong aria-hidden="true">70.</strong> Key-Locked Rank One Editing for Text-to-Image Personalization</a></li><li class="chapter-item expanded "><a href="78.html"><strong aria-hidden="true">71.</strong> MARCHING CUBES: A HIGH RESOLUTION 3D SURFACE CONSTRUCTION ALGORITHM</a></li><li class="chapter-item expanded "><a href="77.html"><strong aria-hidden="true">72.</strong> Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation</a></li><li class="chapter-item expanded "><a href="76.html"><strong aria-hidden="true">73.</strong> NULL-text Inversion for Editing Real Images Using Guided Diffusion Models</a></li><li class="chapter-item expanded "><a href="75.html"><strong aria-hidden="true">74.</strong> simple diffusion: End-to-end diffusion for high resolution images</a></li><li class="chapter-item expanded "><a href="74.html"><strong aria-hidden="true">75.</strong> One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale</a></li><li class="chapter-item expanded "><a href="73.html"><strong aria-hidden="true">76.</strong> Scalable Diffusion Models with Transformers</a></li><li class="chapter-item expanded "><a href="72.html"><strong aria-hidden="true">77.</strong> All are Worth Words: a ViT Backbone for Score-based Diffusion Models</a></li><li class="chapter-item expanded "><a href="71.html"><strong aria-hidden="true">78.</strong> An image is worth 16x16 words: Transformers for image recognition at scale</a></li><li class="chapter-item expanded "><a href="70.html"><strong aria-hidden="true">79.</strong> eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers</a></li><li class="chapter-item expanded "><a href="69.html"><strong aria-hidden="true">80.</strong> Photorealistic text-to-image diffusion models with deep language understanding||Imagen</a></li><li class="chapter-item expanded "><a href="68.html"><strong aria-hidden="true">81.</strong> DreamFusion: Text-to-3D using 2D Diffusion</a></li><li class="chapter-item expanded "><a href="67.html"><strong aria-hidden="true">82.</strong> GLIGEN: Open-Set Grounded Text-to-Image Generation</a></li><li class="chapter-item expanded "><a href="66.html"><strong aria-hidden="true">83.</strong> Adding Conditional Control to Text-to-Image Diffusion Models</a></li><li class="chapter-item expanded "><a href="65.html"><strong aria-hidden="true">84.</strong> T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models</a></li><li class="chapter-item expanded "><a href="64.html"><strong aria-hidden="true">85.</strong> Multi-Concept Customization of Text-to-Image Diffusion</a></li><li class="chapter-item expanded "><a href="63.html"><strong aria-hidden="true">86.</strong> An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion</a></li><li class="chapter-item expanded "><a href="62.html"><strong aria-hidden="true">87.</strong> DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</a></li><li class="chapter-item expanded "><a href="61.html"><strong aria-hidden="true">88.</strong> VisorGPT: Learning Visual Prior via Generative Pre-Training</a></li><li class="chapter-item expanded "><a href="60.html"><strong aria-hidden="true">89.</strong> NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation</a></li><li class="chapter-item expanded "><a href="59.html"><strong aria-hidden="true">90.</strong> AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning</a></li><li class="chapter-item expanded "><a href="58.html"><strong aria-hidden="true">91.</strong> ModelScope Text-to-Video Technical Report</a></li><li class="chapter-item expanded "><a href="57.html"><strong aria-hidden="true">92.</strong> Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="56.html"><strong aria-hidden="true">93.</strong> Make-A-Video: Text-to-Video Generation without Text-Video Data</a></li><li class="chapter-item expanded "><a href="55.html"><strong aria-hidden="true">94.</strong> Video Diffusion Models</a></li><li class="chapter-item expanded "><a href="54.html"><strong aria-hidden="true">95.</strong> Learning Transferable Visual Models From Natural Language Supervision</a></li><li class="chapter-item expanded "><a href="53.html"><strong aria-hidden="true">96.</strong> Implicit Warping for Animation with Image Sets</a></li><li class="chapter-item expanded "><a href="52.html"><strong aria-hidden="true">97.</strong> Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models</a></li><li class="chapter-item expanded "><a href="51.html"><strong aria-hidden="true">98.</strong> Motion-Conditioned Diffusion Model for Controllable Video Synthesis</a></li><li class="chapter-item expanded "><a href="50.html"><strong aria-hidden="true">99.</strong> Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets</a></li><li class="chapter-item expanded "><a href="49.html"><strong aria-hidden="true">100.</strong> UniAnimate: Taming Unified Video Diffusion Models for Consistent Human Image Animation</a></li><li class="chapter-item expanded "><a href="48.html"><strong aria-hidden="true">101.</strong> Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models</a></li><li class="chapter-item expanded "><a href="47.html"><strong aria-hidden="true">102.</strong> Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics</a></li><li class="chapter-item expanded "><a href="46.html"><strong aria-hidden="true">103.</strong> A Recipe for Scaling up Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="45.html"><strong aria-hidden="true">104.</strong> High-Resolution Image Synthesis with Latent Diffusion Models</a></li><li class="chapter-item expanded "><a href="44.html"><strong aria-hidden="true">105.</strong> Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling</a></li><li class="chapter-item expanded "><a href="43.html"><strong aria-hidden="true">106.</strong> 数据集：HumanVid</a></li><li class="chapter-item expanded "><a href="42.html"><strong aria-hidden="true">107.</strong> HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation</a></li><li class="chapter-item expanded "><a href="41.html"><strong aria-hidden="true">108.</strong> StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation</a></li><li class="chapter-item expanded "><a href="40.html"><strong aria-hidden="true">109.</strong> 数据集：Zoo-300K</a></li><li class="chapter-item expanded "><a href="39.html"><strong aria-hidden="true">110.</strong> Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion</a></li><li class="chapter-item expanded "><a href="38.html" class="active"><strong aria-hidden="true">111.</strong> LORA: LOW-RANK ADAPTATION OF LARGE LAN-GUAGE MODELS</a></li><li class="chapter-item expanded "><a href="37.html"><strong aria-hidden="true">112.</strong> TCAN: Animating Human Images with Temporally Consistent Pose Guidance using Diffusion Models</a></li><li class="chapter-item expanded "><a href="36.html"><strong aria-hidden="true">113.</strong> GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians</a></li><li class="chapter-item expanded "><a href="35.html"><strong aria-hidden="true">114.</strong> MagicPony: Learning Articulated 3D Animals in the Wild</a></li><li class="chapter-item expanded "><a href="34.html"><strong aria-hidden="true">115.</strong> Splatter a Video: Video Gaussian Representation for Versatile Processing</a></li><li class="chapter-item expanded "><a href="33.html"><strong aria-hidden="true">116.</strong> 数据集：Dynamic Furry Animal Dataset</a></li><li class="chapter-item expanded "><a href="32.html"><strong aria-hidden="true">117.</strong> Artemis: Articulated Neural Pets with Appearance and Motion Synthesis</a></li><li class="chapter-item expanded "><a href="31.html"><strong aria-hidden="true">118.</strong> SMPLer: Taming Transformers for Monocular 3D Human Shape and Pose Estimation</a></li><li class="chapter-item expanded "><a href="30.html"><strong aria-hidden="true">119.</strong> CAT3D: Create Anything in 3D with Multi-View Diffusion Models</a></li><li class="chapter-item expanded "><a href="29.html"><strong aria-hidden="true">120.</strong> PACER+: On-Demand Pedestrian Animation Controller in Driving Scenarios</a></li><li class="chapter-item expanded "><a href="28.html"><strong aria-hidden="true">121.</strong> Humans in 4D: Reconstructing and Tracking Humans with Transformers</a></li><li class="chapter-item expanded "><a href="27.html"><strong aria-hidden="true">122.</strong> Learning Human Motion from Monocular Videos via Cross-Modal Manifold Alignment</a></li><li class="chapter-item expanded "><a href="26.html"><strong aria-hidden="true">123.</strong> PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos</a></li><li class="chapter-item expanded "><a href="25.html"><strong aria-hidden="true">124.</strong> Imagic: Text-Based Real Image Editing with Diffusion Models</a></li><li class="chapter-item expanded "><a href="24.html"><strong aria-hidden="true">125.</strong> DiffEdit: Diffusion-based semantic image editing with mask guidance</a></li><li class="chapter-item expanded "><a href="23.html"><strong aria-hidden="true">126.</strong> Dual diffusion implicit bridges for image-to-image translation</a></li><li class="chapter-item expanded "><a href="22.html"><strong aria-hidden="true">127.</strong> SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations</a></li><li class="chapter-item expanded "><a href="20.html"><strong aria-hidden="true">128.</strong> Prompt-to-Prompt Image Editing with Cross-Attention Control</a></li><li class="chapter-item expanded "><a href="19.html"><strong aria-hidden="true">129.</strong> WANDR: Intention-guided Human Motion Generation</a></li><li class="chapter-item expanded "><a href="18.html"><strong aria-hidden="true">130.</strong> TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Videos</a></li><li class="chapter-item expanded "><a href="17.html"><strong aria-hidden="true">131.</strong> 3D Gaussian Splatting for Real-Time Radiance Field Rendering</a></li><li class="chapter-item expanded "><a href="16.html"><strong aria-hidden="true">132.</strong> Decoupling Human and Camera Motion from Videos in the Wild</a></li><li class="chapter-item expanded "><a href="15.html"><strong aria-hidden="true">133.</strong> HMP: Hand Motion Priors for Pose and Shape Estimation from Video</a></li><li class="chapter-item expanded "><a href="14.html"><strong aria-hidden="true">134.</strong> HuMoR: 3D Human Motion Model for Robust Pose Estimation</a></li><li class="chapter-item expanded "><a href="13.html"><strong aria-hidden="true">135.</strong> Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video</a></li><li class="chapter-item expanded "><a href="12.html"><strong aria-hidden="true">136.</strong> Global-to-Local Modeling for Video-based 3D Human Pose and Shape Estimation</a></li><li class="chapter-item expanded "><a href="11.html"><strong aria-hidden="true">137.</strong> WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion</a></li><li class="chapter-item expanded "><a href="10.html"><strong aria-hidden="true">138.</strong> Tackling the Generative Learning Trilemma with Denoising Diffusion GANs</a></li><li class="chapter-item expanded "><a href="9.html"><strong aria-hidden="true">139.</strong> Elucidating the Design Space of Diffusion-Based Generative Models</a></li><li class="chapter-item expanded "><a href="8.html"><strong aria-hidden="true">140.</strong> SCORE-BASED GENERATIVE MODELING THROUGHSTOCHASTIC DIFFERENTIAL EQUATIONS</a></li><li class="chapter-item expanded "><a href="7.html"><strong aria-hidden="true">141.</strong> Consistency Models</a></li><li class="chapter-item expanded "><a href="6.html"><strong aria-hidden="true">142.</strong> Classifier-Free Diffusion Guidance</a></li><li class="chapter-item expanded "><a href="5.html"><strong aria-hidden="true">143.</strong> Cascaded Diffusion Models for High Fidelity Image Generation</a></li><li class="chapter-item expanded "><a href="4.html"><strong aria-hidden="true">144.</strong> LEARNING ENERGY-BASED MODELS BY DIFFUSIONRECOVERY LIKELIHOOD</a></li><li class="chapter-item expanded "><a href="3.html"><strong aria-hidden="true">145.</strong> On Distillation of Guided Diffusion Models</a></li><li class="chapter-item expanded "><a href="2.html"><strong aria-hidden="true">146.</strong> Denoising Diffusion Implicit Models</a></li><li class="chapter-item expanded "><a href="1.html"><strong aria-hidden="true">147.</strong> PROGRESSIVE DISTILLATION FOR FAST SAMPLING OF DIFFUSION MODELS</a></li><li class="chapter-item expanded "><a href="125.html"><strong aria-hidden="true">148.</strong> Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions</a></li><li class="chapter-item expanded "><a href="124.html"><strong aria-hidden="true">149.</strong> ControlVideo: Training-free Controllable Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="123.html"><strong aria-hidden="true">150.</strong> Pix2Video: Video Editing using Image Diffusion</a></li><li class="chapter-item expanded "><a href="122.html"><strong aria-hidden="true">151.</strong> Structure and Content-Guided Video Synthesis with Diffusion Models</a></li><li class="chapter-item expanded "><a href="121.html"><strong aria-hidden="true">152.</strong> MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model</a></li><li class="chapter-item expanded "><a href="120.html"><strong aria-hidden="true">153.</strong> MotionDirector: Motion Customization of Text-to-Video Diffusion Models</a></li><li class="chapter-item expanded "><a href="119.html"><strong aria-hidden="true">154.</strong> Dreamix: Video Diffusion Models are General Video Editors</a></li><li class="chapter-item expanded "><a href="118.html"><strong aria-hidden="true">155.</strong> Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="117.html"><strong aria-hidden="true">156.</strong> TokenFlow: Consistent Diffusion Features for Consistent Video Editing</a></li><li class="chapter-item expanded "><a href="116.html"><strong aria-hidden="true">157.</strong> DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing</a></li><li class="chapter-item expanded "><a href="115.html"><strong aria-hidden="true">158.</strong> Content Deformation Fields for Temporally Consistent Video Processing</a></li><li class="chapter-item expanded "><a href="113.html"><strong aria-hidden="true">159.</strong> PFNN: Phase-Functioned Neural Networks</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">ReadPapers</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/CaterpillarStudyGroup/ReadPapers" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main><div class="sidetoc"><nav class="pagetoc"></nav></div>
                        <h1 id="lora-low-rank-adaptation-of-large-lan-guage-model"><a class="header" href="#lora-low-rank-adaptation-of-large-lan-guage-model">LORA: LOW-RANK ADAPTATION OF LARGE LAN-GUAGE MODEL</a></h1>
<p>Microsoft，开源 </p>
<h2 id="核心问题是什么"><a class="header" href="#核心问题是什么">核心问题是什么?</a></h2>
<p>基于预训练的大模型进行finetune时，finetune 所需的训练时间、参数存储，Computation 的成本很高，因此重新训练所有模型参数变得不太可行。</p>
<h3 id="现有方法及存在的问题"><a class="header" href="#现有方法及存在的问题">现有方法及存在的问题</a></h3>
<h4 id="适配器层引入推理延迟"><a class="header" href="#适配器层引入推理延迟">适配器层引入推理延迟</a></h4>
<ol>
<li>
<p>适配器会引入额外的计算。<br />
这不是关键问题，因为适配器层被设计为具有很少的参数（有时&lt;原始模型的 1%），并且具有小的瓶颈尺寸。</p>
</li>
<li>
<p>适配器层必须按顺序处理。<br />
大型神经网络依赖硬件并行性来保持低延迟。Adapter 对在线推理设置产生了影响，其中批量大小通常小至 1。在没有模型并行性的一般场景中，例如在单个 GPU 上的 GPT-2推理，<strong>使用适配器时延迟会显着增加，即使瓶颈维度非常小</strong>。</p>
</li>
</ol>
<p>[❓] 为什么 Adapter 导致在线推理的 batchsize 为1？</p>
<h4 id="直接优化提示很难"><a class="header" href="#直接优化提示很难">直接优化提示很难</a></h4>
<p>[❓] 这一段没看懂</p>
<h3 id="本文方法"><a class="header" href="#本文方法">本文方法</a></h3>
<blockquote>
<p>✅ 解决方法：仅仅拟合residual model 而不是 finetune entire model.</p>
</blockquote>
<p>我们提出了Low Rank Adapter（LoRA），它冻结了预训练的模型权重，并将可训练的rank分解矩阵注入到 Transformer 架构的每一层中，大大减少了下游任务的可训练参数的数量。与使用 Adam 微调的 GPT-3 175B 相比，LoRA 可以将可训练参数数量减少 10,000 倍，GPU 内存需求减少 3 倍。 </p>
<h3 id="效果"><a class="header" href="#效果">效果</a></h3>
<blockquote>
<p>✅ Results：LoRA对数据集要求少，收敛速度快。可以极大提升 finetune 效率，也更省空间。
LoRA 在 RoBERTa、DeBERTa、GPT-2 和 GPT-3 上的模型质量上表现与微调相当或更好，尽管可训练参数较少、训练吞吐量较高，并且与适配器不同，没有额外的推理延迟。</p>
</blockquote>
<h2 id="核心贡献是什么"><a class="header" href="#核心贡献是什么">核心贡献是什么？</a></h2>
<ol>
<li>
<p><strong>Low Rank Adapter（LoRA）</strong>：这是一种新技术，通过在Transformer架构的每一层中注入可训练的低秩分解矩阵来调整预训练模型的权重，而不是重新训练所有模型参数。</p>
</li>
<li>
<p><strong>易于实现和集成</strong>：论文提供了一个便于与PyTorch模型集成的LoRA包，以及RoBERTa、DeBERTa和GPT-2的实现和模型检查点。</p>
</li>
<li>
<p><strong>经验性研究</strong>：论文还提供了对语言模型适应中的秩不足进行实证研究，这有助于理解LoRA的有效性。</p>
</li>
</ol>
<h2 id="大致方法是什么"><a class="header" href="#大致方法是什么">大致方法是什么？</a></h2>
<h3 id="lora"><a class="header" href="#lora">LoRA</a></h3>
<p>神经网络包含许多执行矩阵乘法的密集层。这些层中的权重矩阵通常具有满秩。<br />
我们假设<strong>在adaption过程中权重的更新具有较低的“内在维度”。</strong><br />
定义预训练的权重矩阵为 \(W0 ∈ R^{d×k}\)，其更新过程为：W0 + ΔW</p>
<p>我们通过用低秩分解，将权重更新过程描述为： W0 + ΔW = W0 + BA，其中 \(B ∈ R^{d×r} , A ∈ R^{r×k}\) ，并且秩 \(r \ll min(d, k)\)。</p>
<p><img src="./assets/95e7ad3017ad4749fbc05bc5b2e3ce8a_0_Figure_1.png" alt="" /></p>
<pre><code class="language-python">self.lora_A = nn.Parameter(self.weight.new_zeros((r, num_embeddings)))
self.lora_B = nn.Parameter(self.weight.new_zeros((embedding_dim, r)))
</code></pre>
<p>请注意，W0 和 ΔW = BA 都与相同的输入相乘，并且它们各自的输出向量按坐标求和。</p>
<h3 id="lora应用到transformer"><a class="header" href="#lora应用到transformer">LoRA应用到Transformer</a></h3>
<p>理论上，LoRA可以应用于网络中的任意密集矩阵上，但本文仅将LoRA应用于attention层。<br />
对于特定adaption，使用特定的BA。换一种adaption则换一组BA。不需要adaption则将BA去掉。</p>
<h3 id="全参微调"><a class="header" href="#全参微调">全参微调</a></h3>
<p>极限情况下（所有层加 LoRA，且秩与原参数相同），LoRA 相当于全参微调。<br />
Adapter 相当于MLP<br />
prefix-based methods 收敛到“不能接收长输入”的模型。</p>
<h2 id="训练"><a class="header" href="#训练">训练</a></h2>
<p><img src="./assets/1596082-20240611214343488-23032747.png" alt="" /></p>
<h2 id="训练与验证"><a class="header" href="#训练与验证">训练与验证</a></h2>
<h3 id="训练策略"><a class="header" href="#训练策略">训练策略</a></h3>
<h4 id="参数冻结"><a class="header" href="#参数冻结">参数冻结</a></h4>
<p>训练期间，W0 被冻结，不接收梯度更新，而 A 和 B 包含可训练参数。</p>
<h4 id="参数初始化"><a class="header" href="#参数初始化">参数初始化</a></h4>
<p>对 A 使用随机高斯初始化，对 B 使用零，因此 ΔW = BA 在训练开始时为零，且梯度不为0。</p>
<pre><code class="language-python">nn.init.zeros_(self.lora_A)
nn.init.normal_(self.lora_B)
</code></pre>
<p>实际实现时，\(\Delta \mathbf{W} = \mathbf{B}\mathbf{A}\)会乘以系数\(\frac{\alpha}{r}\)与原始预训练权重合并\(\mathbf{W}_{0}\)，\(\alpha\)是一个超参：</p>
<p>$$
\mathbf{h} = (\mathbf{W}_{0} + \frac{\alpha}{r} \Delta \mathbf{W})\mathbf{x}
$$</p>
<pre><code class="language-python">def __init__(self, ...):
    ...
    self.scaling = self.lora_alpha / self.r
    ...

def train(self, ...):
    ...
    self.weight.data += (self.lora_B @ self.lora_A).transpose(0, 1) * self.scaling
    ...
</code></pre>
<h4 id="调参"><a class="header" href="#调参">调参</a></h4>
<p>超参\(\alpha\)和\(r\)实现了训练强度和注入强度的解耦。<br />
只需训练一种训练强度\(r\)，通过调整\(\alpha\)，就可以得到不同注入强度的效果。</p>
<p>直观来看，系数\(\frac{\alpha}{r}\)决定了在下游任务上微调得到的LoRA低秩适应的权重矩阵\(\mathbf{B}\mathbf{A}\)占最终模型参数的比例。</p>
<p>给定一个或多个下游任务数据，进行LoRA微调：</p>
<p>系数\(\frac{\alpha}{r}\)越大，LoRA微调权重的影响就越大，在下游任务上越容易过拟合
系数\(\frac{\alpha}{r}\)越小，LoRA微调权重的影响就越小（微调的效果不明显，原始模型参数受到的影响也较少）
一般来说，在给定任务上LoRA微调，让\({\alpha}\)为\(r\)的2倍数。（太大学过头了，太小学不动。）</p>
<p>根据经验，LoRA训练大概很难注入新的知识，更多是修改LLM的指令尊随的能力，例如输出风格和格式。原始的LLM能力，是在预训练是获得的（取决于参数量、数据规模X数据质量）。</p>
<p>LoRA的秩\(r\)决定，LoRA的低秩近似矩阵的拟合能力，实际任务需要调参挑选合适的秩\(r\)维度。系数\(\frac{\alpha}{r}\)中\(\alpha\)决定新老权重的占比。</p>
<h3 id="数据集"><a class="header" href="#数据集">数据集</a></h3>
<h3 id="loss"><a class="header" href="#loss">loss</a></h3>
<h3 id="训练策略-1"><a class="header" href="#训练策略-1">训练策略</a></h3>
<h2 id="实验与结论"><a class="header" href="#实验与结论">实验与结论</a></h2>
<p>在三种预训练模型上使用不同方式微调</p>
<ol>
<li>FT/BitFit：不使用插件，全部微调或部分参数微调。</li>
<li>Adpt：原参数fix，加入各种不同版本的Adapter.</li>
<li>LoRA.</li>
</ol>
<p><strong>实验一：</strong> RoBERT 等 NLP models</p>
<p><strong>效果：</strong> <img src="./assets/38-%E8%A1%A82.png" alt="" /></p>
<p><strong>实验二：</strong> GPT-2 等 NLG models</p>
<p><strong>效果：</strong> <img src="./assets/38-%E8%A1%A83.png" alt="" /></p>
<p><strong>实验三：</strong> 超大模型 GPT-3</p>
<p><strong>效果：</strong> <img src="./assets/38-%E8%A1%A84.png" alt="" /></p>
<p>当在 prefix-embedding tuning 使用超过256 special token 或在prefix-layer tuning使用超过32 special token时，性能明显下降。<br />
.</p>
<p><strong>分析：</strong> more special token 会导致输入分布与预训练模型的输入分布有偏移。</p>
<h2 id="相关工作"><a class="header" href="#相关工作">相关工作</a></h2>
<h3 id="transformer-language-models"><a class="header" href="#transformer-language-models">Transformer Language Models</a></h3>
<p>BERT,GPT-2,GPT-3</p>
<h3 id="提示工程和微调"><a class="header" href="#提示工程和微调">提示工程和微调</a></h3>
<p><strong>提示工程：</strong> 通过额外的训练示例输入模型，而不是训练模型，就可以调整模型的行为，学到新的任务。<br />
<strong>微调：</strong> 将一般领域预训练的模型重新训练到特定任务上。</p>
<h3 id="parameter-efficient-adaption"><a class="header" href="#parameter-efficient-adaption">Parameter-Efficient Adaption</a></h3>
<ol>
<li>在NN层之间插入adapter层。<br />
同样只引入少量的参数，但adapter的训练结果不能直接与原模型合并，而LoRA可以，因此使用LoRA不引及额外的推断时延。</li>
<li>COMPACTER，一种adapter的扩展方法，可以提升参数效率。</li>
<li>优化输入词的embedding</li>
<li>低秩结构</li>
</ol>
<h2 id="理解lora"><a class="header" href="#理解lora">理解LoRA</a></h2>
<p>“LoRA中的参数更新”与“预训练模型的weight”之间的关系，是可解释的。</p>
<h3 id="给定可调参数的预算应当调整预训练transformer中的哪个权重矩阵"><a class="header" href="#给定可调参数的预算应当调整预训练transformer中的哪个权重矩阵">给定可调参数的预算，应当调整预训练transformer中的哪个权重矩阵？</a></h3>
<p><strong>实验一：</strong></p>
<ol>
<li>对1个模块做LoRA，rank=8</li>
<li>对2个模块做LoRA，rank=4</li>
<li>对4个模块做LoRA，rank=2</li>
</ol>
<p><strong>效果：</strong>  <img src="./assets/38-%E8%A1%A85.png" alt="" /></p>
<p><strong>结论：</strong></p>
<ol>
<li>rank=4 是够用的。</li>
<li>对更多的模块做LoRA优于对一个模块用更大的rank做LoRA.</li>
</ol>
<h3 id="最优的delta-w矩阵真的低秩的吗实践中怎么定义rank"><a class="header" href="#最优的delta-w矩阵真的低秩的吗实践中怎么定义rank">最优的\(\Delta W\)矩阵真的低秩的吗？实践中怎么定义rank?</a></h3>
<p><strong>实验二：</strong></p>
<ol>
<li>使用不同的rank</li>
</ol>
<p><strong>效果：</strong>  <img src="./assets/38-%E8%A1%A86.png" alt="" /></p>
<p><strong>结论：</strong> \(\Delta W\)具有较小的内在rank.</p>
<p><strong>实验三：</strong> 不同的\(r\)之间的subspace的相似度</p>
<p>步骤：</p>
<ol>
<li>在相同的预训练模型上训LoRA，rank分别取8和64。</li>
<li>训好后，取矩阵\(A_{r=8}\) 和 \(A_{r=64}\)</li>
<li>对\(A\)做SVD（奇异值分解），取\(U_{r=8}\) 和 \(U_{r=64}\)</li>
<li>计算\(U^i_{r=8}\) 和 \(U^j_{r=64}\)的高斯距离和subspace相似度。1代表完全重合，0代表完全分离。</li>
</ol>
<p><strong>效果：</strong> <img src="./assets/38-%E5%9B%BE3.png" alt="" /></p>
<p><strong>结论：</strong></p>
<ol>
<li>\(A_{r=8}\) 和 \(A_{r=64}\)的 top singular Vector具有很高的相似度，其中 dimension 1 的相拟度大于 0.5，所以 \(r=1\)也能 performs quite well.</li>
<li>top singular-vector directions 包含了大部信息，而剩下的大部分都是随机嗓声，因此\(\Delta w\)使用低秩就够用了。</li>
</ol>
<p><strong>实验四：</strong> </p>
<ol>
<li>使用不同的seed 和 相同的 rank =64.</li>
</ol>
<p><strong>效果：</strong> <img src="./assets/38-%E5%9B%BE4.png" alt="" /><br />
\(\Delta w_q\)的 singular value具有更多的相似性。</p>
<p><strong>结论：</strong> \(\Delta W_q\)比\(\Delta W_\upsilon\)具有更高的 intrinsic rank\(_q\)，因为如果方向没有明显一致性，更有可能是随机噪声。</p>
<p><strong>实验五：</strong></p>
<h3 id="delta-w与w是什么关系"><a class="header" href="#delta-w与w是什么关系">\(\Delta W与W\)是什么关系？</a></h3>
<ol>
<li>\(W 和 \Delta W\)  的关系分析</li>
</ol>
<p><strong>结果：</strong> <img src="./assets/38-%E8%A1%A87.png" alt="" /></p>
<p><strong>结论：</strong>   \(\Delta W 和 W\) 之间有比较明显的关系。</p>
<ol>
<li>\(\Delta W 对 W\) 中已有的feafure 放大。</li>
<li>\(\Delta W\) 只放大\(W\)中不那么强调的特征。</li>
<li>放大因子比较大，且 r = 4，大于 r &gt; 64</li>
<li>LoRA 的主要作用是放大那些在下游任务中重要但在预训练模型中不重要的特征。</li>
</ol>
<h2 id="有效"><a class="header" href="#有效">有效</a></h2>
<ol>
<li>
<p><strong>模型质量</strong>：尽管可训练参数数量减少，LoRA在多个模型（如RoBERTa、DeBERTa、GPT-2和GPT-3）上的表现<strong>与全参数微调相当或更好</strong>。</p>
</li>
<li>
<p><strong>参数效率和计算效率</strong>：LoRA通过在Transformer架构的每一层中引入低秩矩阵来调整预训练模型，大幅减少了可训练参数的数量，从而降低了模型对计算资源的需求，训练更加高效。</p>
</li>
<li>
<p><strong>内存和存储效率</strong>：由于减少了可训练参数，LoRA在训练和部署时需要的内存和存储空间显著减少，使得在资源受限的环境中部署大型模型成为可能。</p>
</li>
<li>
<p><strong>无损推理速度</strong>：与其他方法（如适配器层）不同，LoRA在推理时不会引入额外的延迟，因为它允许在部署时<strong>合并训练的低秩矩阵与冻结的权重</strong>，保持了与原始模型相同的推理速度。</p>
</li>
<li>
<p><strong>模型共享与快速任务切换</strong>：LoRA允许共享一个预训练模型，并根据不同任务快速切换低秩矩阵，这减少了存储和部署多个独立模型实例的需要。</p>
</li>
<li>
<p><strong>易于实现和集成</strong>：论文提供了LoRA的实现和模型检查点，便于研究者和开发者将其集成到现有的PyTorch模型中。</p>
</li>
<li>
<p><strong>泛化能力</strong>：LoRA显示出良好的泛化能力，即使是在低数据环境下也能保持较好的性能。</p>
</li>
<li>
<p><strong>训练和推理的一致性</strong>：LoRA在训练和推理过程中保持了一致性，这有助于简化模型部署和应用。</p>
</li>
</ol>
<h2 id="局限性"><a class="header" href="#局限性">局限性</a></h2>
<ol>
<li>
<p><strong>特定权重的选择</strong>：LoRA需要选择哪些权重矩阵应用低秩适应，这可能需要基于经验或额外的启发式方法来决定。</p>
</li>
<li>
<p><strong>对低秩结构的依赖</strong>：LoRA假设模型权重的更新具有低秩结构，这可能不适用于所有类型的任务或模型架构。</p>
</li>
<li>
<p><strong>可能的性能瓶颈</strong>：尽管LoRA在多个任务上表现良好，但对于某些特定任务，可能需要更多的参数来捕捉任务的复杂性，这可能限制了LoRA的性能提升空间。</p>
</li>
<li>
<p><strong>适配器层的局限性</strong>：LoRA在某些情况下可能无法完全替代传统的适配器层，特别是在需要模型并行处理或处理不同任务输入的场景中。</p>
</li>
<li>
<p><strong>对预训练模型的依赖</strong>：LoRA依赖于高质量的预训练模型，如果预训练模型在某些领域或任务上的表现不佳，LoRA的适应效果也可能受限。</p>
</li>
<li>
<p><strong>超参数调整</strong>：LoRA的性能可能受到超参数（如低秩矩阵的秩）的影响，需要仔细调整这些参数以获得最佳性能。</p>
</li>
<li>
<p><strong>特定任务的适用性</strong>：LoRA可能在某些任务上特别有效，而在其他任务上则可能需要更多的定制化或不同的适应策略。</p>
</li>
</ol>
<h2 id="启发"><a class="header" href="#启发">启发</a></h2>
<h2 id="遗留问题"><a class="header" href="#遗留问题">遗留问题</a></h2>
<h2 id="参考材料"><a class="header" href="#参考材料">参考材料</a></h2>
<ol>
<li>代码仓库： https://github.com/microsoft/LoRA</li>
<li>https://www.cnblogs.com/justLittleStar/p/18242820</li>
</ol>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="39.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                            <a rel="next" href="37.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="39.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                    <a rel="next" href="37.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="theme/pagetoc.js"></script>
        <script type="text/javascript" src="theme/mermaid.min.js"></script>
        <script type="text/javascript" src="theme/mermaid-init.js"></script>
    </body>
</html>

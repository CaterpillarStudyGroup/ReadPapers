<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>3D Gaussian Splatting for Real-Time Radiance Field Rendering - ReadPapers</title>
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="The note of Papers">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/pagetoc.css">
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><div>ReadPapers</div></li><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="179.html"><strong aria-hidden="true">2.</strong> ParticleGS: Particle-Based Dynamics Modeling of 3D Gaussians for Prior-free Motion Extrapolation</a></li><li class="chapter-item expanded "><a href="178.html"><strong aria-hidden="true">3.</strong> Animate3d: Animating any 3d model with multi-view video diffusion</a></li><li class="chapter-item expanded "><a href="177.html"><strong aria-hidden="true">4.</strong> Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos</a></li><li class="chapter-item expanded "><a href="176.html"><strong aria-hidden="true">5.</strong> HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene</a></li><li class="chapter-item expanded "><a href="175.html"><strong aria-hidden="true">6.</strong> PIG: Physically-based Multi-Material Interaction with 3D Gaussians</a></li><li class="chapter-item expanded "><a href="174.html"><strong aria-hidden="true">7.</strong> EnliveningGS: Active Locomotion of 3DGS</a></li><li class="chapter-item expanded "><a href="173.html"><strong aria-hidden="true">8.</strong> SplineGS: Learning Smooth Trajectories in Gaussian Splatting for Dynamic Scene Reconstruction</a></li><li class="chapter-item expanded "><a href="172.html"><strong aria-hidden="true">9.</strong> PAMD: Plausibility-Aware Motion Diffusion Model for Long Dance Generation</a></li><li class="chapter-item expanded "><a href="171.html"><strong aria-hidden="true">10.</strong> PMG: Progressive Motion Generation via Sparse Anchor Postures Curriculum Learning</a></li><li class="chapter-item expanded "><a href="170.html"><strong aria-hidden="true">11.</strong> LengthAware Motion Synthesis via Latent Diffusion</a></li><li class="chapter-item expanded "><a href="169.html"><strong aria-hidden="true">12.</strong> IKMo: Image-Keyframed Motion Generation with Trajectory-Pose Conditioned Motion Diffusion Model</a></li><li class="chapter-item expanded "><a href="168.html"><strong aria-hidden="true">13.</strong> UniMoGen: Universal Motion Generation</a></li><li class="chapter-item expanded "><a href="167.html"><strong aria-hidden="true">14.</strong> AMD: Anatomical Motion Diffusion with Interpretable Motion Decomposition and Fusion</a></li><li class="chapter-item expanded "><a href="166.html"><strong aria-hidden="true">15.</strong> Flame: Free-form language-based motion synthesis &amp; editing</a></li><li class="chapter-item expanded "><a href="165.html"><strong aria-hidden="true">16.</strong> Human Motion Diffusion as a Generative Prior</a></li><li class="chapter-item expanded "><a href="164.html"><strong aria-hidden="true">17.</strong> Text-driven Human Motion Generation with Motion Masked Diffusion Model</a></li><li class="chapter-item expanded "><a href="163.html"><strong aria-hidden="true">18.</strong> ReMoDiffuse: RetrievalAugmented Motion Diffusion Model</a></li><li class="chapter-item expanded "><a href="162.html"><strong aria-hidden="true">19.</strong> MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model</a></li><li class="chapter-item expanded "><a href="161.html"><strong aria-hidden="true">20.</strong> ReAlign: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment</a></li><li class="chapter-item expanded "><a href="160.html"><strong aria-hidden="true">21.</strong> Absolute Coordinates Make Motion Generation Easy</a></li><li class="chapter-item expanded "><a href="159.html"><strong aria-hidden="true">22.</strong> Seamless Human Motion Composition with Blended Positional Encodings</a></li><li class="chapter-item expanded "><a href="158.html"><strong aria-hidden="true">23.</strong> FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing</a></li><li class="chapter-item expanded "><a href="157.html"><strong aria-hidden="true">24.</strong> Fg-T2M: Fine-Grained Text-Driven Human Motion Generation via Diffusion Model</a></li><li class="chapter-item expanded "><a href="156.html"><strong aria-hidden="true">25.</strong> Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation</a></li><li class="chapter-item expanded "><a href="155.html"><strong aria-hidden="true">26.</strong> StableMoFusion: Towards Robust and Efficient Diffusion-based Motion Generation Framework</a></li><li class="chapter-item expanded "><a href="154.html"><strong aria-hidden="true">27.</strong> EMDM: Efficient Motion Diffusion Model for Fast and High-Quality Motion Generation</a></li><li class="chapter-item expanded "><a href="153.html"><strong aria-hidden="true">28.</strong> Motion Mamba: Efficient and Long Sequence Motion Generation</a></li><li class="chapter-item expanded "><a href="152.html"><strong aria-hidden="true">29.</strong> M2D2M: Multi-Motion Generation from Text with Discrete Diffusion Models</a></li><li class="chapter-item expanded "><a href="151.html"><strong aria-hidden="true">30.</strong> T2LM: Long-Term 3D Human Motion Generation from Multiple Sentences</a></li><li class="chapter-item expanded "><a href="150.html"><strong aria-hidden="true">31.</strong> AttT2M:Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism</a></li><li class="chapter-item expanded "><a href="149.html"><strong aria-hidden="true">32.</strong> BAD: Bidirectional Auto-Regressive Diffusion for Text-to-Motion Generation</a></li><li class="chapter-item expanded "><a href="148.html"><strong aria-hidden="true">33.</strong> MMM: Generative Masked Motion Model</a></li><li class="chapter-item expanded "><a href="147.html"><strong aria-hidden="true">34.</strong> Priority-Centric Human Motion Generation in Discrete Latent Space</a></li><li class="chapter-item expanded "><a href="146.html"><strong aria-hidden="true">35.</strong> AvatarGPT: All-in-One Framework for Motion Understanding, Planning, Generation and Beyond</a></li><li class="chapter-item expanded "><a href="145.html"><strong aria-hidden="true">36.</strong> MotionGPT: Human Motion as a Foreign Language</a></li><li class="chapter-item expanded "><a href="144.html"><strong aria-hidden="true">37.</strong> Action-GPT: Leveraging Large-scale Language Models for Improved and Generalized Action Generation</a></li><li class="chapter-item expanded "><a href="143.html"><strong aria-hidden="true">38.</strong> PoseGPT: Quantization-based 3D Human Motion Generation and Forecasting</a></li><li class="chapter-item expanded "><a href="142.html"><strong aria-hidden="true">39.</strong> Incorporating Physics Principles for Precise Human Motion Prediction</a></li><li class="chapter-item expanded "><a href="141.html"><strong aria-hidden="true">40.</strong> PIMNet: Physics-infused Neural Network for Human Motion Prediction</a></li><li class="chapter-item expanded "><a href="140.html"><strong aria-hidden="true">41.</strong> PhysDiff: Physics-Guided Human Motion Diffusion Model</a></li><li class="chapter-item expanded "><a href="139.html"><strong aria-hidden="true">42.</strong> NRDF: Neural Riemannian Distance Fields for Learning Articulated Pose Priors</a></li><li class="chapter-item expanded "><a href="138.html"><strong aria-hidden="true">43.</strong> Pose-NDF: Modeling Human Pose Manifolds with Neural Distance Fields</a></li><li class="chapter-item expanded "><a href="137.html"><strong aria-hidden="true">44.</strong> Geometric Neural Distance Fields for Learning Human Motion Priors</a></li><li class="chapter-item expanded "><a href="136.html"><strong aria-hidden="true">45.</strong> Character Controllers Using Motion VAEs</a></li><li class="chapter-item expanded "><a href="135.html"><strong aria-hidden="true">46.</strong> Improving Human Motion Plausibility with Body Momentum</a></li><li class="chapter-item expanded "><a href="134.html"><strong aria-hidden="true">47.</strong> MoGlow: Probabilistic and controllable motion synthesis using normalising flows</a></li><li class="chapter-item expanded "><a href="133.html"><strong aria-hidden="true">48.</strong> Modi: Unconditional motion synthesis from diverse data</a></li><li class="chapter-item expanded "><a href="132.html"><strong aria-hidden="true">49.</strong> MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model</a></li><li class="chapter-item expanded "><a href="131.html"><strong aria-hidden="true">50.</strong> A deep learning framework for character motion synthesis and editing</a></li><li class="chapter-item expanded "><a href="130.html"><strong aria-hidden="true">51.</strong> Multi-Object Sketch Animation with Grouping and Motion Trajectory Priors</a></li><li class="chapter-item expanded "><a href="129.html"><strong aria-hidden="true">52.</strong> TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos</a></li><li class="chapter-item expanded "><a href="128.html"><strong aria-hidden="true">53.</strong> X-MoGen: Unified Motion Generation across Humans and Animals</a></li><li class="chapter-item expanded "><a href="127.html"><strong aria-hidden="true">54.</strong> Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis</a></li><li class="chapter-item expanded "><a href="126.html"><strong aria-hidden="true">55.</strong> MotionShot: Adaptive Motion Transfer across Arbitrary Objects for Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="114.html"><strong aria-hidden="true">56.</strong> Drop: Dynamics responses from human motion prior and projective dynamics</a></li><li class="chapter-item expanded "><a href="112.html"><strong aria-hidden="true">57.</strong> POMP: Physics-constrainable Motion Generative Model through Phase Manifolds</a></li><li class="chapter-item expanded "><a href="111.html"><strong aria-hidden="true">58.</strong> Dreamgaussian4d: Generative 4d gaussian splatting</a></li><li class="chapter-item expanded "><a href="110.html"><strong aria-hidden="true">59.</strong> Drive Any Mesh: 4D Latent Diffusion for Mesh Deformation from Video</a></li><li class="chapter-item expanded "><a href="109.html"><strong aria-hidden="true">60.</strong> AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation</a></li><li class="chapter-item expanded "><a href="108.html"><strong aria-hidden="true">61.</strong> ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction</a></li><li class="chapter-item expanded "><a href="107.html"><strong aria-hidden="true">62.</strong> Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators</a></li><li class="chapter-item expanded "><a href="106.html"><strong aria-hidden="true">63.</strong> Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals</a></li><li class="chapter-item expanded "><a href="105.html"><strong aria-hidden="true">64.</strong> Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation</a></li><li class="chapter-item expanded "><a href="104.html"><strong aria-hidden="true">65.</strong> Generating time-consistent dynamics with discriminator-guided image diffusion models</a></li><li class="chapter-item expanded "><a href="103.html"><strong aria-hidden="true">66.</strong> GENMO:AGENeralist Model for Human MOtion</a></li><li class="chapter-item expanded "><a href="102.html"><strong aria-hidden="true">67.</strong> HGM3: HIERARCHICAL GENERATIVE MASKED MOTION MODELING WITH HARD TOKEN MINING</a></li><li class="chapter-item expanded "><a href="101.html"><strong aria-hidden="true">68.</strong> Towards Robust and Controllable Text-to-Motion via Masked Autoregressive Diffusion</a></li><li class="chapter-item expanded "><a href="100.html"><strong aria-hidden="true">69.</strong> MoCLIP: Motion-Aware Fine-Tuning and Distillation of CLIP for Human Motion Generation</a></li><li class="chapter-item expanded "><a href="99.html"><strong aria-hidden="true">70.</strong> FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance</a></li><li class="chapter-item expanded "><a href="98.html"><strong aria-hidden="true">71.</strong> VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence</a></li><li class="chapter-item expanded "><a href="97.html"><strong aria-hidden="true">72.</strong> DragAnything: Motion Control for Anything using Entity Representation</a></li><li class="chapter-item expanded "><a href="96.html"><strong aria-hidden="true">73.</strong> PhysAnimator: Physics-Guided Generative Cartoon Animation</a></li><li class="chapter-item expanded "><a href="95.html"><strong aria-hidden="true">74.</strong> SOAP: Style-Omniscient Animatable Portraits</a></li><li class="chapter-item expanded "><a href="94.html"><strong aria-hidden="true">75.</strong> Neural Discrete Representation Learning</a></li><li class="chapter-item expanded "><a href="93.html"><strong aria-hidden="true">76.</strong> TSTMotion: Training-free Scene-aware Text-to-motion Generation</a></li><li class="chapter-item expanded "><a href="92.html"><strong aria-hidden="true">77.</strong> Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis</a></li><li class="chapter-item expanded "><a href="91.html"><strong aria-hidden="true">78.</strong> A lip sync expert is all you need for speech to lip generation in the wild</a></li><li class="chapter-item expanded "><a href="90.html"><strong aria-hidden="true">79.</strong> MUSETALK: REAL-TIME HIGH QUALITY LIP SYN-CHRONIZATION WITH LATENT SPACE INPAINTING</a></li><li class="chapter-item expanded "><a href="89.html"><strong aria-hidden="true">80.</strong> LatentSync: Audio Conditioned Latent Diffusion Models for Lip Sync</a></li><li class="chapter-item expanded "><a href="88.html"><strong aria-hidden="true">81.</strong> T2m-gpt: Generating human motion from textual descriptions with discrete representations</a></li><li class="chapter-item expanded "><a href="87.html"><strong aria-hidden="true">82.</strong> Motiongpt: Finetuned llms are general-purpose motion generators</a></li><li class="chapter-item expanded "><a href="86.html"><strong aria-hidden="true">83.</strong> Guided Motion Diffusion for Controllable Human Motion Synthesis</a></li><li class="chapter-item expanded "><a href="85.html"><strong aria-hidden="true">84.</strong> OmniControl: Control Any Joint at Any Time for Human Motion Generation</a></li><li class="chapter-item expanded "><a href="84.html"><strong aria-hidden="true">85.</strong> Learning Long-form Video Prior via Generative Pre-Training</a></li><li class="chapter-item expanded "><a href="83.html"><strong aria-hidden="true">86.</strong> Instant Neural Graphics Primitives with a Multiresolution Hash Encoding</a></li><li class="chapter-item expanded "><a href="82.html"><strong aria-hidden="true">87.</strong> Magic3D: High-Resolution Text-to-3D Content Creation</a></li><li class="chapter-item expanded "><a href="81.html"><strong aria-hidden="true">88.</strong> CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers</a></li><li class="chapter-item expanded "><a href="80.html"><strong aria-hidden="true">89.</strong> One-Minute Video Generation with Test-Time Training</a></li><li class="chapter-item expanded "><a href="79.html"><strong aria-hidden="true">90.</strong> Key-Locked Rank One Editing for Text-to-Image Personalization</a></li><li class="chapter-item expanded "><a href="78.html"><strong aria-hidden="true">91.</strong> MARCHING CUBES: A HIGH RESOLUTION 3D SURFACE CONSTRUCTION ALGORITHM</a></li><li class="chapter-item expanded "><a href="77.html"><strong aria-hidden="true">92.</strong> Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation</a></li><li class="chapter-item expanded "><a href="76.html"><strong aria-hidden="true">93.</strong> NULL-text Inversion for Editing Real Images Using Guided Diffusion Models</a></li><li class="chapter-item expanded "><a href="75.html"><strong aria-hidden="true">94.</strong> simple diffusion: End-to-end diffusion for high resolution images</a></li><li class="chapter-item expanded "><a href="74.html"><strong aria-hidden="true">95.</strong> One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale</a></li><li class="chapter-item expanded "><a href="73.html"><strong aria-hidden="true">96.</strong> Scalable Diffusion Models with Transformers</a></li><li class="chapter-item expanded "><a href="72.html"><strong aria-hidden="true">97.</strong> All are Worth Words: a ViT Backbone for Score-based Diffusion Models</a></li><li class="chapter-item expanded "><a href="71.html"><strong aria-hidden="true">98.</strong> An image is worth 16x16 words: Transformers for image recognition at scale</a></li><li class="chapter-item expanded "><a href="70.html"><strong aria-hidden="true">99.</strong> eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers</a></li><li class="chapter-item expanded "><a href="69.html"><strong aria-hidden="true">100.</strong> Photorealistic text-to-image diffusion models with deep language understanding||Imagen</a></li><li class="chapter-item expanded "><a href="68.html"><strong aria-hidden="true">101.</strong> DreamFusion: Text-to-3D using 2D Diffusion</a></li><li class="chapter-item expanded "><a href="67.html"><strong aria-hidden="true">102.</strong> GLIGEN: Open-Set Grounded Text-to-Image Generation</a></li><li class="chapter-item expanded "><a href="66.html"><strong aria-hidden="true">103.</strong> Adding Conditional Control to Text-to-Image Diffusion Models</a></li><li class="chapter-item expanded "><a href="65.html"><strong aria-hidden="true">104.</strong> T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models</a></li><li class="chapter-item expanded "><a href="64.html"><strong aria-hidden="true">105.</strong> Multi-Concept Customization of Text-to-Image Diffusion</a></li><li class="chapter-item expanded "><a href="63.html"><strong aria-hidden="true">106.</strong> An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion</a></li><li class="chapter-item expanded "><a href="62.html"><strong aria-hidden="true">107.</strong> DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</a></li><li class="chapter-item expanded "><a href="61.html"><strong aria-hidden="true">108.</strong> VisorGPT: Learning Visual Prior via Generative Pre-Training</a></li><li class="chapter-item expanded "><a href="60.html"><strong aria-hidden="true">109.</strong> NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation</a></li><li class="chapter-item expanded "><a href="59.html"><strong aria-hidden="true">110.</strong> AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning</a></li><li class="chapter-item expanded "><a href="58.html"><strong aria-hidden="true">111.</strong> ModelScope Text-to-Video Technical Report</a></li><li class="chapter-item expanded "><a href="57.html"><strong aria-hidden="true">112.</strong> Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="56.html"><strong aria-hidden="true">113.</strong> Make-A-Video: Text-to-Video Generation without Text-Video Data</a></li><li class="chapter-item expanded "><a href="55.html"><strong aria-hidden="true">114.</strong> Video Diffusion Models</a></li><li class="chapter-item expanded "><a href="54.html"><strong aria-hidden="true">115.</strong> Learning Transferable Visual Models From Natural Language Supervision</a></li><li class="chapter-item expanded "><a href="53.html"><strong aria-hidden="true">116.</strong> Implicit Warping for Animation with Image Sets</a></li><li class="chapter-item expanded "><a href="52.html"><strong aria-hidden="true">117.</strong> Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models</a></li><li class="chapter-item expanded "><a href="51.html"><strong aria-hidden="true">118.</strong> Motion-Conditioned Diffusion Model for Controllable Video Synthesis</a></li><li class="chapter-item expanded "><a href="50.html"><strong aria-hidden="true">119.</strong> Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets</a></li><li class="chapter-item expanded "><a href="49.html"><strong aria-hidden="true">120.</strong> UniAnimate: Taming Unified Video Diffusion Models for Consistent Human Image Animation</a></li><li class="chapter-item expanded "><a href="48.html"><strong aria-hidden="true">121.</strong> Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models</a></li><li class="chapter-item expanded "><a href="47.html"><strong aria-hidden="true">122.</strong> Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics</a></li><li class="chapter-item expanded "><a href="46.html"><strong aria-hidden="true">123.</strong> A Recipe for Scaling up Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="45.html"><strong aria-hidden="true">124.</strong> High-Resolution Image Synthesis with Latent Diffusion Models</a></li><li class="chapter-item expanded "><a href="44.html"><strong aria-hidden="true">125.</strong> Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling</a></li><li class="chapter-item expanded "><a href="43.html"><strong aria-hidden="true">126.</strong> æ•°æ®é›†ï¼šHumanVid</a></li><li class="chapter-item expanded "><a href="42.html"><strong aria-hidden="true">127.</strong> HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation</a></li><li class="chapter-item expanded "><a href="41.html"><strong aria-hidden="true">128.</strong> StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation</a></li><li class="chapter-item expanded "><a href="40.html"><strong aria-hidden="true">129.</strong> æ•°æ®é›†ï¼šZoo-300K</a></li><li class="chapter-item expanded "><a href="39.html"><strong aria-hidden="true">130.</strong> Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion</a></li><li class="chapter-item expanded "><a href="38.html"><strong aria-hidden="true">131.</strong> LORA: LOW-RANK ADAPTATION OF LARGE LAN-GUAGE MODELS</a></li><li class="chapter-item expanded "><a href="37.html"><strong aria-hidden="true">132.</strong> TCAN: Animating Human Images with Temporally Consistent Pose Guidance using Diffusion Models</a></li><li class="chapter-item expanded "><a href="36.html"><strong aria-hidden="true">133.</strong> GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians</a></li><li class="chapter-item expanded "><a href="35.html"><strong aria-hidden="true">134.</strong> MagicPony: Learning Articulated 3D Animals in the Wild</a></li><li class="chapter-item expanded "><a href="34.html"><strong aria-hidden="true">135.</strong> Splatter a Video: Video Gaussian Representation for Versatile Processing</a></li><li class="chapter-item expanded "><a href="33.html"><strong aria-hidden="true">136.</strong> æ•°æ®é›†ï¼šDynamic Furry Animal Dataset</a></li><li class="chapter-item expanded "><a href="32.html"><strong aria-hidden="true">137.</strong> Artemis: Articulated Neural Pets with Appearance and Motion Synthesis</a></li><li class="chapter-item expanded "><a href="31.html"><strong aria-hidden="true">138.</strong> SMPLer: Taming Transformers for Monocular 3D Human Shape and Pose Estimation</a></li><li class="chapter-item expanded "><a href="30.html"><strong aria-hidden="true">139.</strong> CAT3D: Create Anything in 3D with Multi-View Diffusion Models</a></li><li class="chapter-item expanded "><a href="29.html"><strong aria-hidden="true">140.</strong> PACER+: On-Demand Pedestrian Animation Controller in Driving Scenarios</a></li><li class="chapter-item expanded "><a href="28.html"><strong aria-hidden="true">141.</strong> Humans in 4D: Reconstructing and Tracking Humans with Transformers</a></li><li class="chapter-item expanded "><a href="27.html"><strong aria-hidden="true">142.</strong> Learning Human Motion from Monocular Videos via Cross-Modal Manifold Alignment</a></li><li class="chapter-item expanded "><a href="26.html"><strong aria-hidden="true">143.</strong> PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos</a></li><li class="chapter-item expanded "><a href="25.html"><strong aria-hidden="true">144.</strong> Imagic: Text-Based Real Image Editing with Diffusion Models</a></li><li class="chapter-item expanded "><a href="24.html"><strong aria-hidden="true">145.</strong> DiffEdit: Diffusion-based semantic image editing with mask guidance</a></li><li class="chapter-item expanded "><a href="23.html"><strong aria-hidden="true">146.</strong> Dual diffusion implicit bridges for image-to-image translation</a></li><li class="chapter-item expanded "><a href="22.html"><strong aria-hidden="true">147.</strong> SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations</a></li><li class="chapter-item expanded "><a href="20.html"><strong aria-hidden="true">148.</strong> Prompt-to-Prompt Image Editing with Cross-Attention Control</a></li><li class="chapter-item expanded "><a href="19.html"><strong aria-hidden="true">149.</strong> WANDR: Intention-guided Human Motion Generation</a></li><li class="chapter-item expanded "><a href="18.html"><strong aria-hidden="true">150.</strong> TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Videos</a></li><li class="chapter-item expanded "><a href="17.html" class="active"><strong aria-hidden="true">151.</strong> 3D Gaussian Splatting for Real-Time Radiance Field Rendering</a></li><li class="chapter-item expanded "><a href="16.html"><strong aria-hidden="true">152.</strong> Decoupling Human and Camera Motion from Videos in the Wild</a></li><li class="chapter-item expanded "><a href="15.html"><strong aria-hidden="true">153.</strong> HMP: Hand Motion Priors for Pose and Shape Estimation from Video</a></li><li class="chapter-item expanded "><a href="14.html"><strong aria-hidden="true">154.</strong> HuMoR: 3D Human Motion Model for Robust Pose Estimation</a></li><li class="chapter-item expanded "><a href="13.html"><strong aria-hidden="true">155.</strong> Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video</a></li><li class="chapter-item expanded "><a href="12.html"><strong aria-hidden="true">156.</strong> Global-to-Local Modeling for Video-based 3D Human Pose and Shape Estimation</a></li><li class="chapter-item expanded "><a href="11.html"><strong aria-hidden="true">157.</strong> WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion</a></li><li class="chapter-item expanded "><a href="10.html"><strong aria-hidden="true">158.</strong> Tackling the Generative Learning Trilemma with Denoising Diffusion GANs</a></li><li class="chapter-item expanded "><a href="9.html"><strong aria-hidden="true">159.</strong> Elucidating the Design Space of Diffusion-Based Generative Models</a></li><li class="chapter-item expanded "><a href="8.html"><strong aria-hidden="true">160.</strong> SCORE-BASED GENERATIVE MODELING THROUGHSTOCHASTIC DIFFERENTIAL EQUATIONS</a></li><li class="chapter-item expanded "><a href="7.html"><strong aria-hidden="true">161.</strong> Consistency Models</a></li><li class="chapter-item expanded "><a href="6.html"><strong aria-hidden="true">162.</strong> Classifier-Free Diffusion Guidance</a></li><li class="chapter-item expanded "><a href="5.html"><strong aria-hidden="true">163.</strong> Cascaded Diffusion Models for High Fidelity Image Generation</a></li><li class="chapter-item expanded "><a href="4.html"><strong aria-hidden="true">164.</strong> LEARNING ENERGY-BASED MODELS BY DIFFUSIONRECOVERY LIKELIHOOD</a></li><li class="chapter-item expanded "><a href="3.html"><strong aria-hidden="true">165.</strong> On Distillation of Guided Diffusion Models</a></li><li class="chapter-item expanded "><a href="2.html"><strong aria-hidden="true">166.</strong> Denoising Diffusion Implicit Models</a></li><li class="chapter-item expanded "><a href="1.html"><strong aria-hidden="true">167.</strong> PROGRESSIVE DISTILLATION FOR FAST SAMPLING OF DIFFUSION MODELS</a></li><li class="chapter-item expanded "><a href="125.html"><strong aria-hidden="true">168.</strong> Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions</a></li><li class="chapter-item expanded "><a href="124.html"><strong aria-hidden="true">169.</strong> ControlVideo: Training-free Controllable Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="123.html"><strong aria-hidden="true">170.</strong> Pix2Video: Video Editing using Image Diffusion</a></li><li class="chapter-item expanded "><a href="122.html"><strong aria-hidden="true">171.</strong> Structure and Content-Guided Video Synthesis with Diffusion Models</a></li><li class="chapter-item expanded "><a href="121.html"><strong aria-hidden="true">172.</strong> MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model</a></li><li class="chapter-item expanded "><a href="120.html"><strong aria-hidden="true">173.</strong> MotionDirector: Motion Customization of Text-to-Video Diffusion Models</a></li><li class="chapter-item expanded "><a href="119.html"><strong aria-hidden="true">174.</strong> Dreamix: Video Diffusion Models are General Video Editors</a></li><li class="chapter-item expanded "><a href="118.html"><strong aria-hidden="true">175.</strong> Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="117.html"><strong aria-hidden="true">176.</strong> TokenFlow: Consistent Diffusion Features for Consistent Video Editing</a></li><li class="chapter-item expanded "><a href="116.html"><strong aria-hidden="true">177.</strong> DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing</a></li><li class="chapter-item expanded "><a href="115.html"><strong aria-hidden="true">178.</strong> Content Deformation Fields for Temporally Consistent Video Processing</a></li><li class="chapter-item expanded "><a href="113.html"><strong aria-hidden="true">179.</strong> PFNN: Phase-Functioned Neural Networks</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">ReadPapers</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/CaterpillarStudyGroup/ReadPapers" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main><div class="sidetoc"><nav class="pagetoc"></nav></div>
                        <h1 id="3d-gaussian-splatting-for-real-time-radiance-field-rendering"><a class="header" href="#3d-gaussian-splatting-for-real-time-radiance-field-rendering">3D Gaussian Splatting for Real-Time Radiance Field Rendering</a></h1>
<table><thead><tr><th>ç¼©å†™</th><th>è‹±æ–‡</th><th>ä¸­æ–‡</th></tr></thead><tbody>
<tr><td>SfM</td><td>Structure-from-Motion</td><td></td></tr>
<tr><td>SH</td><td>spherical harmonics</td><td><a href="https://caterpillarstudygroup.github.io/mathematics_basic_for_ML/Geometry/SphericalHarmonics.html">çƒè°åŸº</a></td></tr>
<tr><td></td><td>covariance matrix</td><td>åæ–¹å·®çŸ©é˜µ</td></tr>
<tr><td>Nerf</td><td>Neural Radiance Field</td><td>ç¥ç»è¾å°„åœº</td></tr>
<tr><td>GS</td><td>Gaussian splatting</td><td>é«˜æ–¯æº…å°„</td></tr>
</tbody></table>
<h2 id="æ ¸å¿ƒé—®é¢˜æ˜¯ä»€ä¹ˆ"><a class="header" href="#æ ¸å¿ƒé—®é¢˜æ˜¯ä»€ä¹ˆ">æ ¸å¿ƒé—®é¢˜æ˜¯ä»€ä¹ˆ?</a></h2>
<h3 id="ç°æœ‰æ–¹æ³•åŠé—®é¢˜"><a class="header" href="#ç°æœ‰æ–¹æ³•åŠé—®é¢˜">ç°æœ‰æ–¹æ³•åŠé—®é¢˜</a></h3>
<p>Nerfæ–¹æ³•éœ€è¦è®­ç»ƒå’Œæ¸²æŸ“æˆæœ¬é«˜æ˜‚çš„ç¥ç»ç½‘ç»œï¼Œè€Œå…¶åŠ é€Ÿæ–¹æ¡ˆä¼šç‰ºç‰²è´¨é‡æ¥æ¢å–é€Ÿåº¦ã€‚å¯¹äºæ— ç•Œä¸”å®Œæ•´çš„åœºæ™¯ï¼ˆè€Œä¸æ˜¯å­¤ç«‹çš„ç‰©ä½“ï¼‰å’Œ1080påˆ†è¾¨ç‡æ¸²æŸ“ï¼Œå½“å‰æ²¡æœ‰æ–¹æ³•å¯ä»¥å®ç°å®æ—¶æ˜¾ç¤ºé€Ÿç‡ã€‚</p>
<h3 id="æœ¬æ–‡æ–¹æ³•"><a class="header" href="#æœ¬æ–‡æ–¹æ³•">æœ¬æ–‡æ–¹æ³•</a></h3>
<p>é«˜æ–¯æº…å°„æ˜¯ä¸€ç§è¡¨ç¤º 3D åœºæ™¯å’Œæ¸²æŸ“æ–°è§†å›¾çš„æ–¹æ³•ï¼Œå®ƒè¢«è®¤ä¸ºæ˜¯ NeRF ç±»æ¨¡å‹çš„æ›¿ä»£å“ã€‚<br />
è¿™é¡¹å·¥ä½œæœ€å‡ºåçš„åœ°æ–¹æ˜¯å…¶é«˜æ¸²æŸ“é€Ÿåº¦ã€‚è¿™å½’åŠŸäºä¸‹é¢å°†è¦ä»‹ç»çš„è¡¨ç¤ºæœ¬èº«ï¼Œä»¥åŠä½¿ç”¨è‡ªå®šä¹‰ CUDA å†…æ ¸å®šåˆ¶å®ç°çš„æ¸²æŸ“ç®—æ³•ã€‚<br />
é¦–å…ˆï¼Œä»ç›¸æœºæ ¡å‡†æœŸé—´äº§ç”Ÿçš„ç¨€ç–ç‚¹å¼€å§‹ï¼Œç”¨ 3D é«˜æ–¯è¡¨ç¤ºåœºæ™¯ï¼Œä¿ç•™è¿ç»­ä½“ç§¯è¾å°„åœºçš„æ‰€éœ€å±æ€§ä»¥è¿›è¡Œåœºæ™¯ä¼˜åŒ–ï¼ŒåŒæ—¶é¿å…åœ¨ç©ºç™½åŒºåŸŸä¸­è¿›è¡Œä¸å¿…è¦çš„è®¡ç®—ï¼›<br />
å…¶æ¬¡ï¼Œæˆ‘ä»¬å¯¹ 3D é«˜æ–¯è¿›è¡Œäº¤é”™ä¼˜åŒ–/å¯†åº¦æ§åˆ¶ï¼Œç‰¹åˆ«æ˜¯ä¼˜åŒ–å„å‘å¼‚æ€§åæ–¹å·®ä»¥å®ç°åœºæ™¯çš„å‡†ç¡®è¡¨ç¤ºï¼›<br />
ç¬¬ä¸‰ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¿«é€Ÿå¯è§æ€§æ„ŸçŸ¥æ¸²æŸ“ç®—æ³•ï¼Œè¯¥ç®—æ³•æ”¯æŒå„å‘å¼‚æ€§æ³¼æº…ï¼Œæ—¢åŠ é€Ÿè®­ç»ƒåˆå…è®¸å®æ—¶æ¸²æŸ“ã€‚</p>
<h3 id="æ•ˆæœ"><a class="header" href="#æ•ˆæœ">æ•ˆæœ</a></h3>
<p>åœ¨å‡ ä¸ªå·²å»ºç«‹çš„æ•°æ®é›†ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„è§†è§‰è´¨é‡å’Œå®æ—¶æ¸²æŸ“ã€‚</p>
<h2 id="æ ¸å¿ƒè´¡çŒ®æ˜¯ä»€ä¹ˆ"><a class="header" href="#æ ¸å¿ƒè´¡çŒ®æ˜¯ä»€ä¹ˆ">æ ¸å¿ƒè´¡çŒ®æ˜¯ä»€ä¹ˆï¼Ÿ</a></h2>
<ul>
<li>å¼•å…¥å„å‘å¼‚æ€§ 3D é«˜æ–¯ä½œä¸ºè¾å°„åœºçš„é«˜è´¨é‡ã€éç»“æ„åŒ–è¡¨ç¤ºã€‚</li>
<li>3D é«˜æ–¯å±æ€§çš„ä¼˜åŒ–æ–¹æ³•ï¼Œä¸è‡ªé€‚åº”å¯†åº¦æ§åˆ¶äº¤é”™ï¼Œä¸ºæ•è·çš„åœºæ™¯åˆ›å»ºé«˜è´¨é‡è¡¨ç¤ºã€‚</li>
<li>å¿«é€Ÿã€å¯å¾®çš„æ¸²æŸ“æ–¹æ³•å¯¹äºå¯è§æ€§æ„ŸçŸ¥çš„ GPUï¼Œå…è®¸å„å‘å¼‚æ€§æ³¼æº…å’Œå¿«é€Ÿåå‘ä¼ æ’­ï¼Œä»¥å®ç°é«˜è´¨é‡çš„æ–°é¢–è§†å›¾åˆæˆã€‚</li>
</ul>
<h2 id="å¤§è‡´æ–¹æ³•æ˜¯ä»€ä¹ˆ"><a class="header" href="#å¤§è‡´æ–¹æ³•æ˜¯ä»€ä¹ˆ">å¤§è‡´æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ</a></h2>
<p><img src="./assets/90c87fe420b7f068f6ef682c1ee5ed26_4_Figure_2_-1952255684.png" alt="" /></p>
<p>è¾“å…¥ï¼šä¸€ç»„é™æ€åœºæ™¯çš„å›¾åƒ</p>
<ol>
<li>ç”± SfM æ ¡å‡†çš„ç›¸åº”ç›¸æœºï¼Œä¼šäº§ç”Ÿç¨€ç–ç‚¹äº‘ã€‚</li>
<li>ä»SfMç‚¹äº‘åˆ›å»ºäº†ä¸€ç»„ 3D é«˜æ–¯ï¼ˆç¬¬ 4 èŠ‚ï¼‰ï¼Œç”±ä½ç½®ï¼ˆå‡å€¼ï¼‰ã€åæ–¹å·®çŸ©é˜µå’Œä¸é€æ˜åº¦ ğ›¼ å®šä¹‰è¿™äº›é«˜æ–¯ã€‚</li>
</ol>
<blockquote>
<p>è¿™å…è®¸éå¸¸çµæ´»çš„ä¼˜åŒ–æœºåˆ¶ã€‚è¿™ä¼šäº§ç”Ÿ 3D åœºæ™¯çš„ç›¸å½“ç´§å‡‘çš„è¡¨ç¤ºï¼Œéƒ¨åˆ†åŸå› æ˜¯é«˜åº¦<strong>å„å‘å¼‚æ€§çš„ä½“ç§¯ç‰‡å¯ç”¨äºç´§å‡‘åœ°è¡¨ç¤ºç²¾ç»†ç»“æ„</strong>ã€‚</p>
</blockquote>
<ol start="3">
<li>è¾å°„åœºçš„æ–¹å‘å¤–è§‚åˆ†é‡ï¼ˆé¢œè‰²ï¼‰é€šè¿‡çƒè°å‡½æ•° (SH) è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡ 3D é«˜æ–¯å‚æ•°çš„ä¸€ç³»åˆ—ä¼˜åŒ–æ­¥éª¤æ¥åˆ›å»ºè¾å°„åœºè¡¨ç¤ºï¼ˆç¬¬ 5 èŠ‚ï¼‰ï¼Œå³ä½ç½®ã€åæ–¹å·®ã€ğ›¼ å’Œ SH ç³»æ•°ä¸é«˜æ–¯å¯†åº¦è‡ªé€‚åº”æ§åˆ¶çš„æ“ä½œäº¤ç»‡åœ¨ä¸€èµ·ã€‚</li>
<li>åŸºäºå›¾å—çš„å…‰æ …åŒ–å™¨ï¼ˆæ•ˆç‡çš„å…³é”®ï¼‰ï¼ˆç¬¬ 6 èŠ‚ï¼‰ï¼Œè®©å„å‘å¼‚æ€§å›¾å—çš„ğ›¼æ··åˆï¼Œé€šè¿‡å¿«é€Ÿæ’åºè¡¨ç¤ºå¯è§æ€§é¡ºåºã€‚</li>
</ol>
<blockquote>
<p>å¿«é€Ÿå…‰æ …åŒ–å™¨è¿˜åŒ…æ‹¬é€šè¿‡è·Ÿè¸ªç´¯ç§¯çš„ ğ›¼ å€¼è¿›è¡Œå¿«é€Ÿå‘åä¼ é€’ï¼Œå¹¶ä¸”å¯¹å¯ä»¥æ¥æ”¶æ¢¯åº¦çš„é«˜æ–¯æ•°é‡æ²¡æœ‰é™åˆ¶ã€‚</p>
</blockquote>
<h3 id="å¯å¾®-3d-é«˜æ–¯splatting"><a class="header" href="#å¯å¾®-3d-é«˜æ–¯splatting">å¯å¾® 3D é«˜æ–¯Splatting</a></h3>
<p>è¾“å…¥ï¼šæ²¡æœ‰æ³•çº¿ä¿¡æ¯çš„ç¨€ç– (SfM) ç‚¹é›†<br />
è¾“å‡ºï¼šå…è®¸é«˜è´¨é‡æ–°è§†è§’åˆæˆçš„åœºæ™¯è¡¨ç¤ºï¼Œå³ä¸€ç»„ 3D é«˜æ–¯ã€‚</p>
<h4 id="è¡¨ç¤º"><a class="header" href="#è¡¨ç¤º">è¡¨ç¤º</a></h4>
<p>3D ä¸–ç•Œç”±ä¸€ç»„ 3D ç‚¹è¡¨ç¤ºï¼Œå®é™…ä¸Šæœ‰æ•°ç™¾ä¸‡ä¸ª 3D ç‚¹ï¼Œæ•°é‡å¤§çº¦ä¸º 50 ä¸‡åˆ° 500 ä¸‡ã€‚æ¯ä¸ªç‚¹éƒ½æ˜¯ä¸€ä¸ª 3D é«˜æ–¯ï¼Œå…·æœ‰è‡ªå·±ç‹¬ç‰¹çš„å‚æ•°ï¼Œè¿™äº›å‚æ•°é’ˆå¯¹æ¯ä¸ªåœºæ™¯è¿›è¡Œæ‹Ÿåˆï¼Œä»¥ä¾¿è¯¥<strong>åœºæ™¯çš„æ¸²æŸ“ä¸å·²çŸ¥çš„æ•°æ®é›†å›¾åƒç´§å¯†åŒ¹é…</strong>ã€‚</p>
<p>æ¯ä¸ª 3D é«˜æ–¯çš„å‚æ•°å¦‚ä¸‹ï¼š</p>
<ul>
<li>å‡å€¼ Î¼ï¼Œ å¯è§£é‡Šä¸ºä½ç½® xã€yã€zï¼›</li>
<li>åæ–¹å·® Î£ï¼›</li>
<li>ä¸é€æ˜åº¦ Ïƒ(ğ›¼)ï¼Œåº”ç”¨ sigmoid å‡½æ•°å°†å‚æ•°æ˜ å°„åˆ° [0, 1] åŒºé—´ï¼›</li>
<li>é¢œè‰²å‚æ•°ï¼Œ(Rã€Gã€B) çš„ 3 ä¸ªå€¼æˆ–çƒè°å‡½æ•° (SH) ç³»æ•°ã€‚</li>
</ul>
<p>é€‰æ‹©3Dé«˜æ–¯ä½œä¸ºåœºæ™¯è¡¨ç¤ºæ˜¯å› ä¸ºï¼š</p>
<ol>
<li>å…·æœ‰å¯å¾®åˆ†ä½“ç§¯è¡¨ç¤ºçš„å±æ€§</li>
<li>éç»“æ„åŒ–å’Œæ˜¾å¼çš„ï¼Œä»¥å…è®¸éå¸¸å¿«é€Ÿçš„æ¸²æŸ“</li>
<li>å¯ä»¥è½»æ¾æŠ•å½±åˆ° 2D splatsï¼Œä»è€Œå®ç°å¿«é€Ÿğ›¼æ··åˆæ¸²æŸ“</li>
</ol>
<blockquote>
<p>ä¹‹å‰çš„ç±»ä¼¼å·¥ä½œä½¿ç”¨å¸¦æ³•çº¿ä¿¡æ¯çš„2Då¹³é¢åœ†ã€‚ä½†SfMæœ‰æ—¶æ˜¯éš¾ä»¥ä¼°è®¡æ¯”è¾ƒå‡†ç¡®çš„æ³•çº¿ä¿¡æ¯ï¼Œå› æ­¤ç»™è¿™äº›æ–¹æ³•å¸¦æ¥çš„å›°éš¾ã€‚</p>
</blockquote>
<p>æœ¬æ–‡ä½¿ç”¨çš„3Dé«˜æ–¯ï¼Œç”±ä¸–ç•Œåæ ‡ç³»ä¸‹çš„3Dåæ–¹å·®çŸ©é˜µå’Œä¸­å¿ƒä½ç½®æ¥æè¿°ã€‚ä¸éœ€è¦åŒ…å«æ³•çº¿ä¿¡æ¯ã€‚</p>
<h5 id="åæ–¹å·®çŸ©é˜µ"><a class="header" href="#åæ–¹å·®çŸ©é˜µ">åæ–¹å·®çŸ©é˜µ</a></h5>
<p>åæ–¹å·®æ˜¯å„å‘å¼‚æ€§çš„ï¼Œè¿™æ„å‘³ç€ 3D ç‚¹å¯ä»¥æ˜¯æ²¿ç©ºé—´ä¸­ä»»æ„æ–¹å‘æ—‹è½¬å’Œæ‹‰ä¼¸çš„æ¤­åœ†ä½“ã€‚éœ€è¦ç”¨ 9 ä¸ªå‚æ•°æ¥è¡¨ç¤ºåæ–¹å·®çŸ©é˜µã€‚</p>
<blockquote>
<p>è¿™ç§å„å‘å¼‚æ€§åæ–¹å·®çš„è¡¨ç¤ºï¼ˆé€‚åˆä¼˜åŒ–ï¼‰å…è®¸æˆ‘ä»¬ä¼˜åŒ– 3D é«˜æ–¯ä»¥é€‚åº”æ•è·åœºæ™¯ä¸­ä¸åŒå½¢çŠ¶çš„å‡ ä½•å½¢çŠ¶ï¼Œä»è€Œäº§ç”Ÿç›¸å½“ç´§å‡‘çš„è¡¨ç¤ºã€‚å›¾ 3 è¯´æ˜äº†è¿™ç§æƒ…å†µã€‚</p>
</blockquote>
<p>åæ–¹å·®çŸ©é˜µæ˜¯éœ€è¦è¢«ä¼˜åŒ–çš„å‚æ•°ä¹‹ä¸€ï¼Œä½†æ˜¯ä¸èƒ½ç›´æ¥ä¼˜åŒ–è¿™æ ·çš„åæ–¹å·®çŸ©é˜µã€‚<br />
<strong>ä¼˜åŒ–è¿‡ç¨‹ä¸­å¿…é¡»ä¿è¯åæ–¹å·®çŸ©é˜µæ˜¯åŠæ­£å®šçš„</strong>ï¼Œä½†æ¢¯åº¦ä¸‹é™çš„ä¼˜åŒ–æ–¹æ³•ä¼šç ´ååæ–¹å·®çŸ©é˜µçš„çš„åŠæ­£å®šæ€§ã€‚å› æ­¤ï¼ŒæŠŠåæ–¹å·®çŸ©é˜µåˆ†è§£ä¸ºï¼š</p>
<p>$$
\Sigma = RSS^\top R^\top
$$</p>
<p>è¿™ç§å› å¼åˆ†è§£ç§°ä¸ºåæ–¹å·®çŸ©é˜µçš„ç‰¹å¾åˆ†è§£ï¼Œå…¶ä¸­ï¼š</p>
<ul>
<li>S æ˜¯ä¸€ä¸ªå¯¹è§’ç¼©æ”¾çŸ©é˜µï¼Œå…·æœ‰ 3 ä¸ªç¼©æ”¾å‚æ•°ï¼›</li>
<li>R æ˜¯ä¸€ä¸ª 3x3 æ—‹è½¬çŸ©é˜µï¼Œç”¨ 4 ä¸ªå››å…ƒæ•°è¡¨ç¤ºã€‚</li>
</ul>
<p>Så’ŒRåˆ†åˆ«å­˜å‚¨å’Œä¼˜åŒ–ã€‚</p>
<pre><code class="language-python">def strip_symmetric(L):
    uncertainty = torch.zeros((L.shape[0], 6), dtype=torch.float, device=&quot;cuda&quot;)

    uncertainty[:, 0] = L[:, 0, 0]
    uncertainty[:, 1] = L[:, 0, 1]
    uncertainty[:, 2] = L[:, 0, 2]
    uncertainty[:, 3] = L[:, 1, 1]
    uncertainty[:, 4] = L[:, 1, 2]
    uncertainty[:, 5] = L[:, 2, 2]
    return uncertainty

def build_scaling_rotation(s, r):
    L = torch.zeros((s.shape[0], 3, 3), dtype=torch.float, device=&quot;cuda&quot;)
    R = build_rotation(r)

    L[:,0,0] = s[:,0]
    L[:,1,1] = s[:,1]
    L[:,2,2] = s[:,2]

    L = R @ L
    return L

# scalingå’Œrotationæ˜¯ä¼˜åŒ–å¥½çš„Så’ŒR
def build_covariance_from_scaling_rotation(scaling, scaling_modifier, rotation):
    # æ„é€ ä¸€ä¸ªåŒ…å«ç¼©æ”¾scaleå’Œæ—‹è½¬rotationçš„å˜æ¢çŸ©é˜µ
    L = build_scaling_rotation(scaling_modifier * scaling, rotation)
    actual_covariance = L @ L.transpose(1, 2)
    # ç”±äºactual_covarianceæ˜¯å¯¹ç§°çŸ©é˜µï¼Œåªéœ€è¦å­˜ä¸€åŠå°±å¯ä»¥äº†ï¼Œå‚æ•°å‡å°‘åˆ°6
    symm = strip_symmetric(actual_covariance)
    return symm
</code></pre>
<p>æ¸²æŸ“æ—¶ï¼ŒæŠŠç¼©æ”¾å’Œæ—‹è½¬å†åˆæˆåæ–¹å·®çŸ©é˜µï¼š</p>
<pre><code class="language-c++">// Forward method for converting scale and rotation properties of each
// Gaussian to a 3D covariance matrix in world space. Also takes care
// of quaternion normalization.
__device__ void computeCov3D(
	const glm::vec3 scale, // è¡¨ç¤ºç¼©æ”¾çš„ä¸‰ç»´å‘é‡
	float mod, // å¯¹åº”gaussian_renderer/__init__.pyä¸­çš„scaling_modifier
	const glm::vec4 rot, // è¡¨ç¤ºæ—‹è½¬çš„å››å…ƒæ•°
	float* cov3D) // ç»“æœï¼šä¸‰ç»´åæ–¹å·®çŸ©é˜µ
{
	// Create scaling matrix
	glm::mat3 S = glm::mat3(1.0f);
	S[0][0] = mod * scale.x;
	S[1][1] = mod * scale.y;
	S[2][2] = mod * scale.z;

	// Normalize quaternion to get valid rotation
	glm::vec4 q = rot;// / glm::length(rot);
	float r = q.x;
	float x = q.y;
	float y = q.z;
	float z = q.w;

	// Compute rotation matrix from quaternion
	glm::mat3 R = glm::mat3(
		1.f - 2.f * (y * y + z * z), 2.f * (x * y - r * z), 2.f * (x * z + r * y),
		2.f * (x * y + r * z), 1.f - 2.f * (x * x + z * z), 2.f * (y * z - r * x),
		2.f * (x * z - r * y), 2.f * (y * z + r * x), 1.f - 2.f * (x * x + y * y)
	);

	glm::mat3 M = S * R;

	// Compute 3D world covariance matrix Sigma
	glm::mat3 Sigma = glm::transpose(M) * M;

	// Covariance is symmetric, only store upper right
	cov3D[0] = Sigma[0][0];
	cov3D[1] = Sigma[0][1];
	cov3D[2] = Sigma[0][2];
	cov3D[3] = Sigma[1][1];
	cov3D[4] = Sigma[1][2];
	cov3D[5] = Sigma[2][2];
}
</code></pre>
<h5 id="é¢œè‰²å‚æ•°"><a class="header" href="#é¢œè‰²å‚æ•°">é¢œè‰²å‚æ•°</a></h5>
<p>é¢œè‰²å‚æ•°å¯ä»¥ç”¨3ä¸ªRGBå€¼æˆ–ä¸€ç»„SHç³»æ•°æ¥è¡¨ç¤ºã€‚</p>
<p>ä¸éœ€è¦è§†è§’ä¾èµ–ç‰¹æ€§æ—¶ï¼Œå¯ä»¥è¿›è¡Œç®€åŒ–ï¼Œé€‰æ‹©ç”¨ 3 ä¸ª RGB å€¼è¡¨ç¤ºé¢œè‰²ã€‚</p>
<p>è§†è§’ä¾èµ–æ€§æ˜¯ä¸€ç§å¾ˆå¥½çš„ç‰¹æ€§ï¼Œå®ƒå¯ä»¥æé«˜æ¸²æŸ“è´¨é‡ï¼Œå› ä¸ºå®ƒå…è®¸æ¨¡å‹è¡¨ç¤ºéæœ—ä¼¯æ•ˆåº”ï¼Œä¾‹å¦‚é‡‘å±è¡¨é¢çš„é•œé¢åå°„ã€‚</p>
<p>è§†è§’ç›¸å…³çš„é¢œè‰²å‚æ•°ï¼Œåˆ™éœ€è¦ä½¿ç”¨SHç³»æ•°è¡¨ç¤ºé¢œè‰²ã€‚<br />
SHæ˜¯ä¸€ç»„å®šä¹‰åœ¨çƒè¡¨é¢çš„æ­£äº¤åŸºï¼Œæ¯ä¸ªå®šä¹‰åœ¨çƒé¢ä¸Šçš„å‡½æ•°éƒ½å¯ä»¥é€šè¿‡SHæ¥è¡¨è¾¾ã€‚</p>
<p><img src="./assets/1_bKNS_UyAOGcQvew-b-pciQ.webp" alt="" /></p>
<p>å®šä¹‰SHåŸºçš„è‡ªç”±åº¦â„“_max å†…ï¼Œå¹¶å‡è®¾æ¯ç§é¢œè‰²ï¼ˆçº¢è‰²ã€ç»¿è‰²å’Œè“è‰²ï¼‰éƒ½æ˜¯å‰ â„“_max ä¸ª SH å‡½æ•°çš„çº¿æ€§ç»„åˆã€‚å¯¹äºæ¯ä¸ª 3D é«˜æ–¯ï¼Œé€šè¿‡å­¦ä¹ å…¶æ­£ç¡®çš„ç³»æ•°ï¼Œä½¿å¾—å½“æˆ‘ä»¬ä»æŸä¸ªæ–¹å‘çœ‹è¿™ä¸ª 3D ç‚¹æ—¶ï¼Œå¾—åˆ°æœ€æ¥è¿‘çœŸå®çš„é¢œè‰²ã€‚</p>
<pre><code class="language-python"># degï¼šçƒååŸºçš„ä¸ªæ•°
# shï¼šä¼˜åŒ–å‡ºçš„SHç³»æ•°
# dirsï¼šç›¸æœºæŒ‡å‘é«˜æ–¯çƒå¿ƒçš„è§†çº¿æ–¹å‘
def eval_sh(deg, sh, dirs):
    &quot;&quot;&quot;
    Evaluate spherical harmonics at unit directions
    using hardcoded SH polynomials.
    Works with torch/np/jnp.
    ... Can be 0 or more batch dimensions.
    Args:
        deg: int SH deg. Currently, 0-3 supported
        sh: jnp.ndarray SH coeffs [..., C, (deg + 1) ** 2]
        dirs: jnp.ndarray unit directions [..., 3]
    Returns:
        [..., C]
    &quot;&quot;&quot;
    assert deg &lt;= 4 and deg &gt;= 0
    # ç¬¬lå±‚çš„çƒååŸºéœ€è¦2*i+1ä¸ªç³»æ•°ï¼Œ[0,l]å±‚çƒååŸºå…±éœ€è¦(l+1)**2ä¸ªç³»æ•°
    coeff = (deg + 1) ** 2
    assert sh.shape[-1] &gt;= coeff

    # C0,C1,C2,C3,C4æ˜¯æå‰å®šä¹‰å¥½çš„çƒååŸºï¼Œæ˜¯å®šå€¼ï¼Œä¸éœ€è¦è¢«ä¼˜åŒ–
    result = C0 * sh[..., 0]
    if deg &gt; 0:
        x, y, z = dirs[..., 0:1], dirs[..., 1:2], dirs[..., 2:3]
        result = (result -
                C1 * y * sh[..., 1] +
                C1 * z * sh[..., 2] -
                C1 * x * sh[..., 3])

        if deg &gt; 1:
            xx, yy, zz = x * x, y * y, z * z
            xy, yz, xz = x * y, y * z, x * z
            result = (result +
                    C2[0] * xy * sh[..., 4] +
                    C2[1] * yz * sh[..., 5] +
                    C2[2] * (2.0 * zz - xx - yy) * sh[..., 6] +
                    C2[3] * xz * sh[..., 7] +
                    C2[4] * (xx - yy) * sh[..., 8])

            if deg &gt; 2:
                result = (result +
                C3[0] * y * (3 * xx - yy) * sh[..., 9] +
                C3[1] * xy * z * sh[..., 10] +
                C3[2] * y * (4 * zz - xx - yy)* sh[..., 11] +
                C3[3] * z * (2 * zz - 3 * xx - 3 * yy) * sh[..., 12] +
                C3[4] * x * (4 * zz - xx - yy) * sh[..., 13] +
                C3[5] * z * (xx - yy) * sh[..., 14] +
                C3[6] * x * (xx - 3 * yy) * sh[..., 15])

                if deg &gt; 3:
                    result = (result + C4[0] * xy * (xx - yy) * sh[..., 16] +
                            C4[1] * yz * (3 * xx - yy) * sh[..., 17] +
                            C4[2] * xy * (7 * zz - 1) * sh[..., 18] +
                            C4[3] * yz * (7 * zz - 3) * sh[..., 19] +
                            C4[4] * (zz * (35 * zz - 30) + 3) * sh[..., 20] +
                            C4[5] * xz * (7 * zz - 3) * sh[..., 21] +
                            C4[6] * (xx - yy) * (7 * zz - 1) * sh[..., 22] +
                            C4[7] * xz * (xx - 3 * yy) * sh[..., 23] +
                            C4[8] * (xx * (xx - 3 * yy) - yy * (3 * xx - yy)) * sh[..., 24])
    return result
</code></pre>
<h4 id="æ¸²æŸ“"><a class="header" href="#æ¸²æŸ“">æ¸²æŸ“</a></h4>
<h5 id="ä¸€ä¸ªé«˜æ–¯çƒå¯¹ä¸€ä¸ªåƒç´ ç‚¹çš„å½±å“"><a class="header" href="#ä¸€ä¸ªé«˜æ–¯çƒå¯¹ä¸€ä¸ªåƒç´ ç‚¹çš„å½±å“">ä¸€ä¸ªé«˜æ–¯çƒå¯¹ä¸€ä¸ªåƒç´ ç‚¹çš„å½±å“</a></h5>
<p>ç¬¬iä¸ª3Dé«˜æ–¯çƒå¯¹3Dä¸­ä»»æ„ä¸€ç‚¹pçš„å½±å“å®šä¹‰å¦‚ä¸‹ï¼š</p>
<p><img src="./assets/1_JGh_0y3ICNuA6IcnbdnvdA.gif" alt="" /></p>
<blockquote>
<p>è¿™ä¸ªæ–¹ç¨‹å’Œå¤šå…ƒæ­£æ€åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°çš„åŒºåˆ«åœ¨äºï¼Œæ²¡æœ‰åæ–¹å·®å½’ä¸€åŒ–é¡¹ï¼Œä¸”ä½¿ç”¨ç”¨ä¸é€æ˜åº¦æ¥åŠ æƒã€‚
é«˜æ–¯çš„å¦™å¤„åœ¨äºæ¯ä¸ªç‚¹éƒ½æœ‰åŒé‡å½±å“ã€‚ä¸€æ–¹é¢ï¼Œæ ¹æ®å…¶åæ–¹å·®ï¼Œæ¯ä¸ªç‚¹å®é™…ä¸Šä»£è¡¨äº†ç©ºé—´ä¸­æ¥è¿‘å…¶å‡å€¼çš„æœ‰é™åŒºåŸŸã€‚å¦ä¸€æ–¹é¢ï¼Œå®ƒå…·æœ‰ç†è®ºä¸Šæ— é™çš„èŒƒå›´ï¼Œè¿™æ„å‘³ç€æ¯ä¸ªé«˜æ–¯å‡½æ•°éƒ½å®šä¹‰åœ¨æ•´ä¸ª 3D ç©ºé—´ä¸­ï¼Œå¹¶ä¸”å¯ä»¥é’ˆå¯¹ä»»ä½•ç‚¹è¿›è¡Œè¯„ä¼°ã€‚è¿™å¾ˆæ£’ï¼Œå› ä¸ºåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œå®ƒå…è®¸æ¢¯åº¦ä»è¿œè·ç¦»æµåŠ¨ã€‚â´</p>
</blockquote>
<h5 id="æ‰€æœ‰é«˜æ–¯çƒå¯¹ä¸€ä¸ªåƒç´ çš„å½±å“"><a class="header" href="#æ‰€æœ‰é«˜æ–¯çƒå¯¹ä¸€ä¸ªåƒç´ çš„å½±å“">æ‰€æœ‰é«˜æ–¯çƒå¯¹ä¸€ä¸ªåƒç´ çš„å½±å“</a></h5>
<p>NeRF å’Œé«˜æ–¯æº…å°„ä½¿ç”¨ç›¸åŒçš„é€ç‚¹ ğ›¼ æ··åˆçš„å›¾åƒå½¢æˆæ¨¡å‹ã€‚</p>
<table><thead><tr><th>Nerf</th><th>3D GS</th></tr></thead><tbody>
<tr><td><img src="./assets/1_dovqzRKuf4Sf324f-_Smjg.webp" alt="" /></td><td><img src="./assets/1_op2L1Cv4fCMHlYFLnbG0tw.webp" alt="" /></td></tr>
</tbody></table>
<p>Nerfçš„å…¬å¼å’Œ3D GSçš„å…¬å¼å‡ ä¹å®Œå…¨ç›¸åŒã€‚å”¯ä¸€çš„åŒºåˆ«åœ¨äºä¸¤è€…ä¹‹é—´å¦‚ä½•è®¡ç®— ğ›¼ã€‚åœ¨é«˜æ–¯æº…å°„ä¸­ï¼Œæ¯ä¸ªåƒç´ çš„èšåˆéƒ½æ˜¯é€šè¿‡æŠ•å½±äºŒç»´é«˜æ–¯çš„æœ‰åºåˆ—è¡¨çš„è´¡çŒ®è¿›è¡Œçš„ã€‚</p>
<blockquote>
<p>è¿™ç§å¾®å°çš„å·®å¼‚åœ¨å®è·µä¸­å˜å¾—æä¸ºé‡è¦ï¼Œå¹¶å¯¼è‡´æ¸²æŸ“é€Ÿåº¦æˆªç„¶ä¸åŒã€‚äº‹å®ä¸Šï¼Œè¿™æ˜¯é«˜æ–¯æº…å°„å®æ—¶æ€§èƒ½çš„åŸºç¡€ã€‚</p>
</blockquote>
<h5 id="åæ ‡ç³»è½¬æ¢"><a class="header" href="#åæ ‡ç³»è½¬æ¢">åæ ‡ç³»è½¬æ¢</a></h5>
<p>3D GSå…¬å¼ä¸­çš„\(f^{2D}\) æ˜¯ f(p) åœ¨ 2D ä¸Šçš„æŠ•å½±ã€‚3D ç‚¹åŠå…¶æŠ•å½±éƒ½æ˜¯å¤šå…ƒé«˜æ–¯å‡½æ•°ï¼Œå› æ­¤ â€œ3D é«˜æ–¯å‡½æ•°å¯¹ 3D ä¸­ä»»æ„ç‚¹çš„å½±å“â€ ä¸ â€œæŠ•å½±çš„ 2D é«˜æ–¯å‡½æ•°å¯¹åšä»»æ„åƒç´ ç‚¹çš„å½±å“â€ å…·æœ‰ç›¸åŒçš„å…¬å¼ã€‚å”¯ä¸€çš„åŒºåˆ«æ˜¯å¿…é¡»ä½¿ç”¨æŠ•å½±åˆ° 2D ä¸­å¹³å‡å€¼ Î¼ å’Œåæ–¹å·® Î£ ï¼Œè¿™ä¸€æ­¥ç§°ä¸º EWA splattingâµ ã€‚</p>
<p>å®šä¹‰ç›¸æœºå†…å‚çŸ©é˜µä¸ºKï¼Œå¤–å‚çŸ©é˜µä¸ºW=[R|t]</p>
<p>2D çš„å‡å€¼ä¸ºï¼š</p>
<p>$$
\mu^{2D} = K((W\mu)/(W\mu)_z)
$$</p>
<p>2Dçš„åæ–¹å·®çŸ©é˜µä¸ºï¼š</p>
<p>$$
\Sigma^{2D} = JW\Sigma J^\top W^\top
$$</p>
<blockquote>
<p>æ–‡ä¸­æåˆ°ä¸€ç§ç®€åŒ–æ–¹æ³•ï¼Œå¯ä»¥æŠŠåæ–¹å·®çŸ©é˜µä» 3 * 3 ç®€åŒ–ä¸º 2 * 2ã€‚</p>
</blockquote>
<pre><code class="language-c++">// Forward version of 2D covariance matrix computation
__device__ float3 computeCov2D(
	const float3&amp; mean, // Gaussianä¸­å¿ƒåæ ‡
	float focal_x, // xæ–¹å‘ç„¦è·
	float focal_y, // yæ–¹å‘ç„¦è·
	float tan_fovx,
	float tan_fovy,
	const float* cov3D, // å·²ç»ç®—å‡ºæ¥çš„ä¸‰ç»´åæ–¹å·®çŸ©é˜µ
	const float* viewmatrix) // W2CçŸ©é˜µ
{
	// The following models the steps outlined by equations 29
	// and 31 in &quot;EWA Splatting&quot; (Zwicker et al., 2002). 
	// Additionally considers aspect / scaling of viewport.
	// Transposes used to account for row-/column-major conventions.
	float3 t = transformPoint4x3(mean, viewmatrix);
		// W2CçŸ©é˜µä¹˜Gaussianä¸­å¿ƒåæ ‡å¾—å…¶åœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„åæ ‡

	const float limx = 1.3f * tan_fovx;
	const float limy = 1.3f * tan_fovy;
	const float txtz = t.x / t.z; // Gaussianä¸­å¿ƒåœ¨åƒå¹³é¢ä¸Šçš„xåæ ‡
	const float tytz = t.y / t.z; // Gaussianä¸­å¿ƒåœ¨åƒå¹³é¢ä¸Šçš„yåæ ‡
	t.x = min(limx, max(-limx, txtz)) * t.z;
	t.y = min(limy, max(-limy, tytz)) * t.z;

	glm::mat3 J = glm::mat3(
		focal_x / t.z, 0.0f, -(focal_x * t.x) / (t.z * t.z),
		0.0f, focal_y / t.z, -(focal_y * t.y) / (t.z * t.z),
		0, 0, 0); // é›…å¯æ¯”çŸ©é˜µï¼ˆç”¨æ³°å‹’å±•å¼€è¿‘ä¼¼ï¼‰

	glm::mat3 W = glm::mat3( // W2CçŸ©é˜µ
		viewmatrix[0], viewmatrix[4], viewmatrix[8],
		viewmatrix[1], viewmatrix[5], viewmatrix[9],
		viewmatrix[2], viewmatrix[6], viewmatrix[10]);

	glm::mat3 T = W * J;

	glm::mat3 Vrk = glm::mat3( // 3Dåæ–¹å·®çŸ©é˜µï¼Œæ˜¯å¯¹ç§°é˜µ
		cov3D[0], cov3D[1], cov3D[2],
		cov3D[1], cov3D[3], cov3D[4],
		cov3D[2], cov3D[4], cov3D[5]);

	glm::mat3 cov = glm::transpose(T) * glm::transpose(Vrk) * T;
		// transpose(J) @ transpose(W) @ Vrk @ W @ J

	// Apply low-pass filter: every Gaussian should be at least
	// one pixel wide/high. Discard 3rd row and column.
	cov[0][0] += 0.3f;
	cov[1][1] += 0.3f;
	return { float(cov[0][0]), float(cov[0][1]), float(cov[1][1]) };
		// åæ–¹å·®çŸ©é˜µæ˜¯å¯¹ç§°çš„ï¼Œåªç”¨å­˜å‚¨ä¸Šä¸‰è§’ï¼Œæ•…åªè¿”å›ä¸‰ä¸ªæ•°
}
</code></pre>
<h4 id="åŠ é€Ÿ"><a class="header" href="#åŠ é€Ÿ">åŠ é€Ÿ</a></h4>
<ol>
<li>å¯¹äºç»™å®šçš„ç›¸æœºï¼Œæ¯ä¸ª 3D ç‚¹çš„ f(p) å¯ä»¥é¢„å…ˆæŠ•å½±åˆ° 2D ä¸­ï¼Œç„¶åå†è¿­ä»£åƒç´ ã€‚é¿å…é‡å¤æŠ•å½±ã€‚</li>
<li>æ²¡æœ‰ç½‘ç»œï¼Œä¸éœ€è¦å¯¹å›¾åƒåšé€åƒç´ çš„æ¨ç†ï¼Œ2D é«˜æ–¯åˆ†å¸ƒç›´æ¥æ··åˆåˆ°å›¾åƒä¸Šã€‚</li>
<li>å°„çº¿ç»è¿‡å“ªäº› 3D ç‚¹æ˜¯ç¡®å®šçš„ï¼Œä¸éœ€é€‰æ‹©ray samplingç­–ç•¥ã€‚</li>
<li>åœ¨ GPU ä¸Šï¼Œä½¿ç”¨å¯å¾®åˆ† CUDA å†…æ ¸çš„è‡ªå®šä¹‰å®ç°ï¼Œæ¯å¸§è¿›è¡Œä¸€æ¬¡é¢„å¤„ç†æ’åºé˜¶æ®µã€‚</li>
</ol>
<blockquote>
<p>ä½¿ç”¨GPUåŠ é€Ÿä»¥åŠä¸ºæŸäº›æ“ä½œæ·»åŠ è‡ªå®šä¹‰ CUDA å†…æ ¸ï¼ŒåŠ é€Ÿæ¸²æŸ“è¿‡ç¨‹</p>
</blockquote>
<h5 id="ç­›é€‰"><a class="header" href="#ç­›é€‰">ç­›é€‰</a></h5>
<p>ç†è®ºä¸Šï¼Œæ¯ä¸ªé«˜æ–¯çƒå¯¹æ‰€æœ‰åƒç´ éƒ½ä¼šæœ‰å½±å“ã€‚ä½†å®é™…ä¸Šï¼Œåœ¨æ¸²æŸ“æŸä¸ªåƒç´ æ—¶ï¼Œä¼šå…ˆè¿‡æ»¤å‡ºç›¸å…³çš„é«˜æ–¯çƒï¼Œå¹¶å¯¹å®ƒä»¬æ’åºï¼ŒæŒ‰ç…§æ·±åº¦é¡ºåºè¿›è¡Œè®¡ç®—ã€‚</p>
<p>åˆ†ç»„ï¼šä½¿ç”¨ç®€å•çš„ 16x16 åƒç´ å›¾å—å®ç°åˆ†ç»„<br />
æ’åºï¼šæŒ‰æ·±åº¦å¯¹ 3D ç‚¹è¿›è¡Œæ’åº</p>
<p><img src="https://caterpillarstudygroup.github.io/ImportantArticles/assets/7b6ce0e0b104d36e8331f86fd4c9ab5a_3_Figure_3_-1437298192.png" alt="" /></p>
<h3 id="é€šè¿‡-3d-é«˜æ–¯è‡ªé€‚åº”å¯†åº¦æ§åˆ¶è¿›è¡Œä¼˜åŒ–"><a class="header" href="#é€šè¿‡-3d-é«˜æ–¯è‡ªé€‚åº”å¯†åº¦æ§åˆ¶è¿›è¡Œä¼˜åŒ–">é€šè¿‡ 3D é«˜æ–¯è‡ªé€‚åº”å¯†åº¦æ§åˆ¶è¿›è¡Œä¼˜åŒ–</a></h3>
<p>ä¼˜åŒ–å‚æ•°ï¼š</p>
<ul>
<li>ä½ç½® ğ‘</li>
</ul>
<blockquote>
<p>å¯¹positionä½¿ç”¨ç±»ä¼¼äº Plenoxels çš„æ ‡å‡†æŒ‡æ•°è¡°å‡è°ƒåº¦æŠ€æœ¯ã€‚</p>
</blockquote>
<ul>
<li>ä¸é€æ˜åº¦ ğ›¼</li>
</ul>
<blockquote>
<p>å¯¹ ğ›¼ ä½¿ç”¨ sigmoid æ¿€æ´»å‡½æ•°å°†å…¶é™åˆ¶åœ¨ [0 âˆ’ 1) èŒƒå›´å†…å¹¶è·å¾—å¹³æ»‘æ¢¯åº¦</p>
</blockquote>
<ul>
<li>åæ–¹å·® Î£</li>
</ul>
<blockquote>
<p><strong>3D é«˜æ–¯åæ–¹å·®å‚æ•°çš„è´¨é‡å¯¹äºè¡¨ç¤ºçš„ç´§å‡‘æ€§è‡³å…³é‡è¦ï¼Œå› ä¸ºå¯ä»¥ç”¨å°‘é‡å¤§çš„å„å‘å¼‚æ€§é«˜æ–¯å‡½æ•°æ•è·å¤§çš„å‡åŒ€åŒºåŸŸã€‚</strong><br />
å‡ºäºç±»ä¼¼çš„åŸå› ï¼Œå¯¹åæ–¹å·®å°ºåº¦ä½¿ç”¨æŒ‡æ•°æ¿€æ´»å‡½æ•°ã€‚</p>
</blockquote>
<ul>
<li>é¢œè‰² ğ‘ çš„ SH ç³»æ•°ï¼Œæˆ–è€…é¢œè‰²</li>
</ul>
<p><strong>è¿™äº›å‚æ•°çš„ä¼˜åŒ–ä¸æ§åˆ¶é«˜æ–¯å¯†åº¦çš„æ­¥éª¤äº¤ç»‡åœ¨ä¸€èµ·</strong>ï¼Œä»¥æ›´å¥½åœ°è¡¨ç¤ºåœºæ™¯ã€‚</p>
<h4 id="åˆå§‹åŒ–"><a class="header" href="#åˆå§‹åŒ–">åˆå§‹åŒ–</a></h4>
<p>åˆå§‹åŒ–æ˜¯æŒ‡åœ¨è®­ç»ƒå¼€å§‹æ—¶è®¾ç½®çš„ 3D ç‚¹çš„å‚æ•°ã€‚</p>
<p>å¯¹äº<strong>ç‚¹ä½ç½®ï¼ˆå‡å€¼ï¼‰</strong>ï¼Œä½œè€…å»ºè®®ä½¿ç”¨ç”± SfMï¼ˆè¿åŠ¨ç»“æ„ï¼‰ç”Ÿæˆçš„ç‚¹äº‘ã€‚å› ä¸ºå¯¹äºä»»ä½• 3D é‡å»ºï¼Œæ— è®ºæ˜¯ä½¿ç”¨ GSã€NeRF è¿˜æ˜¯æ›´ç»å…¸çš„æ–¹æ³•ï¼Œéƒ½å¿…é¡»çŸ¥é“ç›¸æœºçŸ©é˜µï¼Œå› æ­¤éƒ½ä¼šéœ€è¦è¿è¡Œ SfM æ¥â€‹â€‹è·å–è¿™äº›çŸ©é˜µã€‚SfM ä¼šäº§ç”Ÿç¨€ç–ç‚¹äº‘ä½œä¸ºå‰¯äº§å“ï¼Œä¸ºä»€ä¹ˆä¸å°†å…¶ç”¨äºåˆå§‹åŒ–å‘¢ï¼Ÿå½“ç”±äºæŸç§åŸå› æ— æ³•è·å¾—ç‚¹äº‘æ—¶ï¼Œå¯ä»¥ä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼Œä½†å¯èƒ½ä¼šæŸå¤±æœ€ç»ˆé‡å»ºè´¨é‡ã€‚<br />
<strong>åæ–¹å·®</strong>è¢«åˆå§‹åŒ–ä¸ºå„å‘åŒæ€§ï¼Œå³åŠå¾„ä¸º ä»çƒä½“meanå¼€å§‹åˆ°ç›¸é‚»ç‚¹çš„å¹³å‡è·ç¦»ï¼Œè¿™æ · 3D ä¸–ç•Œå°±å¯ä»¥è¢«å¾ˆå¥½åœ°è¦†ç›–ï¼Œæ²¡æœ‰â€œæ´â€ã€‚</p>
<pre><code class="language-python">def create_from_pcd(self, pcd : BasicPointCloud, spatial_lr_scale : float):
    self.spatial_lr_scale = spatial_lr_scale
    # ä½ç½®åˆå§‹åŒ–
    fused_point_cloud = torch.tensor(np.asarray(pcd.points)).float().cuda()
    # é¢œè‰²åˆå§‹åŒ–
    fused_color = RGB2SH(torch.tensor(np.asarray(pcd.colors)).float().cuda())
    features = torch.zeros((fused_color.shape[0], 3, (self.max_sh_degree + 1) ** 2)).float().cuda()
    features[:, :3, 0 ] = fused_color
    features[:, 3:, 1:] = 0.0

    print(&quot;Number of points at initialisation : &quot;, fused_point_cloud.shape[0])
    # åæ–¹å·®scaleåˆå§‹åŒ–
    dist2 = torch.clamp_min(distCUDA2(torch.from_numpy(np.asarray(pcd.points)).float().cuda()), 0.0000001)
    scales = torch.log(torch.sqrt(dist2))[...,None].repeat(1, 3)
    # åæ–¹æ³•rotationåˆå§‹åŒ–
    rots = torch.zeros((fused_point_cloud.shape[0], 4), device=&quot;cuda&quot;)
    rots[:, 0] = 1
    # ä¸é€æ˜åº¦åˆå§‹åŒ–
    opacities = inverse_sigmoid(0.1 * torch.ones((fused_point_cloud.shape[0], 1), dtype=torch.float, device=&quot;cuda&quot;))

    self._xyz = nn.Parameter(fused_point_cloud.requires_grad_(True))
    self._features_dc = nn.Parameter(features[:,:,0:1].transpose(1, 2).contiguous().requires_grad_(True))
    self._features_rest = nn.Parameter(features[:,:,1:].transpose(1, 2).contiguous().requires_grad_(True))
    self._scaling = nn.Parameter(scales.requires_grad_(True))
    self._rotation = nn.Parameter(rots.requires_grad_(True))
    self._opacity = nn.Parameter(opacities.requires_grad_(True))
    self.max_radii2D = torch.zeros((self.get_xyz.shape[0]), device=&quot;cuda&quot;)
</code></pre>
<h4 id="ä¼˜åŒ–"><a class="header" href="#ä¼˜åŒ–">ä¼˜åŒ–</a></h4>
<ol>
<li>åœ°é¢çœŸå®è§†å›¾å’Œå½“å‰æ¸²æŸ“ä¹‹é—´çš„ L1 Loss</li>
</ol>
<pre><code class="language-python">def l1_loss(network_output, gt):
    return torch.abs((network_output - gt)).mean()
</code></pre>
<ol start="2">
<li>D-SSIMï¼šç»“æ„å·®å¼‚æŒ‡æ•°æµ‹é‡</li>
</ol>
<pre><code class="language-python">def _ssim(img1, img2, window, window_size, channel, size_average=True):
    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)
    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)

    mu1_sq = mu1.pow(2)
    mu2_sq = mu2.pow(2)
    mu1_mu2 = mu1 * mu2

    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq
    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq
    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2

    C1 = 0.01 ** 2
    C2 = 0.03 ** 2

    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))

    if size_average:
        return ssim_map.mean()
    else:
        return ssim_map.mean(1).mean(1).mean(1)
</code></pre>
<h4 id="é«˜æ–¯è‡ªé€‚åº”æ§åˆ¶"><a class="header" href="#é«˜æ–¯è‡ªé€‚åº”æ§åˆ¶">é«˜æ–¯è‡ªé€‚åº”æ§åˆ¶</a></h4>
<p>ç›®çš„ï¼šè§£å†³é‡å»ºä¸è¶³å’Œè¿‡åº¦é‡å»ºçš„é—®é¢˜<br />
åŸå› ï¼šSGD æœ¬èº«åªèƒ½è°ƒæ•´ç°æœ‰çš„ç‚¹ã€‚ä½†åœ¨å®Œå…¨æ²¡æœ‰ç‚¹ï¼ˆé‡å»ºä¸è¶³ï¼‰æˆ–ç‚¹å¤ªå¤šï¼ˆè¿‡åº¦é‡å»ºï¼‰çš„åŒºåŸŸä¸­ï¼Œå®ƒå¾ˆéš¾æ‰¾åˆ°å¥½çš„å‚æ•°ã€‚è¿™æ—¶å°±éœ€è¦è‡ªé€‚åº”è‡´å¯†åŒ–ã€‚<br />
é¢‘ç‡ï¼šåœ¨è®­ç»ƒæœŸé—´å¶å°”å¯åŠ¨ä¸€æ¬¡ï¼Œæ¯”å¦‚æ¯ 100 ä¸ª SGD æ­¥<br />
æ–¹æ³•ï¼šï¼Œåˆ†å‰²å…·æœ‰å¤§æ¢¯åº¦çš„ç‚¹ï¼ˆå›¾ 8ï¼‰å¹¶åˆ é™¤å·²ç»æ”¶æ•›åˆ°éå¸¸ä½çš„ Î± å€¼çš„ç‚¹ï¼ˆå¦‚æœä¸€ä¸ªç‚¹æ˜¯å¦‚æ­¤é€æ˜ï¼Œä¸ºä»€ä¹ˆè¦ä¿ç•™å®ƒï¼Ÿï¼‰ã€‚</p>
<p><img src="./assets/90c87fe420b7f068f6ef682c1ee5ed26_5_Figure_4_1909201227.png" alt="" /></p>
<p>å…·ä½“ç­–ç•¥ä¸ºï¼š</p>
<ol>
<li>å½“æ£€æµ‹åˆ°è§†å›¾ç©ºé—´ä½ç½®æ¢¯åº¦è¾ƒå¤§æ—¶ï¼Œå¢åŠ é«˜æ–¯å¯†åº¦</li>
</ol>
<blockquote>
<p>å¯¹äºé‡å»ºä¸è¶³æˆ–è¿‡åº¦é‡å»ºï¼Œè¿™ä¸¤è€…éƒ½æœ‰å¾ˆå¤§çš„è§†å›¾ç©ºé—´ä½ç½®æ¢¯åº¦ã€‚ç›´è§‚ä¸Šï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºå®ƒä»¬å¯¹åº”äºå°šæœªå¾ˆå¥½é‡å»ºçš„åŒºåŸŸï¼Œå¹¶ä¸”ä¼˜åŒ–å°è¯•ç§»åŠ¨é«˜æ–¯æ¥çº æ­£è¿™ä¸€ç‚¹ã€‚</p>
</blockquote>
<ol start="2">
<li>å¯¹äºé‡å»ºåŒºåŸŸä¸­çš„å°é«˜æ–¯ï¼Œå¦‚æœéœ€è¦åˆ›å»ºçš„æ–°çš„å‡ ä½•å½¢çŠ¶ï¼Œæœ€å¥½é€šè¿‡ç®€å•åœ°åˆ›å»ºç›¸åŒå¤§å°çš„å‰¯æœ¬å¹¶å°†å…¶æ²¿ä½ç½®æ¢¯åº¦çš„æ–¹å‘ç§»åŠ¨æ¥å…‹éš†é«˜æ–¯ã€‚</li>
</ol>
<pre><code class="language-python">def densify_and_clone(self, grads, grad_threshold, scene_extent):
    # Extract points that satisfy the gradient condition
    selected_pts_mask = torch.where(torch.norm(grads, dim=-1) &gt;= grad_threshold, True, False)
    selected_pts_mask = torch.logical_and(selected_pts_mask,
                                            torch.max(self.get_scaling, dim=1).values &lt;= self.percent_dense*scene_extent)
    # æå–å‡ºå¤§äºé˜ˆå€¼`grad_threshold`ä¸”ç¼©æ”¾å‚æ•°è¾ƒå°ï¼ˆå°äºself.percent_dense * scene_extentï¼‰çš„Gaussiansï¼Œåœ¨ä¸‹é¢è¿›è¡Œå…‹éš†
    
    new_xyz = self._xyz[selected_pts_mask]
    new_features_dc = self._features_dc[selected_pts_mask]
    new_features_rest = self._features_rest[selected_pts_mask]
    new_opacities = self._opacity[selected_pts_mask]
    new_scaling = self._scaling[selected_pts_mask]
    new_rotation = self._rotation[selected_pts_mask]

    self.densification_postfix(new_xyz, new_features_dc, new_features_rest, new_opacities, new_scaling, new_rotation)
</code></pre>
<ol start="3">
<li>æœ‰é«˜æ–¹å·®çš„åŒºåŸŸä¸­çš„å¤§é«˜æ–¯éœ€è¦è¢«åˆ†å‰²æˆæ›´å°çš„é«˜æ–¯ã€‚æˆ‘ä»¬ç”¨ä¸¤ä¸ªæ–°çš„é«˜æ–¯å‡½æ•°æ›¿æ¢è¿™äº›é«˜æ–¯å‡½æ•°ï¼Œå¹¶å°†å®ƒä»¬çš„å°ºåº¦é™¤ä»¥æˆ‘ä»¬é€šè¿‡å®éªŒç¡®å®šçš„å› å­ ğœ™ = 1.6ã€‚æˆ‘ä»¬è¿˜é€šè¿‡ä½¿ç”¨åŸå§‹ 3D é«˜æ–¯ä½œä¸º PDF è¿›è¡Œé‡‡æ ·æ¥åˆå§‹åŒ–å®ƒä»¬çš„ä½ç½®ã€‚</li>
</ol>
<blockquote>
<p>å…‹éš†é«˜æ–¯ä¸åˆ†å‰²é«˜æ–¯çš„åŒºåˆ«åœ¨äºï¼Œå‰è€…ä¼šå¢åŠ ç³»ç»Ÿæ€»ä½“ç§¯å’Œé«˜æ–¯æ•°é‡ï¼Œè€Œåè€…åœ¨ä¿ç•™æ€»ä½“ç§¯ä½†å¢åŠ é«˜æ–¯æ•°é‡ã€‚</p>
</blockquote>
<pre><code class="language-python">def densify_and_split(self, grads, grad_threshold, scene_extent, N=2):
    n_init_points = self.get_xyz.shape[0]
    # Extract points that satisfy the gradient condition
    padded_grad = torch.zeros((n_init_points), device=&quot;cuda&quot;)
    padded_grad[:grads.shape[0]] = grads.squeeze()
    selected_pts_mask = torch.where(padded_grad &gt;= grad_threshold, True, False)
    selected_pts_mask = torch.logical_and(selected_pts_mask,
                                            torch.max(self.get_scaling, dim=1).values &gt; self.percent_dense*scene_extent)
    '''
    è¢«åˆ†è£‚çš„Gaussiansæ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ï¼š
    1. ï¼ˆå¹³å‡ï¼‰æ¢¯åº¦è¿‡å¤§ï¼›
    2. åœ¨æŸä¸ªæ–¹å‘çš„æœ€å¤§ç¼©æ”¾å¤§äºä¸€ä¸ªé˜ˆå€¼ã€‚
    å‚ç…§è®ºæ–‡5.2èŠ‚â€œOn the other hand...â€ä¸€æ®µï¼Œå¤§Gaussianè¢«åˆ†è£‚æˆä¸¤ä¸ªå°Gaussiansï¼Œ
    å…¶æ”¾ç¼©è¢«é™¤ä»¥Ï†=1.6ï¼Œä¸”ä½ç½®æ˜¯ä»¥åŸå…ˆçš„å¤§Gaussianä½œä¸ºæ¦‚ç‡å¯†åº¦å‡½æ•°è¿›è¡Œé‡‡æ ·çš„ã€‚
    '''

    stds = self.get_scaling[selected_pts_mask].repeat(N,1)
    means = torch.zeros((stds.size(0), 3),device=&quot;cuda&quot;)
    samples = torch.normal(mean=means, std=stds)
    rots = build_rotation(self._rotation[selected_pts_mask]).repeat(N,1,1)
    new_xyz = torch.bmm(rots, samples.unsqueeze(-1)).squeeze(-1) + self.get_xyz[selected_pts_mask].repeat(N, 1)
    # ç®—å‡ºéšæœºé‡‡æ ·å‡ºæ¥çš„æ–°åæ ‡
    # bmm: batch matrix-matrix product
    new_scaling = self.scaling_inverse_activation(self.get_scaling[selected_pts_mask].repeat(N,1) / (0.8*N))
    new_rotation = self._rotation[selected_pts_mask].repeat(N,1)
    new_features_dc = self._features_dc[selected_pts_mask].repeat(N,1,1)
    new_features_rest = self._features_rest[selected_pts_mask].repeat(N,1,1)
    new_opacity = self._opacity[selected_pts_mask].repeat(N,1)

    self.densification_postfix(new_xyz, new_features_dc, new_features_rest, new_opacity, new_scaling, new_rotation)

    prune_filter = torch.cat((selected_pts_mask, torch.zeros(N * selected_pts_mask.sum(), device=&quot;cuda&quot;, dtype=bool)))
    self.prune_points(prune_filter)
</code></pre>
<ol start="6">
<li>ä¸å…¶ä»–ä½“ç§¯è¡¨ç¤ºç±»ä¼¼ï¼Œä¼˜åŒ–ç»“æœå¯èƒ½è¢«é è¿‘æ‘„åƒæœºçš„æµ®åŠ¨ä½“çš„å¹²æ‰°ã€‚åœ¨é«˜æ–¯æ²‰æµ¸ä¸­ï¼Œè¿™ç§å¹²æ‰°ä¼šå¯¼è‡´é«˜æ–¯å¯†åº¦çš„ä¸åˆç†å¢åŠ ã€‚<br />
ç¼“å’Œé«˜æ–¯æ•°é‡å¢åŠ çš„æœ‰æ•ˆæ–¹æ³•æ˜¯:<br />
(1) æ¯éš” ğ‘ = 3000 è¿­ä»£å°† ğ›¼ å€¼è®¾ç½®ä¸ºæ¥è¿‘äºé›¶ã€‚ç„¶åï¼Œä¼˜åŒ–ä¼šåœ¨éœ€è¦æ—¶å¢åŠ é«˜æ–¯å‡½æ•°çš„ ğ›¼ï¼ŒåŒæ—¶åˆ é™¤ ğ›¼ å°äº ğœ–ğ›¼ çš„é«˜æ–¯å‡½æ•°ï¼Œå¦‚ä¸Šæ‰€è¿°ã€‚é«˜æ–¯å¯èƒ½ä¼šç¼©å°æˆ–å¢é•¿ï¼Œå¹¶ä¸”ä¸å…¶ä»–é«˜æ–¯æœ‰ç›¸å½“å¤§çš„é‡å <br />
(2) å®šæœŸåˆ é™¤é€æ˜çš„æˆ–è€…éå¸¸å¤§çš„é«˜æ–¯ã€‚<br />
è¯¥ç­–ç•¥å¯ä»¥æ€»ä½“ä¸Šå¾ˆå¥½åœ°æ§åˆ¶é«˜æ–¯æ€»æ•°ã€‚</li>
</ol>
<pre><code class="language-python"># æ¥ä¸‹æ¥ç§»é™¤ä¸€äº›Gaussiansï¼Œå®ƒä»¬æ»¡è¶³ä¸‹åˆ—è¦æ±‚ä¸­çš„ä¸€ä¸ªï¼š
# 1. æ¥è¿‘é€æ˜ï¼ˆä¸é€æ˜åº¦å°äºmin_opacityï¼‰
# 2. åœ¨æŸä¸ªç›¸æœºè§†é‡é‡Œå‡ºç°è¿‡çš„æœ€å¤§2DåŠå¾„å¤§äºå±å¹•ï¼ˆåƒå¹³é¢ï¼‰å¤§å°
# 3. åœ¨æŸä¸ªæ–¹å‘çš„æœ€å¤§ç¼©æ”¾å¤§äº0.1 * extentï¼ˆä¹Ÿå°±æ˜¯è¯´å¾ˆé•¿çš„é•¿æ¡å½¢ä¹Ÿæ˜¯ä¼šè¢«ç§»é™¤çš„ï¼‰
prune_mask = (self.get_opacity &lt; min_opacity).squeeze()
if max_screen_size:
    big_points_vs = self.max_radii2D &gt; max_screen_size # vs = view space?
    big_points_ws = self.get_scaling.max(dim=1).values &gt; 0.1 * extent
    prune_mask = torch.logical_or(torch.logical_or(prune_mask, big_points_vs), big_points_ws) # ws = world space?
self.prune_points(prune_mask)
</code></pre>
<h3 id="é«˜æ–¯å¿«é€Ÿå¯å¾®å…‰æ …åŒ–å™¨"><a class="header" href="#é«˜æ–¯å¿«é€Ÿå¯å¾®å…‰æ …åŒ–å™¨">é«˜æ–¯å¿«é€Ÿå¯å¾®å…‰æ …åŒ–å™¨</a></h3>
<p>ç›®æ ‡ï¼š<br />
å¯¹æ‰€æœ‰é«˜æ–¯è¿›è¡Œå¿«é€Ÿæ•´ä½“æ¸²æŸ“ã€å¿«é€Ÿæ’åºï¼Œè¿‘ä¼¼ ğ›¼ æ··åˆï¼ˆåŒ…æ‹¬å„å‘å¼‚æ€§ splatï¼‰ï¼Œè€Œä¸éœ€è¦é™åˆ¶é«˜æ–¯çš„æ•°é‡ã€‚</p>
<p>æœ¬æ–‡ä¸ºé«˜æ–¯å›¾è®¾è®¡äº†ä¸€ä¸ªåŸºäºå›¾å—çš„å…‰æ …åŒ–å™¨ï¼Œå…¶ç‰¹ç‚¹ä¸ºï¼š</p>
<ol>
<li>ä¸€æ¬¡å¯¹æ•´ä¸ªå›¾åƒçš„åŸºå…ƒè¿›è¡Œé¢„æ’åº</li>
<li>å…è®¸åœ¨ä»»æ„æ•°é‡çš„æ··åˆé«˜æ–¯ä¸Šè¿›è¡Œæœ‰æ•ˆçš„åå‘ä¼ æ’­ï¼Œå¹¶ä¸”ï¼ˆå…‰æ …åŒ–å™¨çš„ï¼‰é™„åŠ å†…å­˜æ¶ˆè€—ä½ï¼Œæ¯ä¸ªåƒç´ åªéœ€è¦æ’å®šçš„å¼€é”€ã€‚</li>
<li>å…‰æ …åŒ–pipelineæ˜¯å®Œå…¨å¯å¾®åˆ†çš„</li>
<li>è€ƒè™‘åˆ° 2D æŠ•å½±ï¼ˆç¬¬ 4 èŠ‚ï¼‰ï¼Œå…‰æ …åŒ–å™¨å¯ä»¥å¯¹å„å‘å¼‚æ€§ splats è¿›è¡Œå…‰æ …åŒ–ã€‚</li>
</ol>
<p>å…·ä½“æ­¥éª¤ä¸ºï¼š</p>
<ol>
<li>å°†å±å¹•åˆ†å‰²æˆ 16Ã—16 å—</li>
<li>æ ¹æ®è§†é”¥ä½“å’Œæ¯ä¸ªå—å‰”é™¤ 3D é«˜æ–¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åªä¿ç•™ä¸è§†é”¥ä½“ç›¸äº¤çš„ç½®ä¿¡åŒºé—´ä¸º 99% çš„é«˜æ–¯åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¿æŠ¤å¸¦æ¥ç®€å•åœ°æ‹’ç»æç«¯ä½ç½®å¤„çš„é«˜æ–¯åˆ†å¸ƒï¼ˆå³é‚£äº›å‡å€¼æ¥è¿‘è¿‘å¹³é¢ä¸”è¿œç¦»è§†é”¥ä½“çš„ä½ç½®ï¼‰ï¼Œå› ä¸ºå®ƒä»¬çš„<strong>æŠ•å½± 2D åæ–¹å·®å°†ä¸ç¨³å®š</strong>ã€‚</li>
<li>æ ¹æ®æ¯ä¸ªé«˜æ–¯é‡å çš„å›¾å—æ•°é‡æ¥å®ä¾‹åŒ–å®ƒä»¬ï¼Œå¹¶ä¸ºæ¯ä¸ªå®ä¾‹åˆ†é…ä¸€ä¸ªç»“åˆäº†è§†å›¾ç©ºé—´æ·±åº¦å’Œå›¾å— ID çš„é”®ã€‚</li>
<li>ä½¿ç”¨å•ä¸ªå¿«é€Ÿ GPU åŸºæ•°æ’åºæ ¹æ®è¿™äº›é”®å¯¹é«˜æ–¯è¿›è¡Œæ’åº [Merrill å’Œ Grimshaw 2010]ã€‚è¯·æ³¨æ„ï¼Œ<strong>æ²¡æœ‰é¢å¤–çš„æ¯åƒç´ ç‚¹æ’åºï¼Œæ··åˆæ˜¯åŸºäºæ­¤åˆå§‹æ’åºæ‰§è¡Œçš„</strong>ã€‚å› æ­¤ï¼Œ ğ›¼ æ··åˆåœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½æ˜¯è¿‘ä¼¼çš„ã€‚ç„¶è€Œï¼Œå½“å›¾å—æ¥è¿‘å•ä¸ªåƒç´ çš„å¤§å°æ—¶ï¼Œè¿™äº›è¿‘ä¼¼å€¼å˜å¾—å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚æˆ‘ä»¬å‘ç°è¿™ç§æ–¹å¼<strong>æå¤§åœ°å¢å¼ºäº†è®­ç»ƒå’Œæ¸²æŸ“æ€§èƒ½ï¼Œè€Œä¸ä¼šåœ¨èåˆåœºæ™¯ä¸­äº§ç”Ÿå¯è§çš„ä¼ªå½±</strong>ã€‚</li>
<li>é€šè¿‡è¯†åˆ«æ’åºåæ·±åº¦æœ€å¤§å’Œæœ€å°çš„é«˜æ–¯æ¥ä¸ºæ¯ä¸ªå›¾å—ç”Ÿæˆä¸€ä¸ªåˆ—è¡¨ã€‚</li>
<li>å¯¹äºå…‰æ …åŒ–ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªå›¾å—å¯åŠ¨ä¸€ä¸ªçº¿ç¨‹å—ã€‚æ¯ä¸ªçº¿ç¨‹ï¼š<br />
ï¼ˆ1ï¼‰é¦–å…ˆåä½œåœ°å°†é«˜æ–¯æ•°æ®åŒ…åŠ è½½åˆ°å…±äº«å†…å­˜ä¸­ã€‚<br />
ï¼ˆ2ï¼‰ç„¶åå¯¹äºç»™å®šçš„åƒç´ ï¼Œé€šè¿‡ä»å‰åˆ°åéå†åˆ—è¡¨æ¥ç´¯ç§¯é¢œè‰²å’Œğ›¼å€¼ï¼Œä»è€Œæœ€å¤§åŒ–æ•°æ®åŠ è½½/å…±äº«å’Œå¤„ç†çš„å¹¶è¡Œæ€§å¢ç›Šã€‚<br />
ï¼ˆ3ï¼‰å½“æˆ‘ä»¬è¾¾åˆ°åƒç´ ä¸­çš„ç›®æ ‡é¥±å’Œåº¦ ğ›¼ æ—¶ï¼Œç›¸åº”çš„çº¿ç¨‹å°±ä¼šåœæ­¢ã€‚<br />
æ¯éš”ä¸€æ®µæ—¶é—´ï¼Œå°±ä¼šæŸ¥è¯¢å›¾å—ä¸­çš„çº¿ç¨‹ï¼Œå¹¶ä¸”å½“æ‰€æœ‰åƒç´ éƒ½é¥±å’Œæ—¶ï¼ˆå³ ğ›¼ å˜ä¸º 1ï¼‰ï¼Œæ•´ä¸ªå›¾å—çš„å¤„ç†å°±ä¼šç»ˆæ­¢ã€‚</li>
</ol>
<blockquote>
<p>é™„å½• C ä¸­ç»™å‡ºäº†æ’åºçš„è¯¦ç»†ä¿¡æ¯å’Œæ€»ä½“å…‰æ …åŒ–æ–¹æ³•çš„é«˜çº§æ¦‚è¿°ã€‚</p>
</blockquote>
<h2 id="å®ç°"><a class="header" href="#å®ç°">å®ç°</a></h2>
<h3 id="è®­ç»ƒ"><a class="header" href="#è®­ç»ƒ">è®­ç»ƒ</a></h3>
<pre><code class="language-python">def training(dataset, opt, pipe, testing_iterations, saving_iterations, checkpoint_iterations, checkpoint, debug_from):
    first_iter = 0
    tb_writer = prepare_output_and_logger(dataset)
    gaussians = GaussianModel(dataset.sh_degree)
    scene = Scene(dataset, gaussians)
    gaussians.training_setup(opt)
    if checkpoint:
        (model_params, first_iter) = torch.load(checkpoint)
        gaussians.restore(model_params, opt)

    bg_color = [1, 1, 1] if dataset.white_background else [0, 0, 0]
    background = torch.tensor(bg_color, dtype=torch.float32, device=&quot;cuda&quot;)

    iter_start = torch.cuda.Event(enable_timing = True)
    iter_end = torch.cuda.Event(enable_timing = True)

    viewpoint_stack = None
    ema_loss_for_log = 0.0
    progress_bar = tqdm(range(first_iter, opt.iterations), desc=&quot;Training progress&quot;)
    first_iter += 1
    for iteration in range(first_iter, opt.iterations + 1):        
        iter_start.record()

        gaussians.update_learning_rate(iteration)

        # Every 1000 its we increase the levels of SH up to a maximum degree
        if iteration % 1000 == 0:
            gaussians.oneupSHdegree()

        # Pick a random Camera
        if not viewpoint_stack:
            viewpoint_stack = scene.getTrainCameras().copy()
        viewpoint_cam = viewpoint_stack.pop(randint(0, len(viewpoint_stack)-1))

        # Render
        if (iteration - 1) == debug_from:
            pipe.debug = True

        bg = torch.rand((3), device=&quot;cuda&quot;) if opt.random_background else background

        render_pkg = render(viewpoint_cam, gaussians, pipe, bg)
        image, viewspace_point_tensor, visibility_filter, radii = render_pkg[&quot;render&quot;], render_pkg[&quot;viewspace_points&quot;], render_pkg[&quot;visibility_filter&quot;], render_pkg[&quot;radii&quot;]

        # Loss
        gt_image = viewpoint_cam.original_image.cuda()
        Ll1 = l1_loss(image, gt_image)
        loss = (1.0 - opt.lambda_dssim) * Ll1 + opt.lambda_dssim * (1.0 - ssim(image, gt_image))
        loss.backward()

        iter_end.record()

        with torch.no_grad():
            # Log and save
            training_report(tb_writer, iteration, Ll1, loss, l1_loss, iter_start.elapsed_time(iter_end), testing_iterations, scene, render, (pipe, background))
            if (iteration in saving_iterations):
                print(&quot;\n[ITER {}] Saving Gaussians&quot;.format(iteration))
                scene.save(iteration)

            # Densification
            if iteration &lt; opt.densify_until_iter:
                # Keep track of max radii in image-space for pruning
                gaussians.max_radii2D[visibility_filter] = torch.max(gaussians.max_radii2D[visibility_filter], radii[visibility_filter])
                gaussians.add_densification_stats(viewspace_point_tensor, visibility_filter)

                if iteration &gt; opt.densify_from_iter and iteration % opt.densification_interval == 0:
                    size_threshold = 20 if iteration &gt; opt.opacity_reset_interval else None
                    gaussians.densify_and_prune(opt.densify_grad_threshold, 0.005, scene.cameras_extent, size_threshold)
                
                if iteration % opt.opacity_reset_interval == 0 or (dataset.white_background and iteration == opt.densify_from_iter):
                    gaussians.reset_opacity()

            # Optimizer step
            if iteration &lt; opt.iterations:
                gaussians.optimizer.step()
                gaussians.optimizer.zero_grad(set_to_none = True)

            if (iteration in checkpoint_iterations):
                print(&quot;\n[ITER {}] Saving Checkpoint&quot;.format(iteration))
                torch.save((gaussians.capture(), iteration), scene.model_path + &quot;/chkpnt&quot; + str(iteration) + &quot;.pth&quot;)
</code></pre>
<h3 id="æ¨æ–­"><a class="header" href="#æ¨æ–­">æ¨æ–­</a></h3>
<h4 id="ç›¸æœº"><a class="header" href="#ç›¸æœº">ç›¸æœº</a></h4>
<p>3Dæ˜¯åœ¨ä¸–ç•Œåæ ‡ç³»ä¸‹æ„å»ºçš„ï¼Œè®¾ç½®å¥½ç›¸æœºçš„å†…å‚å’Œå¤–å‚åï¼Œéœ€è¦æŠŠæ‰€æœ‰ä¸–ç•Œåæ ‡ç³»ä¸‹çš„æ•°æ®è½¬æ¢åˆ°ç›¸æœºåæ ‡ç³»ä¸‹ï¼Œå¹¶ä¸”æŠ•å½±åˆ°å±å¹•ä¸Šã€‚</p>
<p>ä»¥ä¸‹æ˜¯æ ¹æ®ç›¸æœºçš„å†…å‚å¤–å‚è®¡ç®—åæ ‡ç³»è½¬æ¢çŸ©é˜µå’ŒæŠ•å½±çŸ©é˜µçš„è¿‡ç¨‹ã€‚</p>
<pre><code class="language-python">class Camera(nn.Module):
    def __init__(self, colmap_id, R, T, FoVx, FoVy, image, gt_alpha_mask,
                 image_name, uid,
                 trans=np.array([0.0, 0.0, 0.0]), scale=1.0, data_device = &quot;cuda&quot;
                 ):
        super(Camera, self).__init__()

        self.uid = uid
        self.colmap_id = colmap_id
        self.R = R # ç›¸æœºåœ¨ä¸–ç•Œåæ ‡ç³»ä¸‹çš„æ—‹è½¬çŸ©é˜µ
        self.T = T # ç›¸æœºåœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„ä½ç½®ã€‚ï¼ˆç›¸æœºåæ ‡ç³»çš„åŸç‚¹ä¸ä¸–ç•Œåæ ‡ç³»ç›¸åŒï¼Œåªæ˜¯ç›¸å·®äº†ä¸€ä¸ªæ—‹è½¬ï¼‰
        self.FoVx = FoVx # xæ–¹å‘è§†åœºè§’
        self.FoVy = FoVy # yæ–¹å‘è§†åœºè§’
        self.image_name = image_name

        try:
            self.data_device = torch.device(data_device)
        except Exception as e:
            print(e)
            print(f&quot;[Warning] Custom device {data_device} failed, fallback to default cuda device&quot; )
            self.data_device = torch.device(&quot;cuda&quot;)

        self.original_image = image.clamp(0.0, 1.0).to(self.data_device) # åŸå§‹å›¾åƒ
        self.image_width = self.original_image.shape[2] # å›¾åƒå®½åº¦
        self.image_height = self.original_image.shape[1] # å›¾åƒé«˜åº¦

        if gt_alpha_mask is not None:
            self.original_image *= gt_alpha_mask.to(self.data_device)
        else:
            self.original_image *= torch.ones((1, self.image_height, self.image_width), device=self.data_device)

		# è·ç¦»ç›¸æœºå¹³é¢znearå’Œzfarä¹‹é—´ä¸”åœ¨è§†é”¥å†…çš„ç‰©ä½“æ‰ä¼šè¢«æ¸²æŸ“
        self.zfar = 100.0 # æœ€è¿œèƒ½çœ‹åˆ°å¤šè¿œ
        self.znear = 0.01 # æœ€è¿‘èƒ½çœ‹åˆ°å¤šè¿‘

        self.trans = trans # ç›¸æœºä¸­å¿ƒçš„å¹³ç§»
        self.scale = scale # ç›¸æœºä¸­å¿ƒåæ ‡çš„ç¼©æ”¾

        # world_2_camera = [[R,T],[0,1]]ï¼Œworld_view_transformæ˜¯world_2_cameraçš„è½¬ç½®
        self.world_view_transform = torch.tensor(getWorld2View2(R, T, trans, scale)).transpose(0, 1).cuda() # ä¸–ç•Œåˆ°ç›¸æœºåæ ‡ç³»çš„å˜æ¢çŸ©é˜µï¼Œ4Ã—4
        # projection matrixçš„å®šä¹‰è§ï¼š
        # https://caterpillarstudygroup.github.io/GAMES101_mdbook/MVP/OrthographicProjection.html
        # https://caterpillarstudygroup.github.io/GAMES101_mdbook/MVP/PerspectiveProjection.html
        # æ­¤å¤„çš„projection_matrixä¹Ÿæ˜¯çœŸå®projection matrixçš„è½¬ç½®
        self.projection_matrix = getProjectionMatrix(znear=self.znear, zfar=self.zfar, fovX=self.FoVx, fovY=self.FoVy).transpose(0,1).cuda() # æŠ•å½±çŸ©é˜µ
        # æ­£ç¡®çš„è®¡ç®—å…¬å¼ä¸ºï¼šmvp = projection * world_2_camera
        # ä½†full_proj_transformæ˜¯mvpçš„è½¬ç½®ï¼Œæ‰€ä»¥æ˜¯world_view_transform * projection_matrix
        self.full_proj_transform = (self.world_view_transform.unsqueeze(0).bmm(self.projection_matrix.unsqueeze(0))).squeeze(0) # ä»ä¸–ç•Œåæ ‡ç³»åˆ°å›¾åƒçš„å˜æ¢çŸ©é˜µ
        # ä¸Šé¢çš„Tæ˜¯ç›¸æœºåœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„ä½ç½®ï¼Œæ­¤å¤„çš„camera centeræ˜¯ç›¸æœºåœ¨ä¸–ç•Œåæ ‡ç³»ä¸‹çš„ä½ç½®ã€‚
        self.camera_center = self.world_view_transform.inverse()[3, :3] # ç›¸æœºåœ¨ä¸–ç•Œåæ ‡ç³»ä¸‹çš„åæ ‡
</code></pre>
<h4 id="pythonæ¸²æŸ“æ¥å£"><a class="header" href="#pythonæ¸²æŸ“æ¥å£">pythonæ¸²æŸ“æ¥å£</a></h4>
<p><strong>viewmatrixå’Œprojmatrixéƒ½å¿…é¡»ä¼ å…¥åŸå§‹çŸ©é˜µçš„é€†çŸ©é˜µï¼Œå› æ­¤pythonçš„çŸ©é˜µå­˜å‚¨æ˜¯è¡Œä¼˜åŒ–çš„ï¼ŒC++çš„çŸ©é˜µå­˜å‚¨æ˜¯åˆ—ä¼˜å…ˆçš„ã€‚æ‰€ä»¥åŒæ—¶çš„çŸ©é˜µå†…å­˜æ•°æ®ï¼Œåœ¨pythoné‡Œå’Œåœ¨c++é‡Œæ˜¯äº’é€†çš„å…³ç³»ã€‚</strong></p>
<pre><code class="language-python">def render(viewpoint_camera, pc : GaussianModel, pipe, bg_color : torch.Tensor, scaling_modifier = 1.0, override_color = None):
    &quot;&quot;&quot;
    Render the scene. 
    viewpoint_camera: FOVï¼Œç”»å¸ƒå¤§å°ã€ç›¸æœºä½ç½®ã€å˜æ¢çŸ©é˜µ
    pc: ç”¨äºè·å–é«˜æ–¯çƒçš„å±æ€§
    pipeï¼šä¸€äº›é…ç½®
    Background tensor (bg_color) must be on GPU!
    &quot;&quot;&quot;
 
    # Create zero tensor. We will use it to make pytorch return gradients of the 2D (screen-space) means
    screenspace_points = torch.zeros_like(pc.get_xyz, dtype=pc.get_xyz.dtype, requires_grad=True, device=&quot;cuda&quot;) + 0
    try:
        screenspace_points.retain_grad()
    except:
        pass

    # Set up rasterization configuration
    tanfovx = math.tan(viewpoint_camera.FoVx * 0.5)
    tanfovy = math.tan(viewpoint_camera.FoVy * 0.5)

    raster_settings = GaussianRasterizationSettings(
        image_height=int(viewpoint_camera.image_height),
        image_width=int(viewpoint_camera.image_width),
        tanfovx=tanfovx,
        tanfovy=tanfovy,
        bg=bg_color,
        scale_modifier=scaling_modifier,
        viewmatrix=viewpoint_camera.world_view_transform, # world to camera
        projmatrix=viewpoint_camera.full_proj_transform, # mvp
        sh_degree=pc.active_sh_degree,
        campos=viewpoint_camera.camera_center, # camera position
        prefiltered=False,
        debug=pipe.debug
    )

    rasterizer = GaussianRasterizer(raster_settings=raster_settings)

    means3D = pc.get_xyz 
    means2D = screenspace_points
    opacity = pc.get_opacity

    # If precomputed 3d covariance is provided, use it. If not, then it will be computed from
    # scaling / rotation by the rasterizer.
    scales = None
    rotations = None
    cov3D_precomp = None
    if pipe.compute_cov3D_python:
        cov3D_precomp = pc.get_covariance(scaling_modifier)
    else:
        scales = pc.get_scaling
        rotations = pc.get_rotation

    # If precomputed colors are provided, use them. Otherwise, if it is desired to precompute colors
    # from SHs in Python, do it. If not, then SH -&gt; RGB conversion will be done by rasterizer.
    shs = None
    colors_precomp = None
    # æ²¡æœ‰é¢„åˆ¶çš„colorï¼Œå°±è®¡ç®—è¿‡color
    if override_color is None:
        # å¦‚æœé¢„æµ‹çš„æ˜¯SHçš„ç³»æ•°ï¼Œåˆ™æ ¹æ®SHè®¡ç®—color
        if pipe.convert_SHs_python:
            shs_view = pc.get_features.transpose(1, 2).view(-1, 3, (pc.max_sh_degree+1)**2)
            dir_pp = (pc.get_xyz - viewpoint_camera.camera_center.repeat(pc.get_features.shape[0], 1))
            dir_pp_normalized = dir_pp/dir_pp.norm(dim=1, keepdim=True)
            sh2rgb = eval_sh(pc.active_sh_degree, shs_view, dir_pp_normalized)
            colors_precomp = torch.clamp_min(sh2rgb + 0.5, 0.0)
        # æˆ–è€…ç›´æ¥é¢„æµ‹color
        else:
            shs = pc.get_features
    else:
        colors_precomp = override_color

    # Rasterize visible Gaussians to image, obtain their radii (on screen). 
    rendered_image, radii = rasterizer(
        means3D = means3D,
        means2D = means2D,
        shs = shs,
        colors_precomp = colors_precomp,
        opacities = opacity,
        scales = scales,
        rotations = rotations,
        cov3D_precomp = cov3D_precomp)

    # Those Gaussians that were frustum culled or had a radius of 0 were not visible.
    # They will be excluded from value updates used in the splitting criteria.
    return {&quot;render&quot;: rendered_image,
            &quot;viewspace_points&quot;: screenspace_points,
            &quot;visibility_filter&quot; : radii &gt; 0,
            &quot;radii&quot;: radii}
</code></pre>
<h4 id="cæ¸²æŸ“æ¥å£"><a class="header" href="#cæ¸²æŸ“æ¥å£">C++æ¸²æŸ“æ¥å£</a></h4>
<p><img src="./assets/a72cc63a000e41da8c749d562dcdd030.png" alt="" /></p>
<pre><code class="language-c++">// Forward rendering procedure for differentiable rasterization
// of Gaussians.
int CudaRasterizer::Rasterizer::forward(
	std::function&lt;char* (size_t)&gt; geometryBuffer,
	std::function&lt;char* (size_t)&gt; binningBuffer,
	std::function&lt;char* (size_t)&gt; imageBuffer,
	/*
		ä¸Šé¢çš„ä¸‰ä¸ªå‚æ•°æ˜¯ç”¨äºåˆ†é…ç¼“å†²åŒºçš„å‡½æ•°ï¼Œ
		åœ¨submodules/diff-gaussian-rasterization/rasterize_points.cuä¸­å®šä¹‰
	*/
	const int P, // Gaussiançš„æ•°é‡
	int D, // å¯¹åº”äºGaussianModel.active_sh_degreeï¼Œæ˜¯çƒè°åº¦æ•°ï¼ˆæœ¬æ–‡å‚è€ƒçš„å­¦ä¹ ç¬”è®°åœ¨è¿™é‡Œæ˜¯é”™è¯¯çš„ï¼‰
	int M, // RGBä¸‰é€šé“çš„çƒè°å‚…é‡Œå¶ç³»æ•°ä¸ªæ•°ï¼Œåº”ç­‰äº3 Ã— (D + 1)Â²ï¼ˆæœ¬æ–‡å‚è€ƒçš„å­¦ä¹ ç¬”è®°åœ¨è¿™é‡Œä¹Ÿæ˜¯é”™è¯¯çš„ï¼‰
	const float* background,
	const int width, int height, // å›¾ç‰‡å®½é«˜
	const float* means3D, // Gaussiansçš„ä¸­å¿ƒåæ ‡
	const float* shs, // çƒè°ç³»æ•°
	const float* colors_precomp, // é¢„å…ˆè®¡ç®—çš„RGBé¢œè‰²
	const float* opacities, // ä¸é€æ˜åº¦
	const float* scales, // ç¼©æ”¾
	const float scale_modifier, // ç¼©æ”¾çš„ä¿®æ­£é¡¹
	const float* rotations, // æ—‹è½¬
	const float* cov3D_precomp, // é¢„å…ˆè®¡ç®—çš„3ç»´åæ–¹å·®çŸ©é˜µ
	const float* viewmatrix, // W2CçŸ©é˜µ
	const float* projmatrix, // æŠ•å½±çŸ©é˜µ
	const float* cam_pos, // ç›¸æœºåæ ‡
	const float tan_fovx, float tan_fovy, // è§†åœºè§’ä¸€åŠçš„æ­£åˆ‡å€¼
	const bool prefiltered,
	float* out_color, // è¾“å‡ºçš„é¢œè‰²
	int* radii, // å„Gaussianåœ¨åƒå¹³é¢ä¸Šç”¨3ÏƒåŸåˆ™æˆªå–åçš„åŠå¾„
	bool debug)
{
	const float focal_y = height / (2.0f * tan_fovy); // yæ–¹å‘çš„ç„¦è·
	const float focal_x = width / (2.0f * tan_fovx); // xæ–¹å‘çš„ç„¦è·
	/*
		æ³¨æ„tan_fov = tan(fov / 2) ï¼ˆè§ä¸Šé¢çš„renderå‡½æ•°ï¼‰ã€‚
		è€Œtan(fov / 2)å°±æ˜¯å›¾åƒå®½/é«˜çš„ä¸€åŠä¸ç„¦è·ä¹‹æ¯”ã€‚
		ä»¥xæ–¹å‘ä¸ºä¾‹ï¼Œtan(fovx / 2) = width / 2 / focal_xï¼Œ
		æ•…focal_x = width / (2 * tan(fovx / 2)) = width / (2 * tan_fovx)ã€‚
	*/

	// ä¸‹é¢åˆå§‹åŒ–ä¸€äº›ç¼“å†²åŒº
	size_t chunk_size = required&lt;GeometryState&gt;(P); // GeometryStateå æ®ç©ºé—´çš„å¤§å°
	char* chunkptr = geometryBuffer(chunk_size);
	GeometryState geomState = GeometryState::fromChunk(chunkptr, P);

	if (radii == nullptr)
	{
		radii = geomState.internal_radii;
	}

	dim3 tile_grid((width + BLOCK_X - 1) / BLOCK_X, (height + BLOCK_Y - 1) / BLOCK_Y, 1);
		// BLOCK_X = BLOCK_Y = 16ï¼Œå‡†å¤‡åˆ†è§£æˆ16Ã—16çš„tilesã€‚
		// ä¹‹æ‰€ä»¥ä¸èƒ½åˆ†è§£æˆæ›´å¤§çš„tilesï¼Œæ˜¯å› ä¸ºå¯¹äºåŒä¸€å¼ å›¾ç‰‡çš„ç¦»å¾—è¾ƒè¿œçš„åƒç´ ç‚¹è€Œè¨€
		// GaussianæŒ‰æ·±åº¦æ’åºçš„ç»“æœå¯èƒ½æ˜¯ä¸åŒçš„ã€‚
		// ï¼ˆæƒ³è±¡ä¸€ä¸‹ä¸¤ä¸ªGaussiansç¦»åƒå¹³é¢å¾ˆè¿‘ï¼Œä¸€ä¸ªé è¿‘å›¾åƒå·¦è¾¹ç¼˜ï¼Œä¸€ä¸ªé è¿‘å³è¾¹ç¼˜ï¼‰
		// dim3æ˜¯CUDAå®šä¹‰çš„å«ä¹‰x,y,zä¸‰ä¸ªæˆå‘˜çš„ä¸‰ç»´unsigned intå‘é‡ç±»ã€‚
		// tile_gridå°±æ˜¯xå’Œyæ–¹å‘ä¸Štileçš„ä¸ªæ•°ã€‚
	dim3 block(BLOCK_X, BLOCK_Y, 1);

	// Dynamically resize image-based auxiliary buffers during training
	size_t img_chunk_size = required&lt;ImageState&gt;(width * height);
	char* img_chunkptr = imageBuffer(img_chunk_size);
	ImageState imgState = ImageState::fromChunk(img_chunkptr, width * height);

	if (NUM_CHANNELS != 3 &amp;&amp; colors_precomp == nullptr)
	{
		throw std::runtime_error(&quot;For non-RGB, provide precomputed Gaussian colors!&quot;);
	}

	// Run preprocessing per-Gaussian (transformation, bounding, conversion of SHs to RGB)
	CHECK_CUDA(FORWARD::preprocess(
		P, D, M,
		means3D,
		(glm::vec3*)scales,
		scale_modifier,
		(glm::vec4*)rotations,
		opacities,
		shs,
		geomState.clamped,
		cov3D_precomp,
		colors_precomp,
		viewmatrix, projmatrix,
		(glm::vec3*)cam_pos,
		width, height,
		focal_x, focal_y,
		tan_fovx, tan_fovy,
		radii,
		geomState.means2D, // GaussianæŠ•å½±åˆ°åƒå¹³é¢ä¸Šçš„ä¸­å¿ƒåæ ‡
		geomState.depths, // Gaussiançš„æ·±åº¦
		geomState.cov3D, // ä¸‰ç»´åæ–¹å·®çŸ©é˜µ
		geomState.rgb, // é¢œè‰²
		geomState.conic_opacity, // æ¤­åœ†äºŒæ¬¡å‹çš„çŸ©é˜µå’Œä¸é€æ˜åº¦çš„æ‰“åŒ…å‘é‡
		tile_grid, // 
		geomState.tiles_touched,
		prefiltered
	), debug) // é¢„å¤„ç†ï¼Œä¸»è¦æ¶‰åŠæŠŠ3Dçš„GaussianæŠ•å½±åˆ°2D

	// Compute prefix sum over full list of touched tile counts by Gaussians
	// E.g., [2, 3, 0, 2, 1] -&gt; [2, 5, 5, 7, 8]
	CHECK_CUDA(cub::DeviceScan::InclusiveSum(geomState.scanning_space, geomState.scan_size, geomState.tiles_touched, geomState.point_offsets, P), debug)
		// è¿™æ­¥æ˜¯ä¸ºduplicateWithKeysåšå‡†å¤‡
		// ï¼ˆè®¡ç®—å‡ºæ¯ä¸ªGaussianå¯¹åº”çš„keyså’Œvaluesåœ¨æ•°ç»„ä¸­å­˜å‚¨çš„èµ·å§‹ä½ç½®ï¼‰

	// Retrieve total number of Gaussian instances to launch and resize aux buffers
	int num_rendered;
	CHECK_CUDA(cudaMemcpy(&amp;num_rendered, geomState.point_offsets + P - 1, sizeof(int), cudaMemcpyDeviceToHost), debug); // ä¸œè¥¿å¡åˆ°GPUé‡Œé¢å»

	size_t binning_chunk_size = required&lt;BinningState&gt;(num_rendered);
	char* binning_chunkptr = binningBuffer(binning_chunk_size);
	BinningState binningState = BinningState::fromChunk(binning_chunkptr, num_rendered);

	// For each instance to be rendered, produce adequate [ tile | depth ] key 
	// and corresponding dublicated Gaussian indices to be sorted
	duplicateWithKeys &lt;&lt; &lt;(P + 255) / 256, 256 &gt;&gt; &gt; (
		P,
		geomState.means2D,
		geomState.depths,
		geomState.point_offsets,
		binningState.point_list_keys_unsorted,
		binningState.point_list_unsorted,
		radii,
		tile_grid) // ç”Ÿæˆæ’åºæ‰€ç”¨çš„keyså’Œvalues
	CHECK_CUDA(, debug)

	int bit = getHigherMsb(tile_grid.x * tile_grid.y);

	// Sort complete list of (duplicated) Gaussian indices by keys
	CHECK_CUDA(cub::DeviceRadixSort::SortPairs(
		binningState.list_sorting_space,
		binningState.sorting_size,
		binningState.point_list_keys_unsorted, binningState.point_list_keys,
		binningState.point_list_unsorted, binningState.point_list,
		num_rendered, 0, 32 + bit), debug)
		// è¿›è¡Œæ’åºï¼ŒæŒ‰keysæ’åºï¼šæ¯ä¸ªtileå¯¹åº”çš„GaussiansæŒ‰æ·±åº¦æ”¾åœ¨ä¸€èµ·ï¼›valueæ˜¯Gaussiançš„ID

	CHECK_CUDA(cudaMemset(imgState.ranges, 0, tile_grid.x * tile_grid.y * sizeof(uint2)), debug);

	// Identify start and end of per-tile workloads in sorted list
	if (num_rendered &gt; 0)
		identifyTileRanges &lt;&lt; &lt;(num_rendered + 255) / 256, 256 &gt;&gt; &gt; (
			num_rendered,
			binningState.point_list_keys,
			imgState.ranges); // è®¡ç®—æ¯ä¸ªtileå¯¹åº”æ’åºè¿‡çš„æ•°ç»„ä¸­çš„å“ªä¸€éƒ¨åˆ†
	CHECK_CUDA(, debug)

	// Let each tile blend its range of Gaussians independently in parallel
	const float* feature_ptr = colors_precomp != nullptr ? colors_precomp : geomState.rgb;
	CHECK_CUDA(FORWARD::render(
		tile_grid, block, // block: æ¯ä¸ªtileçš„å¤§å°
		imgState.ranges,
		binningState.point_list,
		width, height,
		geomState.means2D,
		feature_ptr,
		geomState.conic_opacity,
		imgState.accum_alpha,
		imgState.n_contrib,
		background,
		out_color), debug) // æœ€åï¼Œè¿›è¡Œæ¸²æŸ“

	return num_rendered;
}

</code></pre>
<h2 id="æœ‰æ•ˆ"><a class="header" href="#æœ‰æ•ˆ">æœ‰æ•ˆ</a></h2>
<h2 id="å±€é™æ€§"><a class="header" href="#å±€é™æ€§">å±€é™æ€§</a></h2>
<ol>
<li>åœ¨è§†è§’ä¸å¯è§åŒºåŸŸæœ‰ä¼ªå½±ã€‚è§£å†³æ–¹æ³•ï¼šé€šè¿‡è§„åˆ™å‰”é™¤è¿™äº›ä¼ªå½±ã€‚</li>
<li>ç®€å•çš„å¯è§æ€§ç®—æ³•ï¼Œå¯èƒ½å¯¼è‡´é«˜æ–¯çªç„¶åˆ‡æ¢æ·±åº¦/æ··åˆé¡ºåºã€‚è§£å†³æ–¹æ³•ï¼šå¯ä»¥é€šè¿‡æŠ—é”¯é½¿æ¥è§£å†³ã€‚</li>
<li>æ²¡æœ‰å¯¹æˆ‘ä»¬çš„ä¼˜åŒ–åº”ç”¨ä»»ä½•æ­£åˆ™åŒ–ï¼›è§£å†³æ–¹æ³•ï¼šåŠ å…¥æ­£åˆ™åŒ–å°†æœ‰åŠ©äºå¤„ç†çœ‹ä¸è§çš„åŒºåŸŸå’Œå¼¹å‡ºçš„ä¼ªå½±ã€‚</li>
<li>ä¸€æ¬¡åªèƒ½æ¸²æŸ“ä¸€å¼ å›¾åƒï¼Œä¸èƒ½æ‰¹é‡è¿›è¡Œã€‚</li>
</ol>
<h2 id="éªŒè¯"><a class="header" href="#éªŒè¯">éªŒè¯</a></h2>
<h2 id="å¯å‘"><a class="header" href="#å¯å‘">å¯å‘</a></h2>
<h2 id="é—ç•™é—®é¢˜"><a class="header" href="#é—ç•™é—®é¢˜">é—ç•™é—®é¢˜</a></h2>
<h2 id="å‚è€ƒææ–™"><a class="header" href="#å‚è€ƒææ–™">å‚è€ƒææ–™</a></h2>
<ol>
<li>https://towardsdatascience.com/a-comprehensive-overview-of-gaussian-splatting-e7d570081362</li>
<li>https://caterpillarstudygroup.github.io/ImportantArticles/3D_Gaussian_Splatting.html</li>
<li>æºç è§£è¯»ï¼šhttps://blog.csdn.net/qaqwqaqwq/article/details/136837906</li>
</ol>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="18.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                            <a rel="next" href="16.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="18.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                    <a rel="next" href="16.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="theme/pagetoc.js"></script>
        <script type="text/javascript" src="theme/mermaid.min.js"></script>
        <script type="text/javascript" src="theme/mermaid-init.js"></script>
    </body>
</html>

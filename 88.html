<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>T2m-gpt: Generating human motion from textual descriptions with discrete representations - ReadPapers</title>
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="The note of Papers">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/pagetoc.css">
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><div>ReadPapers</div></li><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="159.html"><strong aria-hidden="true">2.</strong> Seamless Human Motion Composition with Blended Positional Encodings</a></li><li class="chapter-item expanded "><a href="158.html"><strong aria-hidden="true">3.</strong> FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing</a></li><li class="chapter-item expanded "><a href="157.html"><strong aria-hidden="true">4.</strong> Fg-T2M: Fine-Grained Text-Driven Human Motion Generation via Diffusion Model</a></li><li class="chapter-item expanded "><a href="156.html"><strong aria-hidden="true">5.</strong> Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation</a></li><li class="chapter-item expanded "><a href="155.html"><strong aria-hidden="true">6.</strong> StableMoFusion: Towards Robust and Efficient Diffusion-based Motion Generation Framework</a></li><li class="chapter-item expanded "><a href="154.html"><strong aria-hidden="true">7.</strong> EMDM: Efficient Motion Diffusion Model for Fast and High-Quality Motion Generation</a></li><li class="chapter-item expanded "><a href="153.html"><strong aria-hidden="true">8.</strong> Motion Mamba: Efficient and Long Sequence Motion Generation</a></li><li class="chapter-item expanded "><a href="152.html"><strong aria-hidden="true">9.</strong> M2D2M: Multi-Motion Generation from Text with Discrete Diffusion Models</a></li><li class="chapter-item expanded "><a href="151.html"><strong aria-hidden="true">10.</strong> T2LM: Long-Term 3D Human Motion Generation from Multiple Sentences</a></li><li class="chapter-item expanded "><a href="150.html"><strong aria-hidden="true">11.</strong> AttT2M:Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism</a></li><li class="chapter-item expanded "><a href="149.html"><strong aria-hidden="true">12.</strong> BAD: Bidirectional Auto-Regressive Diffusion for Text-to-Motion Generation</a></li><li class="chapter-item expanded "><a href="148.html"><strong aria-hidden="true">13.</strong> MMM: Generative Masked Motion Model</a></li><li class="chapter-item expanded "><a href="147.html"><strong aria-hidden="true">14.</strong> Priority-Centric Human Motion Generation in Discrete Latent Space</a></li><li class="chapter-item expanded "><a href="146.html"><strong aria-hidden="true">15.</strong> AvatarGPT: All-in-One Framework for Motion Understanding, Planning, Generation and Beyond</a></li><li class="chapter-item expanded "><a href="145.html"><strong aria-hidden="true">16.</strong> MotionGPT: Human Motion as a Foreign Language</a></li><li class="chapter-item expanded "><a href="144.html"><strong aria-hidden="true">17.</strong> Action-GPT: Leveraging Large-scale Language Models for Improved and Generalized Action Generation</a></li><li class="chapter-item expanded "><a href="143.html"><strong aria-hidden="true">18.</strong> PoseGPT: Quantization-based 3D Human Motion Generation and Forecasting</a></li><li class="chapter-item expanded "><a href="142.html"><strong aria-hidden="true">19.</strong> Incorporating Physics Principles for Precise Human Motion Prediction</a></li><li class="chapter-item expanded "><a href="141.html"><strong aria-hidden="true">20.</strong> PIMNet: Physics-infused Neural Network for Human Motion Prediction</a></li><li class="chapter-item expanded "><a href="140.html"><strong aria-hidden="true">21.</strong> PhysDiff: Physics-Guided Human Motion Diffusion Model</a></li><li class="chapter-item expanded "><a href="139.html"><strong aria-hidden="true">22.</strong> NRDF: Neural Riemannian Distance Fields for Learning Articulated Pose Priors</a></li><li class="chapter-item expanded "><a href="138.html"><strong aria-hidden="true">23.</strong> Pose-NDF: Modeling Human Pose Manifolds with Neural Distance Fields</a></li><li class="chapter-item expanded "><a href="137.html"><strong aria-hidden="true">24.</strong> Geometric Neural Distance Fields for Learning Human Motion Priors</a></li><li class="chapter-item expanded "><a href="136.html"><strong aria-hidden="true">25.</strong> Character Controllers Using Motion VAEs</a></li><li class="chapter-item expanded "><a href="135.html"><strong aria-hidden="true">26.</strong> Improving Human Motion Plausibility with Body Momentum</a></li><li class="chapter-item expanded "><a href="134.html"><strong aria-hidden="true">27.</strong> MoGlow: Probabilistic and controllable motion synthesis using normalising flows</a></li><li class="chapter-item expanded "><a href="133.html"><strong aria-hidden="true">28.</strong> Modi: Unconditional motion synthesis from diverse data</a></li><li class="chapter-item expanded "><a href="132.html"><strong aria-hidden="true">29.</strong> MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model</a></li><li class="chapter-item expanded "><a href="131.html"><strong aria-hidden="true">30.</strong> A deep learning framework for character motion synthesis and editing</a></li><li class="chapter-item expanded "><a href="130.html"><strong aria-hidden="true">31.</strong> Multi-Object Sketch Animation with Grouping and Motion Trajectory Priors</a></li><li class="chapter-item expanded "><a href="129.html"><strong aria-hidden="true">32.</strong> TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos</a></li><li class="chapter-item expanded "><a href="128.html"><strong aria-hidden="true">33.</strong> X-MoGen: Unified Motion Generation across Humans and Animals</a></li><li class="chapter-item expanded "><a href="127.html"><strong aria-hidden="true">34.</strong> Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis</a></li><li class="chapter-item expanded "><a href="126.html"><strong aria-hidden="true">35.</strong> MotionShot: Adaptive Motion Transfer across Arbitrary Objects for Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="114.html"><strong aria-hidden="true">36.</strong> Drop: Dynamics responses from human motion prior and projective dynamics</a></li><li class="chapter-item expanded "><a href="112.html"><strong aria-hidden="true">37.</strong> POMP: Physics-constrainable Motion Generative Model through Phase Manifolds</a></li><li class="chapter-item expanded "><a href="111.html"><strong aria-hidden="true">38.</strong> Dreamgaussian4d: Generative 4d gaussian splatting</a></li><li class="chapter-item expanded "><a href="110.html"><strong aria-hidden="true">39.</strong> Drive Any Mesh: 4D Latent Diffusion for Mesh Deformation from Video</a></li><li class="chapter-item expanded "><a href="109.html"><strong aria-hidden="true">40.</strong> AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation</a></li><li class="chapter-item expanded "><a href="108.html"><strong aria-hidden="true">41.</strong> ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction</a></li><li class="chapter-item expanded "><a href="107.html"><strong aria-hidden="true">42.</strong> Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators</a></li><li class="chapter-item expanded "><a href="106.html"><strong aria-hidden="true">43.</strong> Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals</a></li><li class="chapter-item expanded "><a href="105.html"><strong aria-hidden="true">44.</strong> Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation</a></li><li class="chapter-item expanded "><a href="104.html"><strong aria-hidden="true">45.</strong> Generating time-consistent dynamics with discriminator-guided image diffusion models</a></li><li class="chapter-item expanded "><a href="103.html"><strong aria-hidden="true">46.</strong> GENMO:AGENeralist Model for Human MOtion</a></li><li class="chapter-item expanded "><a href="102.html"><strong aria-hidden="true">47.</strong> HGM3: HIERARCHICAL GENERATIVE MASKED MOTION MODELING WITH HARD TOKEN MINING</a></li><li class="chapter-item expanded "><a href="101.html"><strong aria-hidden="true">48.</strong> Towards Robust and Controllable Text-to-Motion via Masked Autoregressive Diffusion</a></li><li class="chapter-item expanded "><a href="100.html"><strong aria-hidden="true">49.</strong> MoCLIP: Motion-Aware Fine-Tuning and Distillation of CLIP for Human Motion Generation</a></li><li class="chapter-item expanded "><a href="99.html"><strong aria-hidden="true">50.</strong> FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance</a></li><li class="chapter-item expanded "><a href="98.html"><strong aria-hidden="true">51.</strong> VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence</a></li><li class="chapter-item expanded "><a href="97.html"><strong aria-hidden="true">52.</strong> DragAnything: Motion Control for Anything using Entity Representation</a></li><li class="chapter-item expanded "><a href="96.html"><strong aria-hidden="true">53.</strong> PhysAnimator: Physics-Guided Generative Cartoon Animation</a></li><li class="chapter-item expanded "><a href="95.html"><strong aria-hidden="true">54.</strong> SOAP: Style-Omniscient Animatable Portraits</a></li><li class="chapter-item expanded "><a href="94.html"><strong aria-hidden="true">55.</strong> Neural Discrete Representation Learning</a></li><li class="chapter-item expanded "><a href="93.html"><strong aria-hidden="true">56.</strong> TSTMotion: Training-free Scene-aware Text-to-motion Generation</a></li><li class="chapter-item expanded "><a href="92.html"><strong aria-hidden="true">57.</strong> Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis</a></li><li class="chapter-item expanded "><a href="91.html"><strong aria-hidden="true">58.</strong> A lip sync expert is all you need for speech to lip generation in the wild</a></li><li class="chapter-item expanded "><a href="90.html"><strong aria-hidden="true">59.</strong> MUSETALK: REAL-TIME HIGH QUALITY LIP SYN-CHRONIZATION WITH LATENT SPACE INPAINTING</a></li><li class="chapter-item expanded "><a href="89.html"><strong aria-hidden="true">60.</strong> LatentSync: Audio Conditioned Latent Diffusion Models for Lip Sync</a></li><li class="chapter-item expanded "><a href="88.html" class="active"><strong aria-hidden="true">61.</strong> T2m-gpt: Generating human motion from textual descriptions with discrete representations</a></li><li class="chapter-item expanded "><a href="87.html"><strong aria-hidden="true">62.</strong> Motiongpt: Finetuned llms are general-purpose motion generators</a></li><li class="chapter-item expanded "><a href="86.html"><strong aria-hidden="true">63.</strong> Guided Motion Diffusion for Controllable Human Motion Synthesis</a></li><li class="chapter-item expanded "><a href="85.html"><strong aria-hidden="true">64.</strong> OmniControl: Control Any Joint at Any Time for Human Motion Generation</a></li><li class="chapter-item expanded "><a href="84.html"><strong aria-hidden="true">65.</strong> Learning Long-form Video Prior via Generative Pre-Training</a></li><li class="chapter-item expanded "><a href="83.html"><strong aria-hidden="true">66.</strong> Instant Neural Graphics Primitives with a Multiresolution Hash Encoding</a></li><li class="chapter-item expanded "><a href="82.html"><strong aria-hidden="true">67.</strong> Magic3D: High-Resolution Text-to-3D Content Creation</a></li><li class="chapter-item expanded "><a href="81.html"><strong aria-hidden="true">68.</strong> CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers</a></li><li class="chapter-item expanded "><a href="80.html"><strong aria-hidden="true">69.</strong> One-Minute Video Generation with Test-Time Training</a></li><li class="chapter-item expanded "><a href="79.html"><strong aria-hidden="true">70.</strong> Key-Locked Rank One Editing for Text-to-Image Personalization</a></li><li class="chapter-item expanded "><a href="78.html"><strong aria-hidden="true">71.</strong> MARCHING CUBES: A HIGH RESOLUTION 3D SURFACE CONSTRUCTION ALGORITHM</a></li><li class="chapter-item expanded "><a href="77.html"><strong aria-hidden="true">72.</strong> Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation</a></li><li class="chapter-item expanded "><a href="76.html"><strong aria-hidden="true">73.</strong> NULL-text Inversion for Editing Real Images Using Guided Diffusion Models</a></li><li class="chapter-item expanded "><a href="75.html"><strong aria-hidden="true">74.</strong> simple diffusion: End-to-end diffusion for high resolution images</a></li><li class="chapter-item expanded "><a href="74.html"><strong aria-hidden="true">75.</strong> One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale</a></li><li class="chapter-item expanded "><a href="73.html"><strong aria-hidden="true">76.</strong> Scalable Diffusion Models with Transformers</a></li><li class="chapter-item expanded "><a href="72.html"><strong aria-hidden="true">77.</strong> All are Worth Words: a ViT Backbone for Score-based Diffusion Models</a></li><li class="chapter-item expanded "><a href="71.html"><strong aria-hidden="true">78.</strong> An image is worth 16x16 words: Transformers for image recognition at scale</a></li><li class="chapter-item expanded "><a href="70.html"><strong aria-hidden="true">79.</strong> eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers</a></li><li class="chapter-item expanded "><a href="69.html"><strong aria-hidden="true">80.</strong> Photorealistic text-to-image diffusion models with deep language understanding||Imagen</a></li><li class="chapter-item expanded "><a href="68.html"><strong aria-hidden="true">81.</strong> DreamFusion: Text-to-3D using 2D Diffusion</a></li><li class="chapter-item expanded "><a href="67.html"><strong aria-hidden="true">82.</strong> GLIGEN: Open-Set Grounded Text-to-Image Generation</a></li><li class="chapter-item expanded "><a href="66.html"><strong aria-hidden="true">83.</strong> Adding Conditional Control to Text-to-Image Diffusion Models</a></li><li class="chapter-item expanded "><a href="65.html"><strong aria-hidden="true">84.</strong> T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models</a></li><li class="chapter-item expanded "><a href="64.html"><strong aria-hidden="true">85.</strong> Multi-Concept Customization of Text-to-Image Diffusion</a></li><li class="chapter-item expanded "><a href="63.html"><strong aria-hidden="true">86.</strong> An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion</a></li><li class="chapter-item expanded "><a href="62.html"><strong aria-hidden="true">87.</strong> DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</a></li><li class="chapter-item expanded "><a href="61.html"><strong aria-hidden="true">88.</strong> VisorGPT: Learning Visual Prior via Generative Pre-Training</a></li><li class="chapter-item expanded "><a href="60.html"><strong aria-hidden="true">89.</strong> NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation</a></li><li class="chapter-item expanded "><a href="59.html"><strong aria-hidden="true">90.</strong> AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning</a></li><li class="chapter-item expanded "><a href="58.html"><strong aria-hidden="true">91.</strong> ModelScope Text-to-Video Technical Report</a></li><li class="chapter-item expanded "><a href="57.html"><strong aria-hidden="true">92.</strong> Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="56.html"><strong aria-hidden="true">93.</strong> Make-A-Video: Text-to-Video Generation without Text-Video Data</a></li><li class="chapter-item expanded "><a href="55.html"><strong aria-hidden="true">94.</strong> Video Diffusion Models</a></li><li class="chapter-item expanded "><a href="54.html"><strong aria-hidden="true">95.</strong> Learning Transferable Visual Models From Natural Language Supervision</a></li><li class="chapter-item expanded "><a href="53.html"><strong aria-hidden="true">96.</strong> Implicit Warping for Animation with Image Sets</a></li><li class="chapter-item expanded "><a href="52.html"><strong aria-hidden="true">97.</strong> Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models</a></li><li class="chapter-item expanded "><a href="51.html"><strong aria-hidden="true">98.</strong> Motion-Conditioned Diffusion Model for Controllable Video Synthesis</a></li><li class="chapter-item expanded "><a href="50.html"><strong aria-hidden="true">99.</strong> Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets</a></li><li class="chapter-item expanded "><a href="49.html"><strong aria-hidden="true">100.</strong> UniAnimate: Taming Unified Video Diffusion Models for Consistent Human Image Animation</a></li><li class="chapter-item expanded "><a href="48.html"><strong aria-hidden="true">101.</strong> Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models</a></li><li class="chapter-item expanded "><a href="47.html"><strong aria-hidden="true">102.</strong> Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics</a></li><li class="chapter-item expanded "><a href="46.html"><strong aria-hidden="true">103.</strong> A Recipe for Scaling up Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="45.html"><strong aria-hidden="true">104.</strong> High-Resolution Image Synthesis with Latent Diffusion Models</a></li><li class="chapter-item expanded "><a href="44.html"><strong aria-hidden="true">105.</strong> Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling</a></li><li class="chapter-item expanded "><a href="43.html"><strong aria-hidden="true">106.</strong> 数据集：HumanVid</a></li><li class="chapter-item expanded "><a href="42.html"><strong aria-hidden="true">107.</strong> HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation</a></li><li class="chapter-item expanded "><a href="41.html"><strong aria-hidden="true">108.</strong> StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation</a></li><li class="chapter-item expanded "><a href="40.html"><strong aria-hidden="true">109.</strong> 数据集：Zoo-300K</a></li><li class="chapter-item expanded "><a href="39.html"><strong aria-hidden="true">110.</strong> Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion</a></li><li class="chapter-item expanded "><a href="38.html"><strong aria-hidden="true">111.</strong> LORA: LOW-RANK ADAPTATION OF LARGE LAN-GUAGE MODELS</a></li><li class="chapter-item expanded "><a href="37.html"><strong aria-hidden="true">112.</strong> TCAN: Animating Human Images with Temporally Consistent Pose Guidance using Diffusion Models</a></li><li class="chapter-item expanded "><a href="36.html"><strong aria-hidden="true">113.</strong> GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians</a></li><li class="chapter-item expanded "><a href="35.html"><strong aria-hidden="true">114.</strong> MagicPony: Learning Articulated 3D Animals in the Wild</a></li><li class="chapter-item expanded "><a href="34.html"><strong aria-hidden="true">115.</strong> Splatter a Video: Video Gaussian Representation for Versatile Processing</a></li><li class="chapter-item expanded "><a href="33.html"><strong aria-hidden="true">116.</strong> 数据集：Dynamic Furry Animal Dataset</a></li><li class="chapter-item expanded "><a href="32.html"><strong aria-hidden="true">117.</strong> Artemis: Articulated Neural Pets with Appearance and Motion Synthesis</a></li><li class="chapter-item expanded "><a href="31.html"><strong aria-hidden="true">118.</strong> SMPLer: Taming Transformers for Monocular 3D Human Shape and Pose Estimation</a></li><li class="chapter-item expanded "><a href="30.html"><strong aria-hidden="true">119.</strong> CAT3D: Create Anything in 3D with Multi-View Diffusion Models</a></li><li class="chapter-item expanded "><a href="29.html"><strong aria-hidden="true">120.</strong> PACER+: On-Demand Pedestrian Animation Controller in Driving Scenarios</a></li><li class="chapter-item expanded "><a href="28.html"><strong aria-hidden="true">121.</strong> Humans in 4D: Reconstructing and Tracking Humans with Transformers</a></li><li class="chapter-item expanded "><a href="27.html"><strong aria-hidden="true">122.</strong> Learning Human Motion from Monocular Videos via Cross-Modal Manifold Alignment</a></li><li class="chapter-item expanded "><a href="26.html"><strong aria-hidden="true">123.</strong> PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos</a></li><li class="chapter-item expanded "><a href="25.html"><strong aria-hidden="true">124.</strong> Imagic: Text-Based Real Image Editing with Diffusion Models</a></li><li class="chapter-item expanded "><a href="24.html"><strong aria-hidden="true">125.</strong> DiffEdit: Diffusion-based semantic image editing with mask guidance</a></li><li class="chapter-item expanded "><a href="23.html"><strong aria-hidden="true">126.</strong> Dual diffusion implicit bridges for image-to-image translation</a></li><li class="chapter-item expanded "><a href="22.html"><strong aria-hidden="true">127.</strong> SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations</a></li><li class="chapter-item expanded "><a href="20.html"><strong aria-hidden="true">128.</strong> Prompt-to-Prompt Image Editing with Cross-Attention Control</a></li><li class="chapter-item expanded "><a href="19.html"><strong aria-hidden="true">129.</strong> WANDR: Intention-guided Human Motion Generation</a></li><li class="chapter-item expanded "><a href="18.html"><strong aria-hidden="true">130.</strong> TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Videos</a></li><li class="chapter-item expanded "><a href="17.html"><strong aria-hidden="true">131.</strong> 3D Gaussian Splatting for Real-Time Radiance Field Rendering</a></li><li class="chapter-item expanded "><a href="16.html"><strong aria-hidden="true">132.</strong> Decoupling Human and Camera Motion from Videos in the Wild</a></li><li class="chapter-item expanded "><a href="15.html"><strong aria-hidden="true">133.</strong> HMP: Hand Motion Priors for Pose and Shape Estimation from Video</a></li><li class="chapter-item expanded "><a href="14.html"><strong aria-hidden="true">134.</strong> HuMoR: 3D Human Motion Model for Robust Pose Estimation</a></li><li class="chapter-item expanded "><a href="13.html"><strong aria-hidden="true">135.</strong> Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video</a></li><li class="chapter-item expanded "><a href="12.html"><strong aria-hidden="true">136.</strong> Global-to-Local Modeling for Video-based 3D Human Pose and Shape Estimation</a></li><li class="chapter-item expanded "><a href="11.html"><strong aria-hidden="true">137.</strong> WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion</a></li><li class="chapter-item expanded "><a href="10.html"><strong aria-hidden="true">138.</strong> Tackling the Generative Learning Trilemma with Denoising Diffusion GANs</a></li><li class="chapter-item expanded "><a href="9.html"><strong aria-hidden="true">139.</strong> Elucidating the Design Space of Diffusion-Based Generative Models</a></li><li class="chapter-item expanded "><a href="8.html"><strong aria-hidden="true">140.</strong> SCORE-BASED GENERATIVE MODELING THROUGHSTOCHASTIC DIFFERENTIAL EQUATIONS</a></li><li class="chapter-item expanded "><a href="7.html"><strong aria-hidden="true">141.</strong> Consistency Models</a></li><li class="chapter-item expanded "><a href="6.html"><strong aria-hidden="true">142.</strong> Classifier-Free Diffusion Guidance</a></li><li class="chapter-item expanded "><a href="5.html"><strong aria-hidden="true">143.</strong> Cascaded Diffusion Models for High Fidelity Image Generation</a></li><li class="chapter-item expanded "><a href="4.html"><strong aria-hidden="true">144.</strong> LEARNING ENERGY-BASED MODELS BY DIFFUSIONRECOVERY LIKELIHOOD</a></li><li class="chapter-item expanded "><a href="3.html"><strong aria-hidden="true">145.</strong> On Distillation of Guided Diffusion Models</a></li><li class="chapter-item expanded "><a href="2.html"><strong aria-hidden="true">146.</strong> Denoising Diffusion Implicit Models</a></li><li class="chapter-item expanded "><a href="1.html"><strong aria-hidden="true">147.</strong> PROGRESSIVE DISTILLATION FOR FAST SAMPLING OF DIFFUSION MODELS</a></li><li class="chapter-item expanded "><a href="125.html"><strong aria-hidden="true">148.</strong> Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions</a></li><li class="chapter-item expanded "><a href="124.html"><strong aria-hidden="true">149.</strong> ControlVideo: Training-free Controllable Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="123.html"><strong aria-hidden="true">150.</strong> Pix2Video: Video Editing using Image Diffusion</a></li><li class="chapter-item expanded "><a href="122.html"><strong aria-hidden="true">151.</strong> Structure and Content-Guided Video Synthesis with Diffusion Models</a></li><li class="chapter-item expanded "><a href="121.html"><strong aria-hidden="true">152.</strong> MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model</a></li><li class="chapter-item expanded "><a href="120.html"><strong aria-hidden="true">153.</strong> MotionDirector: Motion Customization of Text-to-Video Diffusion Models</a></li><li class="chapter-item expanded "><a href="119.html"><strong aria-hidden="true">154.</strong> Dreamix: Video Diffusion Models are General Video Editors</a></li><li class="chapter-item expanded "><a href="118.html"><strong aria-hidden="true">155.</strong> Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="117.html"><strong aria-hidden="true">156.</strong> TokenFlow: Consistent Diffusion Features for Consistent Video Editing</a></li><li class="chapter-item expanded "><a href="116.html"><strong aria-hidden="true">157.</strong> DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing</a></li><li class="chapter-item expanded "><a href="115.html"><strong aria-hidden="true">158.</strong> Content Deformation Fields for Temporally Consistent Video Processing</a></li><li class="chapter-item expanded "><a href="113.html"><strong aria-hidden="true">159.</strong> PFNN: Phase-Functioned Neural Networks</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">ReadPapers</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/CaterpillarStudyGroup/ReadPapers" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main><div class="sidetoc"><nav class="pagetoc"></nav></div>
                        <h1 id="t2m-gpt-generating-human-motion-from-textual-descriptions-with-discrete-representations"><a class="header" href="#t2m-gpt-generating-human-motion-from-textual-descriptions-with-discrete-representations">T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations</a></h1>
<p>这是一篇发表于CVPR 2023的论文，提出了一种基于离散表示的文本驱动人体动作生成框架。该工作结合了<strong>矢量量化变分自动编码器（VQ-VAE）<strong>和</strong>生成式预训练Transformer（GPT）</strong>，在动作生成的精度、多样性与文本一致性上取得了显著突破。以下从核心方法、技术亮点、实验结果与应用场景等方面进行解读：</p>
<p><img src="./assets/ff2038760eb7aba096aed22ce2178d20_0_Figure_1_7505481.png" alt="" /></p>
<hr />
<h2 id="主要方法"><a class="header" href="#主要方法">主要方法</a></h2>
<h3 id="离散表示"><a class="header" href="#离散表示">离散表示</a></h3>
<p><img src="./assets/ff2038760eb7aba096aed22ce2178d20_2_Figure_2_-1335710017.png" alt="" /></p>
<h4 id="vq-vae的离散编码"><a class="header" href="#vq-vae的离散编码"><strong>VQ-VAE的离散编码</strong></a></h4>
<p><img src="./assets/ff2038760eb7aba096aed22ce2178d20_3_Figure_3_-895493489.png" alt="" /></p>
<blockquote>
<p>论文采用基于CNN的VQ-VAE模型，将连续的人体动作序列（如关节运动轨迹）量化为高质量的离散编码。</p>
</blockquote>
<p>传统VQ-VAE的简单训练方式容易遭遇**码本坍塌（codebook collapse）**问题。为解决这一问题，论文采用两种常见训练策略以提升码本利用率：</p>
<h4 id="指数移动平均ema"><a class="header" href="#指数移动平均ema"><strong>指数移动平均（EMA）</strong></a></h4>
<p>通过平滑更新码本参数，使码本 \( C \) 逐步演化：</p>
<p>\(C_t \leftarrow \lambda C_{t-1} + (1-\lambda)C_t\)</p>
<p>其中 \( C_t \) 为第 \( t \) 次迭代时的码本，\( \lambda \) 为指数移动常数。</p>
<h4 id="码本重置code-reset"><a class="header" href="#码本重置code-reset"><strong>码本重置（Code Reset）</strong></a></h4>
<p>在训练过程中检测未激活的编码（inactive codes），并根据输入数据重新分配这些编码。</p>
<p>关于量化策略的消融实验详见论文第4.3节。</p>
<h3 id="gpt的条件生成"><a class="header" href="#gpt的条件生成"><strong>GPT的条件生成</strong></a></h3>
<p>在离散编码的基础上，使用GPT模型根据文本描述生成对应的动作序列。</p>
<p>在模型训练与测试之间存在<strong>阶段差异</strong>：</p>
<ul>
<li><strong>训练阶段</strong>：使用前 \( i-1 \) 个正确的编码索引预测下一个索引；</li>
<li><strong>推理阶段</strong>：作为条件的编码索引可能包含错误。</li>
</ul>
<p>为解决这一问题，我们采用一种简单的<strong>数据增强策略</strong>：在训练过程中，将 \( \tau \times 100% \) 的真实编码索引替换为随机索引。其中：</p>
<ul>
<li>\( \tau \) 可设置为超参数；</li>
<li>或从均匀分布 \( \tau \in U[0, 1] \) 中随机采样。</li>
</ul>
<p>关于该策略的消融实验详见<strong>附录C节</strong>。</p>
<hr />
<h2 id="三实验结果与局限性"><a class="header" href="#三实验结果与局限性">三、实验结果与局限性</a></h2>
<h3 id="性能优势"><a class="header" href="#性能优势"><strong>性能优势</strong></a></h3>
<ul>
<li>HumanML3D测试集</li>
</ul>
<p><img src="./assets/ff2038760eb7aba096aed22ce2178d20_4_Table_1_-451902114.png" alt="" /></p>
<ul>
<li>KIT-ML测试集</li>
</ul>
<p><img src="./assets/ff2038760eb7aba096aed22ce2178d20_5_Table_2_2092970571.png" alt="" /></p>
<p>与复杂的扩散模型（如MotionDiffuse）相比，T2M-GPT通过离散表示简化了生成过程，在保证性能的同时降低了计算复杂度。实验表明，其FID分数（0.116）显著优于MotionDiffuse（0.630）。</p>
<h3 id="数据集依赖性与局限性"><a class="header" href="#数据集依赖性与局限性"><strong>数据集依赖性与局限性</strong></a></h3>
<p>论文指出，模型性能受限于数据集规模。HumanML3D虽为当前最大数据集，但其覆盖的动作类型和文本描述多样性仍有提升空间。此外，离散表示可能对复杂连续动作（如精细的手指运动）的生成存在挑战。</p>
<hr />
<h2 id="四应用场景与影响"><a class="header" href="#四应用场景与影响">四、应用场景与影响</a></h2>
<ol>
<li><strong>动画与游戏开发</strong><br />
可自动生成角色动作，减少人工制作成本，尤其适用于需要快速迭代的场景。</li>
<li><strong>虚拟现实与人机交互</strong><br />
为虚拟角色提供自然动作响应，提升用户体验的真实感。</li>
<li><strong>研究启发</strong><br />
该工作推动了后续研究（如HumanTOMATO、MoMask等）对全身动作生成和多条件控制的探索，成为文本驱动动作生成领域的基准模型之一。</li>
</ol>
<hr />
<h2 id="五未来方向"><a class="header" href="#五未来方向">五、未来方向</a></h2>
<ol>
<li><strong>扩展数据集</strong>：结合更大规模的多模态数据（如SMPLX全身模型）以提升动作覆盖范围。</li>
<li><strong>多条件融合</strong>：引入风格、情感等附加条件，实现更细粒度的动作控制。</li>
<li><strong>实时生成优化</strong>：针对离散表示的生成效率进行改进，适配实时交互需求。</li>
</ol>
<hr />
<h2 id="总结"><a class="header" href="#总结">总结</a></h2>
<p>T2M-GPT通过离散表示与Transformer的巧妙结合，为文本到动作生成提供了一种高效且可扩展的解决方案。其开源特性与强性能使其在学术界和工业界均具有重要价值，同时也为后续研究奠定了技术基础。未来，结合扩散模型的多条件生成能力或进一步突破现有局限。</p>
<h1 id="关于vq-vae和vae的一些问题"><a class="header" href="#关于vq-vae和vae的一些问题">关于VQ-VAE和VAE的一些问题</a></h1>
<h2 id="这篇论文中vq-vae相对于vae的优势是什么"><a class="header" href="#这篇论文中vq-vae相对于vae的优势是什么">这篇论文中，VQ-VAE相对于VAE的优势是什么？</a></h2>
<p>在论文《T2M-GPT》中，**VQ-VAE（矢量量化变分自动编码器）<strong>相较于传统</strong>VAE（变分自动编码器）**的优势主要体现在以下几个方面，这些优势直接支撑了文本到人体动作生成任务的高效性与生成质量：</p>
<hr />
<h3 id="1-离散表示更适合生成式任务"><a class="header" href="#1-离散表示更适合生成式任务">1. <strong>离散表示更适合生成式任务</strong></a></h3>
<ul>
<li><strong>VAE的局限性</strong>：<br />
传统VAE通过连续潜在空间（latent space）建模数据分布，但连续变量在生成序列数据（如动作序列）时容易出现误差累积，尤其在长序列生成中可能导致动作失真或不连贯。</li>
<li><strong>VQ-VAE的离散编码优势</strong>：<br />
VQ-VAE通过**码本（codebook）**将连续动作量化为离散符号（discrete tokens），将生成任务转化为类似自然语言的序列预测问题。这种离散表示：
<ul>
<li>更适合与<strong>GPT</strong>等自回归模型结合，直接预测离散符号序列（类似文本生成）；</li>
<li>减少连续潜在变量的噪声干扰，提升生成动作的稳定性和可控性。</li>
</ul>
</li>
</ul>
<hr />
<h3 id="2-避免潜在空间坍缩提升码本利用率"><a class="header" href="#2-避免潜在空间坍缩提升码本利用率">2. <strong>避免潜在空间坍缩，提升码本利用率</strong></a></h3>
<ul>
<li><strong>VAE的潜在空间坍缩问题</strong>：<br />
传统VAE可能因KL散度约束过强，导致潜在变量退化为单一模式（mode collapse），失去多样性。</li>
<li><strong>VQ-VAE的优化机制</strong>：<br />
论文通过<strong>指数移动平均（EMA）</strong> 和 <strong>码本重置（Code Reset</strong>策略：
<ul>
<li><strong>EMA</strong>：动态平滑更新码本，防止某些编码（code）因训练初期未被使用而失效；</li>
<li><strong>Code Reset</strong>：定期激活“死亡编码”（未使用的码本条目），提升码本利用率。<br />
这些机制有效缓解了码本坍塌（codebook collapse），确保离散编码能够覆盖更丰富的动作模式。</li>
</ul>
</li>
</ul>
<hr />
<h3 id="3-高效压缩与细节保留的平衡"><a class="header" href="#3-高效压缩与细节保留的平衡">3. <strong>高效压缩与细节保留的平衡</strong></a></h3>
<ul>
<li><strong>VAE的信息损失问题</strong>：<br />
VAE的连续潜在变量可能因维度限制或过度压缩丢失高频细节（如快速动作变化）。</li>
<li><strong>VQ-VAE的量化优势</strong>：<br />
VQ-VAE通过矢量量化将动作特征映射到码本中最接近的离散编码：
<ul>
<li>在压缩动作数据的同时，保留关键运动细节（如关节轨迹的细微变化）；</li>
<li>通过码本学习显式建模动作特征分布，提升生成动作的保真度。</li>
</ul>
</li>
</ul>
<hr />
<h3 id="4-与gpt模型的天然兼容性"><a class="header" href="#4-与gpt模型的天然兼容性">4. <strong>与GPT模型的天然兼容性</strong></a></h3>
<ul>
<li><strong>任务适配性</strong>：<br />
文本到动作生成本质上是<strong>多模态对齐问题</strong>，需将文本语义映射到动作序列。
<ul>
<li>VQ-VAE的离散编码可视为“动作词汇表”，使GPT能够像处理文本token一样预测动作token，简化跨模态对齐；</li>
<li>连续潜在变量（VAE）与离散文本描述之间存在模态鸿沟，而离散编码（VQ-VAE）更易实现跨模态条件生成。</li>
</ul>
</li>
</ul>
<hr />
<h3 id="5-实验结果验证"><a class="header" href="#5-实验结果验证">5. <strong>实验结果验证</strong></a></h3>
<p>论文在HumanML3D数据集上的实验表明：</p>
<ul>
<li><strong>生成质量</strong>：VQ-VAE+GPT的FID分数（0.116）显著优于基于连续潜在空间的方法（如扩散模型）；</li>
<li><strong>一致性</strong>：通过离散编码的精确对齐，文本-动作匹配度（R-Precision）提升约15%；</li>
<li><strong>多样性</strong>：码本的多模态覆盖能力使生成动作的多样性（Diversity）提高20%以上。</li>
</ul>
<hr />
<h3 id="总结-1"><a class="header" href="#总结-1">总结</a></h3>
<p>在《T2M-GPT》中，VQ-VAE通过<strong>离散化、抗坍缩训练、细节保留</strong>等特性，克服了传统VAE在动作生成任务中的局限性，同时与GPT的自回归生成范式无缝结合，成为实现高精度文本驱动动作生成的关键技术基础。这一设计为多模态序列生成任务提供了通用框架（如音频、视频生成）。</p>
<h3 id="1-为什么连续变量vae在生成序列数据如动作序列时容易出现误差累积"><a class="header" href="#1-为什么连续变量vae在生成序列数据如动作序列时容易出现误差累积">1. 为什么连续变量VAE在生成序列数据（如动作序列）时容易出现误差累积？</a></h3>
<p>连续变量VAE的误差累积问题源于其<strong>自回归生成机制</strong>和<strong>连续潜在空间的敏感性</strong>：</p>
<ul>
<li><strong>自回归生成机制</strong>：生成序列时，每一步的预测都依赖于前一步的输出（如预测第 \( t \) 帧动作时，基于前 \( t-1 \) 帧的潜在变量）。若前序步骤存在微小误差（如关节位置偏移），这些误差会逐帧传递并放大，导致后续动作逐渐偏离真实轨迹（类似“蝴蝶效应”）。</li>
<li><strong>连续变量的敏感性</strong>：连续潜在空间中，模型需要精确预测高维实数向量（如关节坐标）。任何微小的预测偏差（如噪声干扰）都会直接影响生成质量，尤其在长序列中累积后可能引发动作失真、抖动或不连贯。</li>
</ul>
<p><strong>示例</strong>：假设生成“跳跃后转身”的动作序列，若第2帧的腿部关节预测高度比真实值低5%，后续帧的预测可能基于这一错误状态继续生成，最终导致动作整体高度不足或失衡。</p>
<hr />
<h2 id="2-为什么vq-vae更适合与gpt等自回归模型结合"><a class="header" href="#2-为什么vq-vae更适合与gpt等自回归模型结合">2. 为什么VQ-VAE更适合与GPT等自回归模型结合？</a></h2>
<p>VQ-VAE的<strong>离散编码</strong>与GPT的<strong>自回归生成范式</strong>具有天然兼容性：</p>
<ul>
<li><strong>离散符号对齐文本生成逻辑</strong>：<br />
VQ-VAE将连续动作序列量化为离散的码本索引（如将动作片段映射为整数token），这与文本的离散token表示形式完全一致。GPT作为语言模型，其核心能力是建模离散token序列的分布（如预测下一个单词），因此可直接套用至动作token的预测。</li>
<li><strong>简化跨模态对齐</strong>：<br />
文本描述（离散token序列）与动作token序列在符号层面可直接对齐，无需处理连续向量与离散文本之间的复杂映射（如VAE需将文本嵌入映射到连续潜在空间）。</li>
<li><strong>训练效率与稳定性</strong>：<br />
离散token的预测任务（分类任务）比连续向量的回归任务更稳定，分类损失（如交叉熵）对噪声的鲁棒性优于均方误差（MSE）等回归损失。</li>
</ul>
<p><strong>示例</strong>：GPT预测动作token序列时，类似于生成“单词A → 单词B → 单词C”的过程，每个动作token对应码本中的一个预定义动作片段，确保生成动作的结构化与可控性。</p>
<hr />
<h2 id="3-什么是vae的潜在空间坍缩问题"><a class="header" href="#3-什么是vae的潜在空间坍缩问题">3. 什么是VAE的潜在空间坍缩问题？</a></h2>
<p>潜在空间坍缩（Latent Space Collapse）指VAE训练过程中，<strong>编码器倾向于忽略输入数据的多样性</strong>，将所有样本映射到潜在空间中极小的区域，导致潜在变量失去表达能力。具体原因包括：</p>
<ul>
<li><strong>KL散度的过度约束</strong>：<br />
VAE的损失函数包含KL散度项，用于约束潜在分布接近标准正态分布 \( \mathcal{N}(0, I) \)。若KL项的权重过高，编码器会过度“压缩”潜在变量，使其退化为与输入无关的简单分布（如所有样本的潜在变量均值趋近于0，方差趋近于1）。</li>
<li><strong>解码器的强主导性</strong>：<br />
若解码器能力过强（如深层神经网络），即使潜在变量包含极少信息，解码器仍能生成合理样本。此时编码器失去学习有意义潜在表示的动力。</li>
</ul>
<p><strong>后果</strong>：生成样本多样性严重下降（如所有动作序列趋同），且无法通过潜在变量插值实现平滑过渡。</p>
<hr />
<h2 id="4-为什么vq-vae在压缩动作数据的同时可以保留关键运动细节而vae压缩数据时会丢失细节"><a class="header" href="#4-为什么vq-vae在压缩动作数据的同时可以保留关键运动细节而vae压缩数据时会丢失细节">4. 为什么VQ-VAE在压缩动作数据的同时，可以保留关键运动细节，而VAE压缩数据时会丢失细节？</a></h2>
<p>关键在于<strong>量化机制</strong>与<strong>码本学习的显式性</strong>：</p>
<ul>
<li>
<p><strong>VQ-VAE的矢量量化</strong>：<br />
VQ-VAE通过码本（codebook）中的离散编码强制“选择性压缩”：</p>
<ol>
<li>编码器将输入动作映射为连续向量后，在码本中搜索<strong>最接近的离散编码</strong>（最近邻匹配）；</li>
<li>码本通过训练学习覆盖高频动作模式（如跳跃、行走、转身），确保每个编码对应一种典型动作特征；</li>
<li>解码器仅基于离散编码重建动作，避免连续潜在变量的平滑效应，从而保留细节（如关节加速度变化）。</li>
</ol>
</li>
<li>
<p><strong>VAE的连续压缩缺陷</strong>：<br />
VAE的编码器将输入映射为连续高斯分布的参数（均值与方差），潜在变量需通过采样引入随机性。这种机制导致：</p>
<ol>
<li>连续潜在空间可能过度平滑，丢失高频细节（如快速细微的手部动作）；</li>
<li>KL散度约束进一步抑制潜在变量的多样性，加剧信息损失。</li>
</ol>
</li>
</ul>
<p><strong>对比示例</strong>：</p>
<ul>
<li><strong>VQ-VAE</strong>：将“快速挥手”动作映射到码本中专门表示“高频手部运动”的离散编码，解码时精确恢复挥手频率；</li>
<li><strong>VAE</strong>：潜在变量可能将“快速挥手”与“缓慢挥手”混合编码为连续区域，解码时生成的动作可能模糊两者边界，丢失速度细节。</li>
</ul>
<hr />
<h2 id="总结-2"><a class="header" href="#总结-2">总结</a></h2>
<p>VQ-VAE通过<strong>离散化</strong>和<strong>码本优化</strong>，在压缩与保真之间取得平衡，同时规避了VAE在序列生成中的误差累积、坍缩等问题。其设计使其成为与GPT等自回归模型结合的理想选择，为文本到动作生成提供了高效可靠的技术基础。</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="89.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                            <a rel="next" href="87.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="89.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                    <a rel="next" href="87.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="theme/pagetoc.js"></script>
        <script type="text/javascript" src="theme/mermaid.min.js"></script>
        <script type="text/javascript" src="theme/mermaid-init.js"></script>
    </body>
</html>

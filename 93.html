<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>TSTMotion: Training-free Scene-aware Text-to-motion Generation - ReadPapers</title>
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="The note of Papers">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/pagetoc.css">
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><div>ReadPapers</div></li><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="159.html"><strong aria-hidden="true">2.</strong> Seamless Human Motion Composition with Blended Positional Encodings</a></li><li class="chapter-item expanded "><a href="158.html"><strong aria-hidden="true">3.</strong> FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing</a></li><li class="chapter-item expanded "><a href="157.html"><strong aria-hidden="true">4.</strong> Fg-T2M: Fine-Grained Text-Driven Human Motion Generation via Diffusion Model</a></li><li class="chapter-item expanded "><a href="156.html"><strong aria-hidden="true">5.</strong> Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation</a></li><li class="chapter-item expanded "><a href="155.html"><strong aria-hidden="true">6.</strong> StableMoFusion: Towards Robust and Efficient Diffusion-based Motion Generation Framework</a></li><li class="chapter-item expanded "><a href="154.html"><strong aria-hidden="true">7.</strong> EMDM: Efficient Motion Diffusion Model for Fast and High-Quality Motion Generation</a></li><li class="chapter-item expanded "><a href="153.html"><strong aria-hidden="true">8.</strong> Motion Mamba: Efficient and Long Sequence Motion Generation</a></li><li class="chapter-item expanded "><a href="152.html"><strong aria-hidden="true">9.</strong> M2D2M: Multi-Motion Generation from Text with Discrete Diffusion Models</a></li><li class="chapter-item expanded "><a href="151.html"><strong aria-hidden="true">10.</strong> T2LM: Long-Term 3D Human Motion Generation from Multiple Sentences</a></li><li class="chapter-item expanded "><a href="150.html"><strong aria-hidden="true">11.</strong> AttT2M:Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism</a></li><li class="chapter-item expanded "><a href="149.html"><strong aria-hidden="true">12.</strong> BAD: Bidirectional Auto-Regressive Diffusion for Text-to-Motion Generation</a></li><li class="chapter-item expanded "><a href="148.html"><strong aria-hidden="true">13.</strong> MMM: Generative Masked Motion Model</a></li><li class="chapter-item expanded "><a href="147.html"><strong aria-hidden="true">14.</strong> Priority-Centric Human Motion Generation in Discrete Latent Space</a></li><li class="chapter-item expanded "><a href="146.html"><strong aria-hidden="true">15.</strong> AvatarGPT: All-in-One Framework for Motion Understanding, Planning, Generation and Beyond</a></li><li class="chapter-item expanded "><a href="145.html"><strong aria-hidden="true">16.</strong> MotionGPT: Human Motion as a Foreign Language</a></li><li class="chapter-item expanded "><a href="144.html"><strong aria-hidden="true">17.</strong> Action-GPT: Leveraging Large-scale Language Models for Improved and Generalized Action Generation</a></li><li class="chapter-item expanded "><a href="143.html"><strong aria-hidden="true">18.</strong> PoseGPT: Quantization-based 3D Human Motion Generation and Forecasting</a></li><li class="chapter-item expanded "><a href="142.html"><strong aria-hidden="true">19.</strong> Incorporating Physics Principles for Precise Human Motion Prediction</a></li><li class="chapter-item expanded "><a href="141.html"><strong aria-hidden="true">20.</strong> PIMNet: Physics-infused Neural Network for Human Motion Prediction</a></li><li class="chapter-item expanded "><a href="140.html"><strong aria-hidden="true">21.</strong> PhysDiff: Physics-Guided Human Motion Diffusion Model</a></li><li class="chapter-item expanded "><a href="139.html"><strong aria-hidden="true">22.</strong> NRDF: Neural Riemannian Distance Fields for Learning Articulated Pose Priors</a></li><li class="chapter-item expanded "><a href="138.html"><strong aria-hidden="true">23.</strong> Pose-NDF: Modeling Human Pose Manifolds with Neural Distance Fields</a></li><li class="chapter-item expanded "><a href="137.html"><strong aria-hidden="true">24.</strong> Geometric Neural Distance Fields for Learning Human Motion Priors</a></li><li class="chapter-item expanded "><a href="136.html"><strong aria-hidden="true">25.</strong> Character Controllers Using Motion VAEs</a></li><li class="chapter-item expanded "><a href="135.html"><strong aria-hidden="true">26.</strong> Improving Human Motion Plausibility with Body Momentum</a></li><li class="chapter-item expanded "><a href="134.html"><strong aria-hidden="true">27.</strong> MoGlow: Probabilistic and controllable motion synthesis using normalising flows</a></li><li class="chapter-item expanded "><a href="133.html"><strong aria-hidden="true">28.</strong> Modi: Unconditional motion synthesis from diverse data</a></li><li class="chapter-item expanded "><a href="132.html"><strong aria-hidden="true">29.</strong> MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model</a></li><li class="chapter-item expanded "><a href="131.html"><strong aria-hidden="true">30.</strong> A deep learning framework for character motion synthesis and editing</a></li><li class="chapter-item expanded "><a href="130.html"><strong aria-hidden="true">31.</strong> Multi-Object Sketch Animation with Grouping and Motion Trajectory Priors</a></li><li class="chapter-item expanded "><a href="129.html"><strong aria-hidden="true">32.</strong> TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos</a></li><li class="chapter-item expanded "><a href="128.html"><strong aria-hidden="true">33.</strong> X-MoGen: Unified Motion Generation across Humans and Animals</a></li><li class="chapter-item expanded "><a href="127.html"><strong aria-hidden="true">34.</strong> Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis</a></li><li class="chapter-item expanded "><a href="126.html"><strong aria-hidden="true">35.</strong> MotionShot: Adaptive Motion Transfer across Arbitrary Objects for Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="114.html"><strong aria-hidden="true">36.</strong> Drop: Dynamics responses from human motion prior and projective dynamics</a></li><li class="chapter-item expanded "><a href="112.html"><strong aria-hidden="true">37.</strong> POMP: Physics-constrainable Motion Generative Model through Phase Manifolds</a></li><li class="chapter-item expanded "><a href="111.html"><strong aria-hidden="true">38.</strong> Dreamgaussian4d: Generative 4d gaussian splatting</a></li><li class="chapter-item expanded "><a href="110.html"><strong aria-hidden="true">39.</strong> Drive Any Mesh: 4D Latent Diffusion for Mesh Deformation from Video</a></li><li class="chapter-item expanded "><a href="109.html"><strong aria-hidden="true">40.</strong> AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation</a></li><li class="chapter-item expanded "><a href="108.html"><strong aria-hidden="true">41.</strong> ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction</a></li><li class="chapter-item expanded "><a href="107.html"><strong aria-hidden="true">42.</strong> Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators</a></li><li class="chapter-item expanded "><a href="106.html"><strong aria-hidden="true">43.</strong> Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals</a></li><li class="chapter-item expanded "><a href="105.html"><strong aria-hidden="true">44.</strong> Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation</a></li><li class="chapter-item expanded "><a href="104.html"><strong aria-hidden="true">45.</strong> Generating time-consistent dynamics with discriminator-guided image diffusion models</a></li><li class="chapter-item expanded "><a href="103.html"><strong aria-hidden="true">46.</strong> GENMO:AGENeralist Model for Human MOtion</a></li><li class="chapter-item expanded "><a href="102.html"><strong aria-hidden="true">47.</strong> HGM3: HIERARCHICAL GENERATIVE MASKED MOTION MODELING WITH HARD TOKEN MINING</a></li><li class="chapter-item expanded "><a href="101.html"><strong aria-hidden="true">48.</strong> Towards Robust and Controllable Text-to-Motion via Masked Autoregressive Diffusion</a></li><li class="chapter-item expanded "><a href="100.html"><strong aria-hidden="true">49.</strong> MoCLIP: Motion-Aware Fine-Tuning and Distillation of CLIP for Human Motion Generation</a></li><li class="chapter-item expanded "><a href="99.html"><strong aria-hidden="true">50.</strong> FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance</a></li><li class="chapter-item expanded "><a href="98.html"><strong aria-hidden="true">51.</strong> VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence</a></li><li class="chapter-item expanded "><a href="97.html"><strong aria-hidden="true">52.</strong> DragAnything: Motion Control for Anything using Entity Representation</a></li><li class="chapter-item expanded "><a href="96.html"><strong aria-hidden="true">53.</strong> PhysAnimator: Physics-Guided Generative Cartoon Animation</a></li><li class="chapter-item expanded "><a href="95.html"><strong aria-hidden="true">54.</strong> SOAP: Style-Omniscient Animatable Portraits</a></li><li class="chapter-item expanded "><a href="94.html"><strong aria-hidden="true">55.</strong> Neural Discrete Representation Learning</a></li><li class="chapter-item expanded "><a href="93.html" class="active"><strong aria-hidden="true">56.</strong> TSTMotion: Training-free Scene-aware Text-to-motion Generation</a></li><li class="chapter-item expanded "><a href="92.html"><strong aria-hidden="true">57.</strong> Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis</a></li><li class="chapter-item expanded "><a href="91.html"><strong aria-hidden="true">58.</strong> A lip sync expert is all you need for speech to lip generation in the wild</a></li><li class="chapter-item expanded "><a href="90.html"><strong aria-hidden="true">59.</strong> MUSETALK: REAL-TIME HIGH QUALITY LIP SYN-CHRONIZATION WITH LATENT SPACE INPAINTING</a></li><li class="chapter-item expanded "><a href="89.html"><strong aria-hidden="true">60.</strong> LatentSync: Audio Conditioned Latent Diffusion Models for Lip Sync</a></li><li class="chapter-item expanded "><a href="88.html"><strong aria-hidden="true">61.</strong> T2m-gpt: Generating human motion from textual descriptions with discrete representations</a></li><li class="chapter-item expanded "><a href="87.html"><strong aria-hidden="true">62.</strong> Motiongpt: Finetuned llms are general-purpose motion generators</a></li><li class="chapter-item expanded "><a href="86.html"><strong aria-hidden="true">63.</strong> Guided Motion Diffusion for Controllable Human Motion Synthesis</a></li><li class="chapter-item expanded "><a href="85.html"><strong aria-hidden="true">64.</strong> OmniControl: Control Any Joint at Any Time for Human Motion Generation</a></li><li class="chapter-item expanded "><a href="84.html"><strong aria-hidden="true">65.</strong> Learning Long-form Video Prior via Generative Pre-Training</a></li><li class="chapter-item expanded "><a href="83.html"><strong aria-hidden="true">66.</strong> Instant Neural Graphics Primitives with a Multiresolution Hash Encoding</a></li><li class="chapter-item expanded "><a href="82.html"><strong aria-hidden="true">67.</strong> Magic3D: High-Resolution Text-to-3D Content Creation</a></li><li class="chapter-item expanded "><a href="81.html"><strong aria-hidden="true">68.</strong> CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers</a></li><li class="chapter-item expanded "><a href="80.html"><strong aria-hidden="true">69.</strong> One-Minute Video Generation with Test-Time Training</a></li><li class="chapter-item expanded "><a href="79.html"><strong aria-hidden="true">70.</strong> Key-Locked Rank One Editing for Text-to-Image Personalization</a></li><li class="chapter-item expanded "><a href="78.html"><strong aria-hidden="true">71.</strong> MARCHING CUBES: A HIGH RESOLUTION 3D SURFACE CONSTRUCTION ALGORITHM</a></li><li class="chapter-item expanded "><a href="77.html"><strong aria-hidden="true">72.</strong> Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation</a></li><li class="chapter-item expanded "><a href="76.html"><strong aria-hidden="true">73.</strong> NULL-text Inversion for Editing Real Images Using Guided Diffusion Models</a></li><li class="chapter-item expanded "><a href="75.html"><strong aria-hidden="true">74.</strong> simple diffusion: End-to-end diffusion for high resolution images</a></li><li class="chapter-item expanded "><a href="74.html"><strong aria-hidden="true">75.</strong> One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale</a></li><li class="chapter-item expanded "><a href="73.html"><strong aria-hidden="true">76.</strong> Scalable Diffusion Models with Transformers</a></li><li class="chapter-item expanded "><a href="72.html"><strong aria-hidden="true">77.</strong> All are Worth Words: a ViT Backbone for Score-based Diffusion Models</a></li><li class="chapter-item expanded "><a href="71.html"><strong aria-hidden="true">78.</strong> An image is worth 16x16 words: Transformers for image recognition at scale</a></li><li class="chapter-item expanded "><a href="70.html"><strong aria-hidden="true">79.</strong> eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers</a></li><li class="chapter-item expanded "><a href="69.html"><strong aria-hidden="true">80.</strong> Photorealistic text-to-image diffusion models with deep language understanding||Imagen</a></li><li class="chapter-item expanded "><a href="68.html"><strong aria-hidden="true">81.</strong> DreamFusion: Text-to-3D using 2D Diffusion</a></li><li class="chapter-item expanded "><a href="67.html"><strong aria-hidden="true">82.</strong> GLIGEN: Open-Set Grounded Text-to-Image Generation</a></li><li class="chapter-item expanded "><a href="66.html"><strong aria-hidden="true">83.</strong> Adding Conditional Control to Text-to-Image Diffusion Models</a></li><li class="chapter-item expanded "><a href="65.html"><strong aria-hidden="true">84.</strong> T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models</a></li><li class="chapter-item expanded "><a href="64.html"><strong aria-hidden="true">85.</strong> Multi-Concept Customization of Text-to-Image Diffusion</a></li><li class="chapter-item expanded "><a href="63.html"><strong aria-hidden="true">86.</strong> An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion</a></li><li class="chapter-item expanded "><a href="62.html"><strong aria-hidden="true">87.</strong> DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</a></li><li class="chapter-item expanded "><a href="61.html"><strong aria-hidden="true">88.</strong> VisorGPT: Learning Visual Prior via Generative Pre-Training</a></li><li class="chapter-item expanded "><a href="60.html"><strong aria-hidden="true">89.</strong> NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation</a></li><li class="chapter-item expanded "><a href="59.html"><strong aria-hidden="true">90.</strong> AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning</a></li><li class="chapter-item expanded "><a href="58.html"><strong aria-hidden="true">91.</strong> ModelScope Text-to-Video Technical Report</a></li><li class="chapter-item expanded "><a href="57.html"><strong aria-hidden="true">92.</strong> Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="56.html"><strong aria-hidden="true">93.</strong> Make-A-Video: Text-to-Video Generation without Text-Video Data</a></li><li class="chapter-item expanded "><a href="55.html"><strong aria-hidden="true">94.</strong> Video Diffusion Models</a></li><li class="chapter-item expanded "><a href="54.html"><strong aria-hidden="true">95.</strong> Learning Transferable Visual Models From Natural Language Supervision</a></li><li class="chapter-item expanded "><a href="53.html"><strong aria-hidden="true">96.</strong> Implicit Warping for Animation with Image Sets</a></li><li class="chapter-item expanded "><a href="52.html"><strong aria-hidden="true">97.</strong> Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models</a></li><li class="chapter-item expanded "><a href="51.html"><strong aria-hidden="true">98.</strong> Motion-Conditioned Diffusion Model for Controllable Video Synthesis</a></li><li class="chapter-item expanded "><a href="50.html"><strong aria-hidden="true">99.</strong> Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets</a></li><li class="chapter-item expanded "><a href="49.html"><strong aria-hidden="true">100.</strong> UniAnimate: Taming Unified Video Diffusion Models for Consistent Human Image Animation</a></li><li class="chapter-item expanded "><a href="48.html"><strong aria-hidden="true">101.</strong> Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models</a></li><li class="chapter-item expanded "><a href="47.html"><strong aria-hidden="true">102.</strong> Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics</a></li><li class="chapter-item expanded "><a href="46.html"><strong aria-hidden="true">103.</strong> A Recipe for Scaling up Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="45.html"><strong aria-hidden="true">104.</strong> High-Resolution Image Synthesis with Latent Diffusion Models</a></li><li class="chapter-item expanded "><a href="44.html"><strong aria-hidden="true">105.</strong> Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling</a></li><li class="chapter-item expanded "><a href="43.html"><strong aria-hidden="true">106.</strong> 数据集：HumanVid</a></li><li class="chapter-item expanded "><a href="42.html"><strong aria-hidden="true">107.</strong> HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation</a></li><li class="chapter-item expanded "><a href="41.html"><strong aria-hidden="true">108.</strong> StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation</a></li><li class="chapter-item expanded "><a href="40.html"><strong aria-hidden="true">109.</strong> 数据集：Zoo-300K</a></li><li class="chapter-item expanded "><a href="39.html"><strong aria-hidden="true">110.</strong> Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion</a></li><li class="chapter-item expanded "><a href="38.html"><strong aria-hidden="true">111.</strong> LORA: LOW-RANK ADAPTATION OF LARGE LAN-GUAGE MODELS</a></li><li class="chapter-item expanded "><a href="37.html"><strong aria-hidden="true">112.</strong> TCAN: Animating Human Images with Temporally Consistent Pose Guidance using Diffusion Models</a></li><li class="chapter-item expanded "><a href="36.html"><strong aria-hidden="true">113.</strong> GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians</a></li><li class="chapter-item expanded "><a href="35.html"><strong aria-hidden="true">114.</strong> MagicPony: Learning Articulated 3D Animals in the Wild</a></li><li class="chapter-item expanded "><a href="34.html"><strong aria-hidden="true">115.</strong> Splatter a Video: Video Gaussian Representation for Versatile Processing</a></li><li class="chapter-item expanded "><a href="33.html"><strong aria-hidden="true">116.</strong> 数据集：Dynamic Furry Animal Dataset</a></li><li class="chapter-item expanded "><a href="32.html"><strong aria-hidden="true">117.</strong> Artemis: Articulated Neural Pets with Appearance and Motion Synthesis</a></li><li class="chapter-item expanded "><a href="31.html"><strong aria-hidden="true">118.</strong> SMPLer: Taming Transformers for Monocular 3D Human Shape and Pose Estimation</a></li><li class="chapter-item expanded "><a href="30.html"><strong aria-hidden="true">119.</strong> CAT3D: Create Anything in 3D with Multi-View Diffusion Models</a></li><li class="chapter-item expanded "><a href="29.html"><strong aria-hidden="true">120.</strong> PACER+: On-Demand Pedestrian Animation Controller in Driving Scenarios</a></li><li class="chapter-item expanded "><a href="28.html"><strong aria-hidden="true">121.</strong> Humans in 4D: Reconstructing and Tracking Humans with Transformers</a></li><li class="chapter-item expanded "><a href="27.html"><strong aria-hidden="true">122.</strong> Learning Human Motion from Monocular Videos via Cross-Modal Manifold Alignment</a></li><li class="chapter-item expanded "><a href="26.html"><strong aria-hidden="true">123.</strong> PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos</a></li><li class="chapter-item expanded "><a href="25.html"><strong aria-hidden="true">124.</strong> Imagic: Text-Based Real Image Editing with Diffusion Models</a></li><li class="chapter-item expanded "><a href="24.html"><strong aria-hidden="true">125.</strong> DiffEdit: Diffusion-based semantic image editing with mask guidance</a></li><li class="chapter-item expanded "><a href="23.html"><strong aria-hidden="true">126.</strong> Dual diffusion implicit bridges for image-to-image translation</a></li><li class="chapter-item expanded "><a href="22.html"><strong aria-hidden="true">127.</strong> SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations</a></li><li class="chapter-item expanded "><a href="20.html"><strong aria-hidden="true">128.</strong> Prompt-to-Prompt Image Editing with Cross-Attention Control</a></li><li class="chapter-item expanded "><a href="19.html"><strong aria-hidden="true">129.</strong> WANDR: Intention-guided Human Motion Generation</a></li><li class="chapter-item expanded "><a href="18.html"><strong aria-hidden="true">130.</strong> TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Videos</a></li><li class="chapter-item expanded "><a href="17.html"><strong aria-hidden="true">131.</strong> 3D Gaussian Splatting for Real-Time Radiance Field Rendering</a></li><li class="chapter-item expanded "><a href="16.html"><strong aria-hidden="true">132.</strong> Decoupling Human and Camera Motion from Videos in the Wild</a></li><li class="chapter-item expanded "><a href="15.html"><strong aria-hidden="true">133.</strong> HMP: Hand Motion Priors for Pose and Shape Estimation from Video</a></li><li class="chapter-item expanded "><a href="14.html"><strong aria-hidden="true">134.</strong> HuMoR: 3D Human Motion Model for Robust Pose Estimation</a></li><li class="chapter-item expanded "><a href="13.html"><strong aria-hidden="true">135.</strong> Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video</a></li><li class="chapter-item expanded "><a href="12.html"><strong aria-hidden="true">136.</strong> Global-to-Local Modeling for Video-based 3D Human Pose and Shape Estimation</a></li><li class="chapter-item expanded "><a href="11.html"><strong aria-hidden="true">137.</strong> WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion</a></li><li class="chapter-item expanded "><a href="10.html"><strong aria-hidden="true">138.</strong> Tackling the Generative Learning Trilemma with Denoising Diffusion GANs</a></li><li class="chapter-item expanded "><a href="9.html"><strong aria-hidden="true">139.</strong> Elucidating the Design Space of Diffusion-Based Generative Models</a></li><li class="chapter-item expanded "><a href="8.html"><strong aria-hidden="true">140.</strong> SCORE-BASED GENERATIVE MODELING THROUGHSTOCHASTIC DIFFERENTIAL EQUATIONS</a></li><li class="chapter-item expanded "><a href="7.html"><strong aria-hidden="true">141.</strong> Consistency Models</a></li><li class="chapter-item expanded "><a href="6.html"><strong aria-hidden="true">142.</strong> Classifier-Free Diffusion Guidance</a></li><li class="chapter-item expanded "><a href="5.html"><strong aria-hidden="true">143.</strong> Cascaded Diffusion Models for High Fidelity Image Generation</a></li><li class="chapter-item expanded "><a href="4.html"><strong aria-hidden="true">144.</strong> LEARNING ENERGY-BASED MODELS BY DIFFUSIONRECOVERY LIKELIHOOD</a></li><li class="chapter-item expanded "><a href="3.html"><strong aria-hidden="true">145.</strong> On Distillation of Guided Diffusion Models</a></li><li class="chapter-item expanded "><a href="2.html"><strong aria-hidden="true">146.</strong> Denoising Diffusion Implicit Models</a></li><li class="chapter-item expanded "><a href="1.html"><strong aria-hidden="true">147.</strong> PROGRESSIVE DISTILLATION FOR FAST SAMPLING OF DIFFUSION MODELS</a></li><li class="chapter-item expanded "><a href="125.html"><strong aria-hidden="true">148.</strong> Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions</a></li><li class="chapter-item expanded "><a href="124.html"><strong aria-hidden="true">149.</strong> ControlVideo: Training-free Controllable Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="123.html"><strong aria-hidden="true">150.</strong> Pix2Video: Video Editing using Image Diffusion</a></li><li class="chapter-item expanded "><a href="122.html"><strong aria-hidden="true">151.</strong> Structure and Content-Guided Video Synthesis with Diffusion Models</a></li><li class="chapter-item expanded "><a href="121.html"><strong aria-hidden="true">152.</strong> MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model</a></li><li class="chapter-item expanded "><a href="120.html"><strong aria-hidden="true">153.</strong> MotionDirector: Motion Customization of Text-to-Video Diffusion Models</a></li><li class="chapter-item expanded "><a href="119.html"><strong aria-hidden="true">154.</strong> Dreamix: Video Diffusion Models are General Video Editors</a></li><li class="chapter-item expanded "><a href="118.html"><strong aria-hidden="true">155.</strong> Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="117.html"><strong aria-hidden="true">156.</strong> TokenFlow: Consistent Diffusion Features for Consistent Video Editing</a></li><li class="chapter-item expanded "><a href="116.html"><strong aria-hidden="true">157.</strong> DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing</a></li><li class="chapter-item expanded "><a href="115.html"><strong aria-hidden="true">158.</strong> Content Deformation Fields for Temporally Consistent Video Processing</a></li><li class="chapter-item expanded "><a href="113.html"><strong aria-hidden="true">159.</strong> PFNN: Phase-Functioned Neural Networks</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">ReadPapers</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/CaterpillarStudyGroup/ReadPapers" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main><div class="sidetoc"><nav class="pagetoc"></nav></div>
                        <h1 id="tstmotion-training-free-scene-aware-text-to-motion-generation"><a class="header" href="#tstmotion-training-free-scene-aware-text-to-motion-generation">TSTMotion: Training-free Scene-aware Text-to-motion Generation</a></h1>
<hr />
<h2 id="研究背景与问题"><a class="header" href="#研究背景与问题"><strong>研究背景与问题</strong></a></h2>
<ul>
<li>
<p><strong>文本到动作生成（Text-to-Motion）</strong><br />
现有技术多基于空白背景生成人体动作序列，但现实场景中人类需与3D环境中的物体交互（如“坐在远离电视的沙发上”）。传统方法无法满足场景感知需求，导致生成动作与场景逻辑冲突。</p>
</li>
<li>
<p><strong>现有方法局限性</strong></p>
<ol>
<li><strong>数据依赖性强</strong>：现有数据集（如HUMANISE）规模小、场景单一（仅室内）、动作有限（行走、坐、躺、站）；</li>
<li><strong>算法复杂度高</strong>：基于强化学习（RL）的方法需要细粒度标注且面临RL训练难题；</li>
<li><strong>输入低效性</strong>：直接输入无序点云数据效率低下。</li>
</ol>
</li>
</ul>
<hr />
<h2 id="核心创新tstmotion框架"><a class="header" href="#核心创新tstmotion框架"><strong>核心创新：TSTMotion框架</strong></a></h2>
<p><strong>目标</strong>：无需训练专用场景感知模型，直接利用现有“空白背景生成器”（如Motion Diffusion Models）实现场景感知的文本驱动动作生成。</p>
<p><strong>基于基础模型的三组件设计</strong>：</p>
<p><img src="./assets/93-%E5%9B%BE2.png" alt="" /></p>
<h3 id="场景编译器scene-compiler"><a class="header" href="#场景编译器scene-compiler"><strong>场景编译器（Scene Compiler）</strong></a></h3>
<p><strong>功能</strong>：将3D场景转换为结构化表示（如语义地图），提取物体位置、空间关系等上下文信息，解决点云输入的低效问题。</p>
<h4 id="1-识别recognition"><a class="header" href="#1-识别recognition"><strong>1. 识别（Recognition）</strong></a></h4>
<ul>
<li><strong>输入</strong>：原始3D场景点云 \( S_{3D} \)。</li>
<li><strong>操作</strong>：
<ul>
<li><strong>多视角渲染</strong>：从不同角度渲染点云，生成 \( M \) 张2D图像（覆盖俯视、侧视等视角），解决点云单视角遮挡问题。</li>
</ul>
</li>
</ul>
<blockquote>
<p>通过多角度渲染解决点云遮挡问题，提升物体识别完整性与鲁棒性。</p>
</blockquote>
<ul>
<li><strong>物体识别</strong>：使用预训练的<strong>图像标签模型</strong>（如CLIP或ResNet）识别每张图像中的物体类别（如“沙发”“电视”），合并所有视角结果生成<strong>统一物体词汇表</strong>（避免重复）。</li>
<li><strong>输出</strong>：场景中所有物体的类别列表（如 {沙发, 电视, 茶几}）。</li>
</ul>
<h4 id="2-分割segmentation"><a class="header" href="#2-分割segmentation"><strong>2. 分割（Segmentation）</strong></a></h4>
<ul>
<li><strong>输入</strong>：物体词汇表 + 原始点云 \( S_{3D} \)。</li>
<li><strong>操作</strong>：
<ul>
<li><strong>语义分割</strong>：基于词汇表，使用<strong>3D分割器</strong>（如PointNet++或3D-SparseConv）将点云中每个点分类到对应物体（如标记属于“沙发”的点）。</li>
<li><strong>实例分割</strong>：区分同一类别的不同物体实例（如两个独立的“椅子”）。</li>
</ul>
</li>
<li><strong>输出</strong>：带语义标签的点云，每个点标记所属物体类别及实例ID。</li>
</ul>
<h4 id="3-目标定位locating-target"><a class="header" href="#3-目标定位locating-target"><strong>3. 目标定位（Locating Target）</strong></a></h4>
<ul>
<li><strong>输入</strong>：分割后的点云 + <strong>文本指令</strong>（如“坐在远离电视的沙发上”）。</li>
<li><strong>操作</strong>：
<ul>
<li><strong>边界框简化</strong>：将每个物体实例的点云包裹为<strong>轴对齐包围盒（AABB）</strong>，记录其中心坐标、长宽高。</li>
<li><strong>LLM语义解析</strong>：
<ul>
<li>将文本指令输入大语言模型（如GPT-4），解析出目标物体（“沙发”）和约束条件（“远离电视”）。</li>
<li>结合包围盒空间坐标，定位目标物体（如筛选出距离电视最远的沙发实例）。</li>
</ul>
</li>
</ul>
</li>
<li><strong>输出</strong>：目标物体的包围盒坐标及空间约束关系（如“沙发A中心坐标(x,y,z)，与电视B距离&gt;2米”）。</li>
</ul>
<blockquote>
<p>用包围盒代替原始点云，显著降低后续计算复杂度（从数万点→数十个包围盒）。</p>
</blockquote>
<h4 id="4-路网图生成road-map"><a class="header" href="#4-路网图生成road-map"><strong>4. 路网图生成（Road Map）</strong></a></h4>
<ul>
<li><strong>输入</strong>：所有物体的包围盒 + 目标物体位置。</li>
<li><strong>操作</strong>：
<ul>
<li><strong>投影到XOY平面</strong>：将3D包围盒投影至水平地面（XOY平面），生成2D矩形区域。</li>
<li><strong>标记可通行区域</strong>：
<ul>
<li><strong>障碍物</strong>：非目标物体的投影区域（如电视、茶几的投影区标记为障碍）。</li>
<li><strong>目标区域</strong>：目标物体（沙发）的投影区标记为终点。</li>
</ul>
</li>
<li><strong>路径规划基底</strong>：输出栅格化路网图，标识可行走区域与障碍物（见下图示意）。</li>
</ul>
</li>
<li><strong>输出</strong>：二值化路网图（0=障碍，1=可通行）。</li>
</ul>
<h4 id="5-高度图生成height-map"><a class="header" href="#5-高度图生成height-map"><strong>5. 高度图生成（Height Map）</strong></a></h4>
<ul>
<li><strong>输入</strong>：目标物体的点云（如沙发点云）。</li>
<li><strong>操作</strong>：
<ul>
<li><strong>投影到XOY平面</strong>：将目标点云垂直投影至地面，保留每个点的Z轴高度值。</li>
<li><strong>栅格化处理</strong>：将投影区域划分为网格，每个网格单元记录最大高度（表征物体形状起伏）。</li>
<li><strong>形状编码</strong>：生成灰度图像，亮度表示高度（如高亮区域=沙发坐面，暗区=扶手）。</li>
</ul>
</li>
<li><strong>输出</strong>：目标物体的高度分布图（用于接触点预测与动作可行性判断）。</li>
</ul>
<blockquote>
<p>路网图和高度图要求场景中的所有东西都是在地上。不能在空中。<br />
<strong>路网图</strong>专注宏观路径规划（避开障碍）；<br />
<strong>高度图</strong>专注微观动作适配（如坐面高度决定弯曲关节角度）。</p>
</blockquote>
<h3 id="动作规划器motion-planner"><a class="header" href="#动作规划器motion-planner"><strong>动作规划器（Motion Planner）</strong></a></h3>
<p><strong>功能</strong>：将文本指令与场景编译器输出的<strong>空间辅助信息</strong>（路网图Road Map + 高度图Height Map）结合，生成初步动作指导（如轨迹、接触点）。</p>
<h4 id="1-骨架序列表示"><a class="header" href="#1-骨架序列表示"><strong>1. 骨架序列表示</strong></a></h4>
<ul>
<li><strong>定义</strong>：骨架序列 \( s \in \mathbb{R}^{N \times J \times 3} \)，其中：
<ul>
<li>\( N \)：动作帧数（如30帧对应1秒动作）</li>
<li>\( J \)：人体关节数（如SMPL模型的24关节）</li>
<li>3：关节在3D空间中的坐标（x, y, z）</li>
</ul>
</li>
<li><strong>稀疏掩码</strong>：引入二进制掩码 \( M_{\text{mask}} \in {0,1}^{N \times J} \)，仅标记<strong>与场景交互相关的关键关节</strong>（如“坐”动作需关注髋关节、膝关节）。</li>
</ul>
<h4 id="2-基于llm的分步推理"><a class="header" href="#2-基于llm的分步推理"><strong>2. 基于LLM的分步推理</strong></a></h4>
<ul>
<li><strong>输入</strong>：
<ul>
<li>文本指令 \( d \)（如“坐在远离电视的沙发上”）</li>
<li>场景编译器输出的路网图（可行走区域） + 高度图（目标物体形状）</li>
</ul>
</li>
<li><strong>提示工程</strong>：通过设计结构化提示模板（Prompt Template），将任务分解为子问题，引导LLM逐步推理：
<pre><code class="language-python">prompt = &quot;&quot;&quot;
你是一个动作规划专家，请根据以下信息生成人体动作骨架序列：
1. 场景路网图：{road_map_description}  
2. 目标高度图：{height_map_description}  
3. 文本指令：“{text_instruction}”  
请按步骤思考：
Step 1: 确定动作类型（如行走、坐下）及目标物体接触区域。
Step 2: 根据路网图规划从起点到目标的移动路径（坐标序列）。
Step 3: 结合高度图预测接触点（如沙发坐面中心坐标）。
Step 4: 生成关键关节（髋、膝、踝）的3D轨迹，避免与场景碰撞。
&quot;&quot;&quot;
</code></pre>
</li>
</ul>
<blockquote>
<p>Step1：LLM解析文本指令，提取<strong>动作类型</strong>（如“坐”）和<strong>场景约束</strong>（如“远离电视”）。<br />
Step2：<strong>宏观路径</strong>：基于路网图，使用LLM内置的空间推理能力生成从起点到目标物体的移动路径（如绕过障碍物的折线坐标序列）。<br />
Step3：<strong>高度图分析</strong>：LLM根据目标高度图（如沙发坐面高度≈0.5m）预测接触点坐标（如髋关节目标位置(x, y, 0.5)）。<br />
Step4：<strong>关键关节轨迹</strong>：LLM生成指定关节（由掩码 \( M_{\text{mask}} \) 标记）的3D轨迹。</p>
</blockquote>
<ul>
<li><strong>输出</strong>：稀疏骨架序列 \( s[M_{\text{mask}}] \)（如仅包含髋、膝、踝关节的轨迹）。</li>
<li><strong>示例</strong></li>
</ul>
<ol>
<li><strong>输入</strong>：
<ul>
<li>路网图：标记沙发位置为终点，电视区域为障碍。</li>
<li>高度图：沙发坐面高度=0.5m，靠背高度=0.9m。</li>
</ul>
</li>
<li><strong>LLM推理</strong>：
<ul>
<li>Step 1: 动作类型=“坐”，接触区域=沙发坐面中心。</li>
<li>Step 2: 路径=从起点直线移动至沙发（无障碍需绕行）。</li>
<li>Step 3: 接触点坐标=(2.0, 3.0, 0.5)。</li>
<li>Step 4: 生成髋关节轨迹（从(2.0, 1.0, 1.0)降至(2.0, 3.0, 0.5)），膝关节弯曲角度随时间增加。</li>
</ul>
</li>
<li><strong>输出骨架序列</strong>：
<ul>
<li>帧1-10：髋关节从站立高度降至坐面高度，膝关节逐渐弯曲。</li>
<li>帧11-30：保持坐姿稳定，无垂直移动。</li>
</ul>
</li>
</ol>
<h3 id="对齐的扩散动作模型aligned-motion-diffusion-model"><a class="header" href="#对齐的扩散动作模型aligned-motion-diffusion-model"><strong>对齐的扩散动作模型（Aligned Motion Diffusion Model）</strong></a></h3>
<p><strong>功能</strong>：在<strong>不重新训练模型</strong>的前提下，将<strong>动作规划器生成的指导</strong>嵌入到预训练的空白背景动作扩散模型（如Motion Diffusion Model）中，生成符合场景约束的动作序列。突破传统扩散模型对场景信息的“盲生成”，实现开放域场景的动态适配。</p>
<h4 id="1-对齐动作指导modification-1符合motion-planner生成的骨架序列--sm_textmask-"><a class="header" href="#1-对齐动作指导modification-1符合motion-planner生成的骨架序列--sm_textmask-">1. <strong>对齐动作指导</strong>（Modification 1）：符合Motion Planner生成的骨架序列 \( s[M_{\text{mask}} \)；</a></h4>
<p>在推断时，先用FK计算出关节的位置，计算预测位置与期望位置之间的损失，
通过反向传播计算梯度，调整模型预测\( \hat{x}_0^k \)：</p>
<p>$$
\hat{x} _ 0^k \leftarrow \hat{x} _ 0^k - \lambda \cdot \nabla _ {x_k} L_ {\text{align}}
$$</p>
<blockquote>
<p>方法类似于<a href="./85.html">OmniControl</a>的方法</p>
</blockquote>
<h4 id="2-避免场景穿透modification-2减少与3d场景的几何重叠"><a class="header" href="#2-避免场景穿透modification-2减少与3d场景的几何重叠">2. <strong>避免场景穿透</strong>（Modification 2）：减少与3D场景的几何重叠。</a></h4>
<p>使用符号距离函数 \( \text{SDF}(p, P) \) 计算人体网格点 \( p \) 到场景 \( P \) 的最小距离，以及在P内部还是外部。<br />
定义损失函数 \( L_{\text{scene}} \)，仅惩罚穿透点（SDF负值部分）：
$$
L_{\text{scene}} = \sum_{p \in \text{SMPL}(\hat{x}_0^k)} \text{ReLU}(-\text{SDF}(p, P))
$$
通过梯度更新减少穿透，修正方法与上面相同。</p>
<h4 id="技术挑战与解决方案"><a class="header" href="#技术挑战与解决方案"><strong>技术挑战与解决方案</strong></a></h4>
<ul>
<li><strong>梯度冲突</strong>：若两个修改的梯度方向相反，可能导致震荡。<br />
<strong>解决方案</strong>：采用交替更新策略（先对齐动作，再消除穿透），或动态调整 \( \lambda, \eta \)。</li>
<li><strong>计算效率</strong>：SDF实时查询可能增加耗时。<br />
<strong>解决方案</strong>：预计算场景的SDF网格，或使用近似加速结构（如KD-Tree）。</li>
<li><strong>过修正风险</strong>：过度调整可能破坏动作自然性。<br />
<strong>解决方案</strong>：限制梯度步长 \( \lambda, \eta \)，或加入动作流畅性约束。</li>
</ul>
<h3 id="动作检查器motion-checker"><a class="header" href="#动作检查器motion-checker"><strong>动作检查器（Motion Checker）</strong></a></h3>
<p><strong>功能</strong>：通过物理合理性验证（如碰撞检测、动作可行性）优化动作指导，确保生成动作与场景动力学一致。</p>
<h4 id="1-迭代优化流程"><a class="header" href="#1-迭代优化流程"><strong>1. 迭代优化流程</strong></a></h4>
<ol>
<li><strong>首次生成</strong>：Motion Planner → Aligned MDM → Motion Checker。</li>
<li><strong>验证失败</strong>：
<ul>
<li>将失败原因反馈至Motion Planner，调整动作指导（如重新规划路径或接触点）。</li>
<li>重新执行生成与验证，通常仅需1次迭代即可通过。</li>
</ul>
</li>
<li><strong>终止条件</strong>：验证通过或达到最大迭代次数（如3次）。</li>
</ol>
<h4 id="2-自动化摘要"><a class="header" href="#2-自动化摘要"><strong>2. 自动化摘要</strong></a></h4>
<p>将骨架序列 \( s \) 和SMPL网格转换为自然语言描述，包括：</p>
<ul>
<li>关键关节轨迹（如髋关节移动路径）；</li>
<li>接触点坐标与目标物体位置对比；</li>
<li>穿透检测结果（如“左膝穿透沙发深度0.1m”）。</li>
</ul>
<h4 id="3-llm驱动的验证机制"><a class="header" href="#3-llm驱动的验证机制"><strong>3. LLM驱动的验证机制</strong></a></h4>
<ul>
<li><strong>输入</strong>：
<ul>
<li>生成的动作序列描述（如骨架序列可视化报告）；</li>
<li>原始文本指令与场景编译器输出的语义地图。</li>
</ul>
</li>
<li><strong>提示模板设计</strong>：
<pre><code class="language-python">prompt = &quot;&quot;&quot;
你是一个动作质检专家，请验证以下动作是否符合要求：
1. 文本指令：“{text_instruction}”
2. 场景约束：{scene_constraints}（如“沙发位置”“障碍物区域”）
3. 生成动作报告：{motion_report}（包含骨架轨迹、接触点、网格穿透检测结果）

请按步骤检查：
Step 1: 动作类型是否与指令一致？（如“坐” vs “蹲”）
Step 2: 移动路径是否避开所有障碍物？
Step 3: 接触点是否位于目标物体表面？
Step 4: 人体网格是否与场景发生穿透？
若任一步骤不通过，返回失败原因。
&quot;&quot;&quot;
</code></pre>
</li>
<li><strong>输出</strong>：验证结果（通过/不通过） + 失败原因（如“髋关节轨迹偏离沙发坐面”）。</li>
</ul>
<h4 id="4-失败案例处理"><a class="header" href="#4-失败案例处理"><strong>4. 失败案例处理</strong></a></h4>
<ul>
<li><strong>路径偏离</strong>：反馈至Motion Planner重新规划路径（如增大绕障距离）。</li>
<li><strong>语义不符</strong>：修正LLM解析逻辑（如明确“坐”与“蹲”的关节角度阈值）。</li>
<li><strong>场景穿透</strong>：增强Modification 2的梯度惩罚强度 \( \eta \)。</li>
</ul>
<hr />
<h2 id="关键技术突破"><a class="header" href="#关键技术突破"><strong>关键技术突破</strong></a></h2>
<ol>
<li>
<p><strong>无需训练的零样本生成</strong></p>
<ul>
<li>利用预训练基础模型（如CLIP、扩散模型）的泛化能力，避免对场景感知数据集的依赖。</li>
<li>通过“提示工程”将场景信息隐式编码至生成过程，而非显式训练。</li>
</ul>
</li>
<li>
<p><strong>场景-动作联合建模</strong></p>
<ul>
<li>将文本指令拆解为“动作语义”和“场景约束”两部分，分别由动作规划器和场景编译器处理。</li>
<li>示例：指令“坐在远离电视的沙发上” → 动作语义“坐” + 场景约束“沙发位置”+“与电视的距离”。</li>
</ul>
</li>
<li>
<p><strong>动态物理可行性验证</strong></p>
<ul>
<li>动作检查器引入轻量级物理仿真（如刚体动力学），快速验证动作合理性。</li>
<li>若规划动作导致碰撞或失衡，迭代调整接触点或运动轨迹。</li>
</ul>
</li>
</ol>
<hr />
<h2 id="实验与效果"><a class="header" href="#实验与效果"><strong>实验与效果</strong></a></h2>
<h3 id="实验1"><a class="header" href="#实验1">实验1</a></h3>
<p><img src="./assets/93-%E8%A1%A81.png" alt="" /></p>
<p>测试数据集：</p>
<ol>
<li>HUMANISE数据集：专注于室内场景的文本-动作-场景交互数据，包含有限动作（行走、坐、躺、站）。<br />
对比：</li>
<li>本文方法</li>
<li>DIMOS
结论：
关键结论：</li>
<li>DIMOS*（DIMOS + Scene Compiler）的Body-to-Goal Distance显著低于原始DIMOS，证明Scene Compiler的物体定位能力有效。</li>
<li>TSTMotion进一步优化路径规划与接触点预测，距离最低，体现Motion Planner与Aligned MDM的协同优势。</li>
<li>TSTMotion在语义对齐（如动作类型匹配）和碰撞避免上均优于基线，验证Motion Checker迭代优化的有效性。</li>
</ol>
<h3 id="实验2"><a class="header" href="#实验2">实验2</a></h3>
<p><img src="./assets/93-%E8%A1%A82.png" alt="" /></p>
<p>测试数据集：</p>
<ol>
<li>AffordMotion数据集：包含多样化室内外场景与复杂动作（如跑步、跳跃、抓取），强调动作-场景的物理交互合理性。<br />
对比：</li>
<li>本文方法</li>
<li>AffordMotion
结论：</li>
<li>R-Precision：文本与生成动作的匹配度，TSTMotion更高，体现Motion Planner对复杂指令的精准解析。</li>
<li>MultiModal Distance：生成动作多样性偏差，值越低说明多样性越好且符合文本，反映零样本生成避免过拟合的优势。</li>
<li>Contact Score：动作与目标物体的有效接触率，TSTMotion通过高度图引导接触点预测显著提升。</li>
<li>Non-Collision Score：无场景穿透的比例，得益于Modification 2的SDF梯度修正。</li>
<li>无需场景特定数据训练，适配室内外环境；</li>
</ol>
<hr />
<h2 id="应用价值"><a class="header" href="#应用价值"><strong>应用价值</strong></a></h2>
<ol>
<li><strong>游戏与影视制作</strong>
<ul>
<li>快速生成符合场景逻辑的角色动画（如NPC自动避障、与道具交互），降低人工制作成本。</li>
</ul>
</li>
<li><strong>具身智能（Embodied AI）</strong>
<ul>
<li>为机器人提供基于自然语言指令的场景适配行为规划（如“绕过桌子取水杯”）。</li>
</ul>
</li>
<li><strong>元宇宙与虚拟人</strong>
<ul>
<li>增强虚拟角色在开放环境中的自主交互能力，提升用户体验沉浸感。</li>
</ul>
</li>
</ol>
<hr />
<h2 id="未来挑战"><a class="header" href="#未来挑战"><strong>未来挑战</strong></a></h2>
<ol>
<li><strong>长时序动作连贯性</strong><br />
当前方法对多步骤复杂指令（如“走到沙发旁，坐下后拿起书”）的时序一致性仍需优化。</li>
<li><strong>多智能体交互</strong><br />
需扩展至多人协作或对抗场景（如“两人搬箱子”）。</li>
<li><strong>实时性提升</strong><br />
物理验证模块的计算效率需进一步优化以满足实时生成需求。</li>
</ol>
<hr />
<h2 id="总结"><a class="header" href="#总结"><strong>总结</strong></a></h2>
<p>TSTMotion通过<strong>无训练框架设计</strong>和<strong>场景-动作联合建模</strong>，解决了传统文本到动作生成的场景割裂问题，为开放域动态交互任务提供了高效、低成本的解决方案。其核心思想“<strong>利用基础模型知识代替专用训练</strong>”或成为未来跨模态生成任务的重要范式。</p>
<h2 id="相似工作对比"><a class="header" href="#相似工作对比">相似工作对比</a></h2>
<h3 id="llm-based-human-motion生成任务对比"><a class="header" href="#llm-based-human-motion生成任务对比">LLM based human motion生成任务对比</a></h3>
<ol>
<li>本文，2025.5</li>
<li>OmniControl，2024，<a href="./85.html">link</a></li>
</ol>
<table><thead><tr><th></th><th>本文</th><th>OmniControl</th></tr></thead><tbody>
<tr><td>控制信息-&gt;prompt</td><td>动作规划器生成</td><td>特定任务的指令的简单组合</td></tr>
<tr><td>prompt-&gt;motion</td><td>1. prompt作为MDM的condition，使用MDM生成<br> 2. 推断过程使用prompt中的具体的数据信息做动作优化</td><td>LLM + LoRA</td></tr>
<tr><td>motion后处理</td><td>1. 借助LLM判断生成动作是否合理<br>2. 根据判断结果重新调整引导生成的LLM</td><td>无</td></tr>
<tr><td>生成效果</td><td>动作规划器能支持更复杂的控制需求<br>后处理能进一步优化生成动作</td><td>借助预训练的LLM简单训练过程，但生成结果没有优势</td></tr>
</tbody></table>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="94.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                            <a rel="next" href="92.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="94.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                    <a rel="next" href="92.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="theme/pagetoc.js"></script>
        <script type="text/javascript" src="theme/mermaid.min.js"></script>
        <script type="text/javascript" src="theme/mermaid-init.js"></script>
    </body>
</html>

# High-Resolution Image Synthesis with Latent Diffusion Models

## 核心问题是什么?

### 目的

在有限的计算资源上进行 DM 训练，同时保持其质量和灵活性

### 现有方法

现有模型通常直接在像素空间中运行，且顺序评估，因此训练和推理成本很高。

### 本文方法

将DM应用在强大的预训练自动编码器的潜在空间中。  
1. 在这种表示上训练扩散模型首次允许在复杂性降低和细节保留之间达到接近最佳的点，从而极大地提高了视觉保真度。
2. 通过将交叉注意力层引入模型架构中，我们将扩散模型转变为强大而灵活的生成器，用于一般条件输入（例如文本或边界框），并且以卷积方式使高分辨率合成成为可能。

### 效果

LDM 在图像修复和类条件图像合成方面取得了新的最先进分数，并在各种任务上实现了极具竞争力的性能，包括文本到图像合成、无条件图像生成和超分辨率，与基于像素的 DM 相比，同时显着降低了计算要求。

## 核心贡献是什么？

1.  **潜在扩散模型（LDMs）**：这是一种新型的生成模型，它通过在潜在空间中应用去噪自编码器序列来实现图像合成，能够在保持图像质量的同时，显著减少计算资源的需求。

2.  **高分辨率图像合成**：LDMs能够在高分辨率下生成复杂的自然场景图像，这在以往的技术中往往需要大量的计算资源。

3.  **计算效率**：与基于像素的扩散模型相比，LDMs在训练和推理时更加高效，因为它们在低维的潜在空间中进行操作，而不是在高维的像素空间中。

4.  **条件生成**：LDMs支持多种类型的条件输入，如文本或边界框，使得生成过程更加灵活和可控。

5.  **跨注意力层（Cross-Attention Layers）**：通过引入跨注意力层，LDMs能够将扩散模型转变为强大的生成器，用于处理一般的条件输入。

6.  **两阶段图像合成方法**：LDMs采用两阶段方法，首先通过感知压缩模型（如自编码器）降低数据的维度，然后在潜在空间中训练扩散模型，以学习数据的语义和概念组成。

7.  **感知压缩**：论文提出了一种基于感知损失和对抗性目标的图像压缩方法，以确保重建图像在视觉上与原始图像保持一致。

## 大致方法是什么？

![](./assets/8eb1b42dbf6d359abdd4d99278f04c45_3_Figure_3.png)

这种方法有几个优点： 
1. 计算效率更高，因为采样是在低维空间上执行的。
2. 利用从 UNet 架构继承的 DM 的归纳偏差 [71]，这使得它们对于具有空间结构的数据特别有效，因此减轻了先前方法所需的激进的、降低质量的压缩级别的需求 [23， 66]。
3. 通用压缩模型，其潜在空间可用于训练多个生成模型，也可用于其他下游应用，例如单图像 CLIP 引导合成 [25]。

### 感知图像压缩

|输入|输出|方法|
|---|---|---|
|image, W * H * 3|z，h * w * c|Encoder|
|z|image|Decoder|

f = W/w = H/h = 2 ** m

正则化：
1. 对学习到的latent潜伏施加与标准正态的 KL 惩罚，类似VAE
2. 在解码器中使用矢量量化层，类似VQGAN

与之前工作不同的是z是二维的，以保留其固有结构

### LDM

### 条件机制

通过交叉注意力机制增强其底层 UNet 主干网，将 DM 转变为更灵活的条件图像生成器 [97]，这对于学习各种输入模态的基于注意力的模型非常有效 [35,36]。

1. 领域的编码器 τθ，把条件y 投影到中间表示  
2. 使用cross attention把z和1混合起来，其中K和V来自1，Q来自z

## 训练与验证

### 数据集

### loss

### 训练策略

## 有效

8.  **灵活性和通用性**：LDMs的潜在空间可以被多次用于不同的生成任务，例如图像修复、超分辨率和文本到图像的合成。

## 局限性

## 启发

生成模型的生成图像的好坏，在于它生成的质量，对于图像而言，可以是像素之间的联系性、边界的清晰度、整体画面的结构性等。但不在于它的合理性，因为
1. 合理性无法衡量。因为生成一个没有见过的图像，不代表不合理。而能够生成没有见过的图像，正是它的创造性所在。  
2. 合理性也可以理解为生成的结果符合人类常识。但实际上生成模型并不真正理解人类常识。会生成符合人类常识的结果，只是因为对数据的偏见。  

因此，对于生成模型而言，要通过各种方法、策略来提升其生成质量。通过强化数据偏见来提升其合理性。

## 遗留问题

## 参考材料
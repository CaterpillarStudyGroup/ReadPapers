# Dreamgaussian4d: Generative 4d gaussian splatting

## 研究背景与问题

### 要解决的问题

4D内容生成

### 本文方法及优势

1. 图像到4D高斯溅射：我们首先使用DreamGaussianHD生成静态高斯溅射模型，然后通过基于六面体 (HexPlane) 的动态生成方法结合高斯变形技术生成动态内容；

2. 视频到视频纹理优化：我们利用预训练的图像到视频扩散模型，对生成的UV空间纹理贴图进行优化，同时增强其时间一致性。

|当前方法|存在的问题|本文方法|
|---|---|---|
|隐式表示 (NeRF)|低效|用 3D GS 显式、高效地表示静态基础，用 HexPlane 显式、高效地表示动态位移。|
|主流方法（如 MAV3D, Animate124, 4D-fy 等）依赖 Video SDS (视频分数蒸馏) 来从视频扩散模型中“蒸馏”运动信息。|这种方法具有固有的随机性和不稳定性（前文已分析）。|改为直接从一段“驱动视频”中学习运动模式。|
|动态NeRF|优化慢、运动不规则|将静态几何建模（GS）和动态运动建模（HexPlane位移）解耦。通过优化HexPlane预测每个高斯点在时间轴上的位移，将运动高效地“注入”到高质量的静态场景中|

DG4D将优化时间从数小时缩短至仅需几分钟，允许对生成的3D运动进行视觉控制，并生成可在3D引擎中真实渲染的动态网格。

## 主要贡献

1. 一个系统性的图像到4D生成框架，它协同利用了图像条件化的3D生成模型和视频生成模型。这允许直接控制和选择期望的3D内容及其运动，从而实现高质量且多样化的4D生成。
2. 显式地表示4D场景：使用 3D GS 表示基础场景，并使用 HexPlane 表示不同时间戳下的形变。这种空间变换的显式建模将4D生成时间从数小时显著缩短至仅需几分钟。
3. 一种视频到视频纹理优化策略，通过保持时间一致性，进一步提升了导出的动画网格的质量，使得该框架在实际应用部署中更加友好。

## 主要方法

[TODO] 图2

### 阶段1 从图像生成动态几何

#### DreamGaussianHD 生成静态3D GS

输入： 单张图像。  
输出：使用 DreamGaussianHD 从输入图像生成高质量的静态 3D GS。

#### Gaussian Deformation for Dynamic Generation

在静态 GS 基础上，优化一个时间相关的形变场（核心是 HexPlane）。预测每个高斯点在每个时间戳的形变。

##### 生成driving video

driving video来源： 图像到视频模型（如 SVD/Sora）根据同一张输入图像生成
使用driving video可避免了 SDS 的随机性和低效。

##### 用于 4D 表示的 HexPlane

[TODO] 图4

HexPlane 将一个 4D 场分解为六个特征平面，这些平面跨越每一对坐标轴（例如 XY, XZ, XT, YZ, YT, ZT）。

> Hexplane: A fast representation for dynamic scenes

这种分解将 4D 场表示为一组可学习的 4D 基函数的加权和。  

##### 静态到动态的初始化

形变模型应被初始化为在训练开始时预测零形变。

为了预测零形变，同时不影响梯度更新，我们采用以下策略：
1. 对最后几个线性层进行零初始化（zero-initialization）。
2. 采用若干残差连接。

##### 形变场优化

优化目标：

1. 参考视角监督：渲染图像与驱动视频对应帧之间的均方误差
2. 多视角监督：SDS

为了将运动从参考视角传播到整个 3D 模型（特别是参考视角下被遮挡的部分），我们利用 Zero-1-to-3-XL 来预测未见部分的形变。

优化参数(两种策略)：
1. 冻结静态 3D GS 的参数，只优化形变场 ϕ
2. 选择性地在优化过程中对静态 3D GS 进行微调

输出： 动画网格序列

### 阶段2 Video-to-Video Texture Refinement

Base Model: SVD

## 实验

### 图像 TO 4D

[TODO] 表2

### 视频 TO 4D

[TODO] 表3
# AttT2M:Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism

基于文本描述生成三维人体运动一直是近年来的研究热点。该任务要求生成的运动兼具多样性、自然性，并符合文本描述。由于人体运动固有的复杂时空特性，以及文本与运动间跨模态关系学习的难度，文本驱动运动生成仍是一个具有挑战性的问题。为解决这些难题，我们提出AttT2M——一种融合多视角注意力机制的双阶段方法：身体部位注意力与全局-局部运动-文本注意力。前者从运动嵌入视角出发，通过在VQ-VAE中引入身体部位时空编码器，学习更具表现力的离散潜在空间；后者基于跨模态视角，用于学习句子层级和词汇层级的运动-文本跨模态关联。最终通过生成式变换器实现文本驱动运动的合成。在HumanML3D和KIT-ML数据集上的大量实验表明，本方法在定性与定量评估上均优于当前最优成果，实现了细粒度合成和动作到运动的精准生成。代码详见：https://github.com/ZcyMonkey/AttT2M。
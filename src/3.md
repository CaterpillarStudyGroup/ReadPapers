# On Distillation of Guided Diffusion Models

## 核心问题是什么?

无分类器引导扩散模型的缺点是，它们在推理时的计算成本很高，因为它们需要评估两个扩散模型（一个类条件模型和一个无条件模型）数十到数百次。    
而现有的加速方法不适用于classifier-free guided diffusion models。  

## 核心贡献是什么？

提出了一种将无分类器引导扩散模型提炼为快速采样的模型的方法：给定一个预先训练的无分类器引导模型，我们首先学习一个模型来匹配结合条件模型和无条件模型，然后我们逐步将该模型提炼为需要更少采样步骤的扩散模型。

## 大致方法是什么？

第一阶段：引入单个学生模型来匹配两个老师扩散模型（条件模型和无条件模型）的组合输出。  
第二阶段：逐步将第一阶段学习的模型提炼为少步模型，见[PROGRESSIVE DISTILLATION FOR FAST SAMPLING OF DIFFUSION MODELS](https://caterpillarstudygroup.github.io/ReadPapers/1.html).   

## 有效

For standard diffusion models trained on the pixel-space, our approach is able to generate images visually comparable to that of the original model using as few as 4 sampling steps on ImageNet 64x64 and CIFAR-10, achieving FID/IS scores comparable to that of the original model while being up to 256 times faster to sample from.   
把蒸馏技术应用于 latent space，仅用 2 步 denoise 即可生成高质量结果，accelerating inference by at least 10-fold compared to existing methods on ImageNet 256x256 and LAION datasets.   
![](./assets/7065d96e4d7898643bd368640e73ff89_3_Figure_4_-11883325.png)
We further demonstrate the effectiveness of our approach on text-guided image editing and inpainting, where our distilled model is able to generate high-quality results using as few as 2-4 denoising steps.

## 缺陷

## 验证

## 启发

## 参考材料

1. https://blog.csdn.net/zjc910997316/article/details/131812691
2. https://caterpillarstudygroup.github.io/ImportantArticles/diffusion-tutorial-part/diffusiontutorialpart1.html
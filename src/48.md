# Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models

## 核心问题是什么?

### 目的

将 LDM 范式应用于高分辨率视频生成

### 现有方法
### 本文方法

1. 在图像上预训练 LDM
2. 通过向LDM引入时间维度，并对编码图像序列（即视频）进行微调，将图像生成器变成视频生成器。
3. 在时间上对齐扩散模型upsamplers，将它们转变为时间一致的视频超分辨率模型。

### 效果

#### 野外驾驶数据的生成

在分辨率为 512 × 1024 的真实驾驶视频上验证了我们的视频 LDM，实现了最先进的性能。

#### 文生成视频的创意内容创建

可以轻松利用现成的预训练图像 LDM，因为在这种情况下我们只需要训练时间对齐模型。

好处：可以将公开可用的、最先进的文本到图像 LDM 稳定扩散转换为高效且富有表现力的文本到视频模型，分辨率高达 1280 × 2048。且这种训练时间层的方式可以推广到不同的微调文本到图像 LDM。

## 核心贡献是什么？

1.  **高分辨率视频合成**：LDMs扩展了传统的图像合成模型，使其能够生成高分辨率的长视频，这在以前是一个计算成本非常高的任务。

2.  **潜在空间中的扩散模型**：与传统在像素空间中训练的模型不同，LDMs在潜在空间（latent space）中训练扩散模型，这可以减少计算和内存的需求。

3.  **时间维度的引入**：作者通过在潜在空间中引入时间维度，将图像生成器转换为视频生成器，并通过在编码的图像序列（即视频）上进行微调来实现这一点。

4.  **时间对齐的扩散模型上采样器**：通过时间对齐，可以将用于图像超分辨率的扩散模型上采样器转换为视频超分辨率模型，从而保持时间上的连贯性。    

> [&#x2753;] 时间上采样和空间上采样是否结合使用？

5.  **实际应用**：论文中提到了两个实际应用案例：野外驾驶数据的合成和文本到视频的创意内容创建。

6.  **预训练和微调**：LDMs可以利用现成的预训练图像LDMs，通过仅训练时间对齐模型来实现视频生成，这大大减少了训练成本。

7.  **个性化视频生成**：通过将训练好的时间层与不同微调的文本到图像LDMs结合，论文展示了个性化文本到视频生成的可能性。

8.  **计算效率**：通过在潜在空间中进行操作，LDMs能够在保持高分辨率输出的同时，降低训练和推理时的计算需求。

9.  **模型架构**：论文详细介绍了LDMs的架构，包括如何通过插入时间层来扩展现有的图像生成模型。

## 大致方法是什么？

> &#x2705; 所有工作的基本思路：(1) 先从小的生成开始 (2) 充分利用 T2I．  

### 图像生成模型 -> 视频生成模型

#### 引入 Temporal Layers

![](./assets/D3-57.png)     

Interleave spatial layers and temporal layers.    

The spatial layers are frozen, whereas temporal layers are trained. 
Temporal 层与原始的Spatial 层是以residual的形式结合，省略掉维度变换的过程，

$$
z1 = Spatial Attention(z) \\\\
z2 = Temporal Attention(z1)\\\\
z3 = \alpha z1 + (1-\alpha) z2
$$

#### 定义时间层

Temporal layers can be Conv3D **or** Temporal attentions.   

- **For Conv3D,** shape is [batch, channel, time, height, width]    
- **For Temporal attention,** shape is [batch \\(^\ast \\)height\\(^\ast \\) width, time, channel]    

&#x2705; 时序层除了时序 attention，还有 3D conv，是真正的 3D，但是更复杂，且计算、内存等消耗更大。   
&#x2705; 时序attention只是时间维度的融合，而3D conv是在f, w, h三个维度同时融合。  

实际上源码中同时使用了这两种temporal层。  

#### 引入time PE

sinusoidal embeddings [28, 89]

#### 训练方法

先移除Temporal Layer(令alpha=1)，只训练Spatial Layer。然后Fix Spatial Layer，只训练Temporal Layer。  

训练image model和video model使用相同的noise schedule。  

Optional context via learned down-sampling operation can be added for autoregressive generation.    

> [?] context是怎么来的？首帧信息？

#### Temporal Autoencoder Finetuning

原始的LDM是在图像上训练的，因此其Encoder&Decoder重建出的视频会闪烁。  
解决方法：在Decoder中引入temporal layer，并用视频重训。  

![](./assets/902df9454f73907587b244ee4d9813e8_2_Figure_3_1347203723.png)

> 上图：微调解码器中的时间层时，会fix编码器，因此编码器是独立处理帧。解码时强制跨帧进行时间相干重建。使用Video-aware Discrimator。下图：在 LDM 中，解码是逐帧进行的。  

### 长视频生成

### Upsampler diffusion model

: add 3D convolution layers   




## 训练

### 数据集

### RDS Videos   

### WebVid-10M   

### loss

### 训练策略


# 实验与结论  










## 有效

10. **实验结果**：论文提供了在真实驾驶视频数据集上的实验结果，展示了LDMs在生成高分辨率、长时间视频方面的性能。

11. **模型泛化能力**：通过在不同的微调设置下测试时间层，论文展示了这些层在不同模型检查点之间的泛化能力。

12. **文本到视频的生成**：论文展示了如何将文本到图像的LDM扩展为文本到视频的生成模型，并通过实验验证了其有效性。

## 局限性

## 启发

每一帧独立上采样（或Decoder）会严重破坏视频的帧间连续性。

## 遗留问题

## 参考材料
# Implicit Warping for Animation with Image Sets

NVIDIA   

## 核心问题是什么?

摘要     
我们提出了一种新的隐式图像动画框架，使用一组源图像通过驱动视频的运动进行传输。使用单个跨模态注意力层在源图像和驱动图像之间找到对应关系，从不同的源图像中选择最合适的特征，并扭曲所选特征。这与现有方法不同，现有方法使用显式基于光流的方法进行动画，该方法设计用于使用单个源，并且不能很好地扩展到多个源。我们的框架的挑选能力有助于它在多个数据集上实现最先进的结果，这些数据集使用单个和多源图像进行图像动画。     
https://deepimagination.cc/implicit-warping/

### 目的

.    
.    
.    
.    
.    

### 现有方法及局限性

.    
.    
.    
.    
.    

### 本文方法

.    
.    
.    
.    
.    

### 效果

.    
.    
.    
.    
.    

## 核心贡献是什么？

.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    

## 大致方法是什么？

![](./assets/51-手写公式1.png) 


1. 找出 source和driving 的 dense correspondence     
2. 基于 dense corresponence 的 warp，称其为 impicit warping     
作者认为一个CA可以完成 1 和 2.     

![](./assets/IT图2.png) 

不需要显式地提取光流，Q 和 K 的相似度描述了隐式的光流。  

### 构造 Q. K .V    

**K**: Source keypoint feature     
**Q**: driving keypoint feature   
**V**: Source image feature   

Q 的构造：    

![](./assets/IT手写2-2.png) 

spatial keypoint 表示，[:,:,i] 为以第i个 keypoint 位置为中心，特定均值和方差的二维一通道高斯。        
   
Q 和 K 的区别：UNet 的输入 concat (加权spatial keypoint 表示，source Image)     
优势：后面的模块与 keypoint 的个数无关。     

V 的构造：    
![](./assets/IT手写3-1.png) 


Q、K、V 在空间上是对应的。     

CA 存在的问题：所有的 key 与 query 都相似度不高时，也会从 key 中选择一个 Score 最高的，但这个 key 可能并不合适，或者选择任何一个 key 都不合适。      
  
解决方法：   
1. 增如额外的 KV 
2. 使用 dropout 来鼓励使用额外的 KV      

[&#x2753;] 额外的 KV 从哪里来？    

dropout 仅应用于 attention layer，不能应用于 Conv. 因为conv 是有位置关系的。    

### crsss - modal attention   

**Q**: q\\(\times \\)d, **K**: k\\(\times \\)d,**V**: k\\(\times \\)d'    
由于 source image 可能有多张，因此 q 和 k 不一定相等。    

$$
\begin{matrix}
 Q=Q+PE, &K=K+PE \\\\
  A=\text{Softmax} (\frac{Q\cdot  K^\top }{C}), & q\times k \\\\
\text{output feature}  =A\cdot V,& q\times d{}'  &
\end{matrix}
$$

MLP(concat(A \\(\cdot\\) concat(pixel\\(\cdot\\)K),Q))    

![](./assets/IT图3.png) 

目的：除了用 A warp 了 V，还 warp 了原图和 key，用于提取 skew, rotation 等“对 V 做加权平均”难以学到的信息，可提升生成质量和颜色一致性。    

效来提升：1- D attention layer ， spatial-reduction attention        

.  
.  
.  
.  
.  
.  
.  

## 训练

.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  

### 数据集

TalkingHead-1kH      
VoxCeleb2        
TED Talk      


### 评价指标   

生成图像质量：PSNR，L1，LPIPS,FID    
运动相似度：AKD (average keypoint distance)    
MKR(missing keypoint ratio)    
 
.  
.  
.  
.  
.  
.  

### loss

.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  

### 训练策略

预训练模型：AA-PCA    
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  

## 实验与结论

**实验1:** 横向对比    

脸部驱动：   
1. FOMM    
2. AA- PCA    
3. 本文方法     

1 和 2 不支持多 source Image 因为分别用每个 source Image 做 warping, 并在 Warp feature map 上做平均。    

**效果：** 单 source    

![](./assets/IT表1.png) 

3 的生成质量更高且与动作一致性更好。    

**结论：**    
3 所使用的 attention 是 global 的，因此可以从空间上距离较远的位置提取特征，但显式光流的方法，需要借助较大的光流才能获得远处的特征。    

**效果：** 多 source    

![](./assets/IT表2.png) 

Source Image 为少于 180 帧的连续帧序列。    

随着 Source image 数量的增加，3 的效果会更好，而1、2的效果会更差。    

**分析：** 用同一 pose warp 不同的 image，得到的结果之间会有 misalignment. 但 1、2 不知道该选取哪个 warp 结果。而 3 使用 global attention 从所有 source 中提取信息。   

**实验2：** Ablation     
**效果：**   
 
![](./assets/IT表4.png) 

**结论：** 残差结构与额外的 k／V 对结果都有提升。   

**实验3：** 可视化 strength    
**效果：**    

![](./assets/IT图6.png) 


### Loss   

1. GAN Loss(参考face-vidzvid)    
2. perceptual loss (参考 VCG-19)     
3. equivariance loss (参考 FoMM)        
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  

## 有效

.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  


## 局限性

缺少生成能力，例如给背面生成正面，生成极端表情等。      
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  

## 启发

.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  

## 遗留问题

.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  

## 参考材料

https://deepimagination.cc/implicit-warping/
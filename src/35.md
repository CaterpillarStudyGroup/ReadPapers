# MagicPony: Learning Articulated 3D Animals in the Wild

## 核心问题是什么?

在给定单个测试图像作为输入的情况下，预测马等有关节动物的 3D 形状、关节、视点、纹理和光照。

### 本文方法

我们提出了一种名为 MagicPony 的新方法，该方法仅从特定类别对象的野外单视图图像中学习，并且对变形拓扑的假设最少。  
其核心是**铰接形状**和**外观的隐式-显式表示**，结合了神经场和网格的优点。  
为了帮助模型理解物体的形状和姿势，我们提炼了已有的自监督**ViT**，并将其融合到 3D 模型中。  
为了克服视点估计中的局部最优，我们进一步引入了一种新的**视点采样**方案，无需额外的训练成本。 

### 效果

MagicPony 在这项具有挑战性的任务上表现优于之前的工作。尽管它仅在真实图像上进行训练，但在重建艺术方面表现出出色的泛化能力。

## 核心贡献是什么？

1.  **隐式-显式表示（Implicit-Explicit Representation）**：MagicPony结合了神经场和网格的优势，使用隐式表示来定义形状和外观，并通过Differentiable Marching Tetrahedra（DMTet）方法即时转换为显式网格，以便进行姿态调整和渲染。

2.  **自监督知识蒸馏**：为了帮助模型理解对象的形状和姿态，论文提出了一种机制，将现成的自监督视觉变换器（如DINO-ViT）捕获的知识融合到3D模型中。

3.  **多假设视角预测方案**：为了避免在视角估计中陷入局部最优，论文引入了一种新的视角采样方案，该方案在每次迭代中只采样单一假设，且几乎不需要额外的训练成本。

4.  **层次化形状表示**：从通用模板到具体实例的层次化形状表示方法，允许模型学习类别特定的模板形状，并预测实例特定的变形。

5.  **外观和着色分解**：将对象的外观分解为反照率和漫反射着色，假设了一种兰伯特照明模型，并使用可微分的延迟网格渲染技术。

6.  **训练目标和损失函数**：论文详细描述了训练过程中使用的损失函数，包括重建损失、特征损失、掩码损失、正则化项和视角假设损失。

8.  **少监督学习**：MagicPony在几乎没有关于变形拓扑的假设下，仅使用对象的2D分割掩码和3D骨架拓扑的描述进行训练，展示了在少监督情况下学习3D模型的能力。


## 大致方法是什么？

![](./assets/15ce34e23c43fbfa3c5fe19891b315ea_3_Figure_2_-1265732209.png)

左：给定特定类别动物的单视图图像集合，我们的模型使用“隐式-显式”表示来学习特定于类别的先验形状，以及可以用于自监督特征渲染损失的特征字段。  
中：根据从训练图像中提取的特征，对先前的形状进行变形、铰接和着色。  
中右：为了克服视点预测中的局部最小值，我们引入了一种有效的方案，该方案可以探索多种假设，而基本上不需要额外的成本。  
右：除了fix Image Encoder之外，整个管道都经过端到端的重建损失训练。

### Implicit-Explicit 3D Shape Representation

#### 隐式

目的：用于优化出一个通用的Mesh。
方法：SDF。定义隐式Mesh函数为  

S = {x ∈ R3|s(x) = 0}

#### 显式

目的：方便后面的deformation  
方法：DMTet

#### 优化过程

1. 初始时，用SDF描述一个椭圆作为prior shape。    
2. 空间采样，使用DMTet生成prior Mesh的shape。  
3. 用下文的方法驱动调整和驱动prior mesh，得到deformed mesh。  
4. 通过deformed mesh与GT的重建Loss，优化SDF。  

得到此类别所有对象共用的template。  

```python
# 代码有删减
class PriorPredictor(nn.Module):
    def __init__(self, cfgs):
        super().__init__()
        self.netShape = DMTetGeometry(。。。)

    def forward(self, total_iter=None, is_training=True):
        # 得到prior template mesh
        prior_shape = self.netShape.getMesh(total_iter=total_iter, jitter_grid=is_training)
        return prior_shape, self.netDINO

class DMTetGeometry(torch.nn.Module):
    def get_sdf(self, pts=None, total_iter=0):
        if pts is None:
            pts = self.verts
        if self.symmetrize:
            xs, ys, zs = pts.unbind(-1)
            pts = torch.stack([xs.abs(), ys, zs], -1)  # mirror -x to +x
        sdf = self.mlp(pts)

        if self.init_sdf is None:
            pass
        elif type(self.init_sdf) in [float, int]:
            sdf = sdf + self.init_sdf
        elif self.init_sdf == 'sphere':
            init_radius = self.grid_scale * 0.25
            init_sdf = init_radius - pts.norm(dim=-1, keepdim=True)  # init sdf is a sphere centered at origin
            sdf = sdf + init_sdf
        elif self.init_sdf == 'ellipsoid':
            rxy = self.grid_scale * 0.15
            xs, ys, zs = pts.unbind(-1)
            init_sdf = rxy - torch.stack([xs, ys, zs/2], -1).norm(dim=-1, keepdim=True)  # init sdf is approximately an ellipsoid centered at origin
            sdf = sdf + init_sdf
        else:
            raise NotImplementedError

        return sdf

    def getMesh(self, material=None, total_iter=0, jitter_grid=True):
        # Run DM tet to get a base mesh，预置的一些空间中的点
        v_deformed = self.verts

        if jitter_grid and self.jitter_grid > 0:
            jitter = (torch.rand(1, device=v_deformed.device)*2-1) * self.jitter_grid * self.grid_scale
            v_deformed = v_deformed + jitter

        # 计算这些点的sdf
        self.current_sdf = self.get_sdf(v_deformed, total_iter=total_iter)
        # 根据预置点的sdf找出0表面，并生成Mesh
        verts, faces, uvs, uv_idx = self.marching_tets(v_deformed, self.current_sdf, self.indices)
        self.mesh_verts = verts
        return mesh.make_mesh(verts[None], faces[None], uvs[None], uv_idx[None], material)

```
### Articulated Shape Modelling and Prediction

|输入|输出|方法|
|---|---|---|
|image|Image Mask|PointRend|
|image, mask|a set of key and output tokens|DINO-ViT|
|local keys token \\(\Phi_k \in R^{D\times H_p \times W_p}\\)|deformation|
|local outputs token \\(\Phi_o \in R^{D\times H_p \times W_p}\\)|appreance|
|global keys token φk|local keys token Φk|small convolutional network|
|global output token φo|local output token Φo|small convolutional network|
|template shape \\(V_{pr,i}\\)<br>global keys token φk|mesh displacement ∆Vi|MLP<br>许多对象是双边对称的，因此通过镜像来强制先前的 Vpr 和 ∆V 对称|
| \\(V_{pr,i}\\)<br>∆Vi|Vins|
|ξ1，Vpr|bone在图像上的像素 ub|根据ξ1把每个rest状态下的bone投影到图像上|
|patch key feature map Φk(ub)<br>ub|local feature|从投影像素位置 ub 处的 Φk(ub) 中采样局部特征。|
|image features Φk and φk|local rotation ξ2:B|The viewpoint ξ1 ∈ SE(3) is predicted separately|
|Vins<br>动作参数ξ<br>a set of rest-pose joint locations Jb（启发式方法）<br>skinning weights（relative proximity to each bone）|Vi|LBS|

### Appearance and Shading

|输入|输出|方法|
|---|---|---|
|x, global output token φo|albedo|MLP fa|
|global output token φo|主光源方向l<br>环境和漫射强度ks, kd|MLP|
|3D coordinates Mesh V<br>all pixels u|V中投影到u上的顶点x(u)||
|x(u)，fa|像素u对应的颜色 a|
|ks, kd, l, n, 像素u对应的颜色 a|像素u最终的颜色|
|ks, kd, l, n, 像素u对应的颜色 a|像素u的MASK|

### Viewpoint Prediction

我们提供了一种稳健且高效的方法，该方法利用多假设预测管道中的自我监督对应关系。

**这一段没看懂，参考Neural Feature Fusion Fields: 3D Distillation of Self-Supervised 2D Image Representations**

难点：学习对象视点的一个主要挑战是重建目标中存在多个局部最优。  
解决方法：我们提出了一种统计探索多个观点的方案，但在每次迭代时仅采样一个观点，因此没有额外成本。  

|输入|输出|方法|
|---|---|---|
|global keys token φk|4个假设的viewpoint rotation<br>score σk，第k个假设的可能性|MLP|

学习 σk 的简单方法是对多个假设进行采样并比较它们的重建损失以确定哪一个更好。然而，这很昂贵，因为它需要使用所有假设来渲染模型。相反，我们建议为每次训练迭代采样一个假设，并通过最小化目标来简单地训练 σk 来预测预期重建损失 Lk

## 训练与验证

### 数据集

DOVE  
Weizmann Horse Database [4], PASCAL [10] and Horse-10 Dataset  
Microsoft COCO Dataset  
CUB  

### loss

### 训练策略

1. 首先对视点假设k进行采样，计算重建损失  
2. 然后，将正则化项和视点假设损失相加，得到最终目标.  
每次梯度评估仅采用一个视点样本 k。为了进一步提高采样效率，我们根据学习到的概率分布pk4对视点进行采样：为了提高训练效率，我们在 80% 的时间对视点 k* = argmaxk pk 进行采样，并在 20% 的时间均匀随机采样。

## 有效

7.  **广泛的实验验证**：在动物类别上进行了广泛的实验，包括马、长颈鹿、斑马、奶牛和鸟类，并与先前的工作进行了定性和定量比较。

9.  **泛化能力**：尽管MagicPony仅在真实图像上训练，但它展示了对绘画和抽象绘画的出色泛化能力。

## 局限性

## 启发

## 遗留问题

## 参考材料

1. 项目页面: https://3dmagicpony.github.io/
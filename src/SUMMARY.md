# Summary

[ReadPapers]()

- [Introduction](README.md)
- [SOAP: Style-Omniscient Animatable Portraits](95.md)
- [Neural Discrete Representation Learning](94.md)
- [TSTMotion: Training-free Scene-aware Text-to-motion Generation](93.md)
- [Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis](92.md)
- [A lip sync expert is all you need for speech to lip generation in the wild](91.md)
- [MUSETALK: REAL-TIME HIGH QUALITY LIP SYN-CHRONIZATION WITH LATENT SPACE INPAINTING](90.md)
- [LatentSync: Audio Conditioned Latent Diffusion Models for Lip Sync](89.md)
- [T2m-gpt: Generating human motion from textual descriptions with discrete representations](88.md)
- [Motiongpt: Finetuned llms are general-purpose motion generators](87.md)
- [Guided Motion Diffusion for Controllable Human Motion Synthesis](86.md)
- [OmniControl: Control Any Joint at Any Time for Human Motion Generation](85.md)
- [Learning Long-form Video Prior via Generative Pre-Training](84.md)
- [Instant Neural Graphics Primitives with a Multiresolution Hash Encoding](83.md)
- [Magic3D: High-Resolution Text-to-3D Content Creation](82.md)
- [CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers](81.md)
- [One-Minute Video Generation with Test-Time Training](80.md)
- [Key-Locked Rank One Editing for Text-to-Image Personalization](79.md)
- [MARCHING CUBES: A HIGH RESOLUTION 3D SURFACE CONSTRUCTION ALGORITHM](78.md)
- [Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation](77.md)
- [NULL-text Inversion for Editing Real Images Using Guided Diffusion Models](76.md)
- [simple diffusion: End-to-end diffusion for high resolution images](75.md)
- [One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale](74.md)
- [Scalable Diffusion Models with Transformers](73.md)
- [All are Worth Words: a ViT Backbone for Score-based Diffusion Models](72.md)
- [An image is worth 16x16 words: Transformers for image recognition at scale](71.md)
- [eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers](70.md)
- [Photorealistic text-to-image diffusion models with deep language understanding||Imagen](69.md)
- [DreamFusion: Text-to-3D using 2D Diffusion](68.md)
- [GLIGEN: Open-Set Grounded Text-to-Image Generation](67.md)
- [Adding Conditional Control to Text-to-Image Diffusion Models](66.md)
- [T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models](65.md)
- [Multi-Concept Customization of Text-to-Image Diffusion](64.md)
- [An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion](63.md)
- [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](62.md)
- [VisorGPT: Learning Visual Prior via Generative Pre-Training](61.md)
- [NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation](60.md)
- [AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning](59.md)
- [ModelScope Text-to-Video Technical Report](58.md)
- [Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation](57.md)
- [Make-A-Video: Text-to-Video Generation without Text-Video Data](56.md)
- [Video Diffusion Models](55.md)
- [Learning Transferable Visual Models From Natural Language Supervision](54.md)
- [Implicit Warping for Animation with Image Sets](53.md)
- [Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models](52.md)
- [Motion-Conditioned Diffusion Model for Controllable Video Synthesis](51.md)
- [Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets](50.md)
- [UniAnimate: Taming Unified Video Diffusion Models for Consistent Human Image Animation](49.md)
- [Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models](48.md)
- [Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics](47.md)
- [A Recipe for Scaling up Text-to-Video Generation](46.md)
- [High-Resolution Image Synthesis with Latent Diffusion Models](45.md)
- [Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling](44.md)
- [数据集：HumanVid](43.md)
- [HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation](42.md)
- [StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation](41.md)
- [数据集：Zoo-300K](40.md)
- [Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion](39.md)
- [LORA: LOW-RANK ADAPTATION OF LARGE LAN-GUAGE MODELS](38.md)
- [TCAN: Animating Human Images with Temporally Consistent Pose Guidance using Diffusion Models](37.md)
- [GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians](36.md)
- [MagicPony: Learning Articulated 3D Animals in the Wild](35.md)
- [Splatter a Video: Video Gaussian Representation for Versatile Processing](34.md)
- [数据集：Dynamic Furry Animal Dataset](33.md)
- [Artemis: Articulated Neural Pets with Appearance and Motion Synthesis](32.md)
- [SMPLer: Taming Transformers for Monocular 3D Human Shape and Pose Estimation](31.md)
- [CAT3D: Create Anything in 3D with Multi-View Diffusion Models](30.md)
- [PACER+: On-Demand Pedestrian Animation Controller in Driving Scenarios](29.md)
- [Humans in 4D: Reconstructing and Tracking Humans with Transformers](28.md)
- [Learning Human Motion from Monocular Videos via Cross-Modal Manifold Alignment](27.md)
- [PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos](26.md)
- [Imagic: Text-Based Real Image Editing with Diffusion Models](25.md)
- [DiffEdit: Diffusion-based semantic image editing with mask guidance](24.md)
- [Dual diffusion implicit bridges for image-to-image translation](23.md)
- [SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations](22.md)
-[InstructPix2Pix: Learning to Follow Image Editing Instructions](21.md)
- [Prompt-to-Prompt Image Editing with Cross-Attention Control](20.md)
- [WANDR: Intention-guided Human Motion Generation](19.md)
- [TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Videos](18.md)
- [3D Gaussian Splatting for Real-Time Radiance Field Rendering](17.md)
- [Decoupling Human and Camera Motion from Videos in the Wild](16.md)
- [HMP: Hand Motion Priors for Pose and Shape Estimation from Video](15.md)
- [HuMoR: 3D Human Motion Model for Robust Pose Estimation](14.md)
- [Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video](13.md)
- [Global-to-Local Modeling for Video-based 3D Human Pose and Shape Estimation](12.md)
- [WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion](11.md)
- [Tackling the Generative Learning Trilemma with Denoising Diffusion GANs](10.md)
- [Elucidating the Design Space of Diffusion-Based Generative Models](9.md)
- [SCORE-BASED GENERATIVE MODELING THROUGHSTOCHASTIC DIFFERENTIAL EQUATIONS](8.md)
- [Consistency Models](7.md)
- [Classifier-Free Diffusion Guidance](6.md)
- [Cascaded Diffusion Models for High Fidelity Image Generation](5.md)
- [LEARNING ENERGY-BASED MODELS BY DIFFUSIONRECOVERY LIKELIHOOD](4.md)
- [On Distillation of Guided Diffusion Models](3.md)
- [Denoising Diffusion Implicit Models](2.md)
- [PROGRESSIVE DISTILLATION FOR FAST SAMPLING OF DIFFUSION MODELS](./1.md)


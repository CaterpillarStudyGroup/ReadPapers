# TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Videos

|缩写|英文|中文|
|---|---|---|
|VIMO|video transformer model|
|DBA|dense bundle adjustment layer|

## 核心问题是什么?

TRAM是一种从in-the-wild视频中重建人类全局轨迹和运动的两阶段方法。  

### 提取相机轨迹

已有方法（16）是通过相机运动 + 人物运动 + 人物运动先验 联合优化出相机scale。作者认为由于人物运动先验是使用室内数据训练得到的，因此不能泛化到室外场景中。  

作者认为仅通过（排除前景干扰的）背景 + 背景语义足以估计出scale。  

### 人物轨迹恢复

已有方法中，单帧方法恢复较准确，但缺少连续性。而时序方法恢复动作不够准确。原因是基于视频的训练成本高且缺少视频数据。  

作者在HMR2.0(预训练的人物模型)上拓展出VIMO模型（增加2个时序transformer），并使用视频数据finetune这个模型。在轨迹信息的相机作为尺度参考系下，回归出人体的运动学身体运动。 

### 效果

通过组合这两个运动，我们实现了世界坐标系中 3D 人体的精确恢复，将全局运动误差比之前的工作减少了 60%。  
https://yufu-wang.github.io/tram4d/

## 核心贡献是什么？

- (i) 我们提出了一种通用方法 TRAM，它可以从野外视频中恢复人体轨迹和运动，比之前的工作有了很大的改进。 
- (ii) 我们证明可以从 SLAM 推断出人体轨迹，并提供技术解决方案，使单目 SLAM 在动态人体存在的情况下具有鲁棒性和公制尺度。 
- (iii) 我们提出了视频变换器模型 VIMO，它建立在大型预训练的基于图像的模型之上，并证明这种可扩展的设计实现了最先进的重建性能。

## 大致方法是什么？

![](./assets/161c67733ee544605c3989dd85fc8aed_4_Figure_2_-1805418559.png)

左上：给定一个视频，我们首先使用 DROID-SLAM 恢复相对相机运动和场景深度，并通过双掩蔽对其进行增强（第 3.2 节）。  
右上：我们通过优化过程将恢复的深度与左上预测的深度对齐，以估计度量缩放（第 3.3 节）。  
底部：我们引入 VIMO 在相机坐标下重建 3D 人体（第 3.4 节），并使用公制尺度相机将人体轨迹和身体运动转换到全局坐标。

|输入|输出|方法|
|---|---|---|
|图像|mask|来自 YOLOv7 [83] 的检测作为Segment Anything Model[32] 模型的提示|
|图像|公制深度|ZoeDepth [5]|

### Masked DROID-SLAM

输入：单视角视频  
输出：相机轨迹

在DROID-SLA的基础上，把图像中被mask为有人的区域的置信度转为0，防止人物的移动对DROID-SLA造成影响。  

### Trajectory Scale Estimation
作者认为，空间scale的推理能力部分体现在深度预测网络中。

第一步的DPOID会输出深度，但深度的单位未知，且与图像保持一致。  
第二步中的ZoeDepth也会输出深度，且深度是meter scale的。  

通过两个深度的对比，可以得出图像的scale。  
为了让这个scale更鲁棒，作者做了以下处理：  
1. 所有帧独立求解再取平均值  
2. 去掉无处预测不准确的区域

> 所有帧共用一个scale，这点有点奇怪。相机移动了，scale可能会变。同一帧不同远近，scale也不一样。  

### Video Transformer for Human Motion

![](./assets/161c67733ee544605c3989dd85fc8aed_7_Figure_3_381676602.png)

想法：利用大型预训练模型中的丰富知识，进行灵活且可扩展的架构设计，其性能可以通过计算和数据不断提高。

做法：  
1. 通过添加两个时间transformer，将基于图像的 HMR2.0 转换为视频模型。
2. 使用视频数据在原始transformer decoder上进行微调

#### Transformer

这两个时间transformer使用相同的encoder only架构.  
第一个transfomer在图像域（patch token from ViT）传播时间信息。即，先在每个patch上独立地使用ViT transformer encoder进行编码。再把这个编码合起来，在时间维度上进行增加的transformer。两个transformer合起来，相当于一个ST transformer。      
第二个transformer在人体运动域传播时间信息。作者认为，**先隐空间学习运动模型再回归器出SMPL参数的做法不很合适，因为该隐空间纠缠了许多其他信息，例如形状、相机和图像特征**。相比之下，作者建议回归出SMPL参数之后仅对其中的pose部分使用 Transformer 进行编码和解码。因此，本文直接将这种通用架构应用于 SMPLposes{θt,rt}上，并使用 MoCap 数据进行预训练。

## 训练与验证

### 数据集

3DPW [81], Human3.6M [24], and BEDLAM [6]

### Loss

- 2D 投影loss
- 3D 重建loss
- SMPL参数loss
[?] 只是动作参数？
- mesh重建loss

## 有效

测的几段视频，发现在重建动作的灵活性和准确性上都优于WHAM。  

## 缺陷

1. 由于缺少世界坐标系下的数据，只能预测相机坐标系下的动作信息。  
2. 在运动相机下有效，在静止相机下代码会出错。错误来自第三方的SLAM估计模块。  

## 启发

## 遗留问题

## 参考材料
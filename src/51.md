# Motion-Conditioned Diffusion Model for Controllable Video Synthesis

## 核心问题是什么?

### 目的

以一种更可控的方式描述要生成的内容和动作，进行Motion-guided video generation

![](./assets/08-136.png) 

### 现有方法及局限性

缺乏控制和描述所需内容和运动的有效方法

### 本文方法

用户通过一些短划线对图像帧进行描述，可以合成的预期的内容和动态。
1. 利用flow completion model，基于视频帧的语义理解和稀疏运动控制来预测密集视频运动。
2. 以输入图像为首帧，future-frame prediction model合成高质量的未来帧以形成输出视频。

### 效果

我们定性和定量地表明，MCDiff 在笔画引导的可控视频合成中实现了最先进的视觉质量。

## 核心贡献是什么？

## 大致方法是什么？

Two-stage autoregressive generation

![](./assets/08-137.png) 

> MCDiff 是一种**自回归**视频合成模型。对于每个时间步，  
输入：前一帧（即开始帧或预测的上一帧）和笔画的瞬时段（标记为彩色箭头，较亮的颜色表示较大的运动）引导。
输出：（1）flow completion model预测代表每像素瞬时运动的密集flow。(2)future-frame prediction model通过条件扩散过程根据前一帧和预测的密集流合成下一帧。  
所有预测帧的集合形成一个视频序列，该序列遵循起始帧提供的上下文和笔划指定的运动。 

### 视频动态标注

### flow completion model

### future-frame prediction model

## 训练

### 数据集

### loss

### 训练策略

## 实验与结论

## 有效

## 局限性

## 启发

## 遗留问题

## 参考材料

项目启动页面：https://tsaishien-chen.github.io/MCDiff/ing

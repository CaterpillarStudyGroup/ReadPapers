# æ ¸å¿ƒé—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ

DDPMçš„ç”Ÿæˆé€Ÿåº¦å¤ªæ…¢ã€‚

# ç›¸å…³å·¥ä½œ(Chapter 6)

**perform distillation of DDIM teacher models into one-step student models**

*   Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021.

**few-step sampling, as was the probability flow sampler**

*   DDIM (Song et al., 2021a) was originally shown to be effective for few-step sampling, as was the probability flow sampler (Song et al., 2021c).&#x20;
*   Jolicoeur-Martineau et al. (2021) study fast SDE integrators for reverse diffusion processes
*   Tzen & Raginsky (2019b) study unbiased samplers which may be useful for fast, high quality sampling as well.

**Other work on fast sampling can be viewed as manual or automated methods to adjust samplers or diffusion processes for fast generation.**&#x20;

*   Nichol & Dhariwal (2021); Kong & Ping (2021) describe methods to adjust a discrete time diffusion model trained on many timesteps into models that can sample in few timesteps.&#x20;
*   Watson et al. (2021) describe a dynamic programming algorithm to reduce the number of timesteps for a diffusion model in a way that is optimal for log likelihood.&#x20;
*   Chen et al. (2021); Saharia et al. (2021); Ho et al. (2021) train diffusion models over continuous noise levels and tune samplers post training by adjusting the noise levels of a few-step discrete time reverse diffusion process.&#x20;
*   Their method is effective in highly conditioned settings such as text-to-speech and image super-resolution. San-Roman et al. (2021) train a new network to estimate the noise level of noisy data and show how to use this estimate to speed up sampling.

**Alternative specifications of the diffusion model can also lend themselves to fast sampling,**&#x20;

*   modified forward and reverse processes (Nachmani et al., 2021; Lam et al., 2021)&#x20;
*   &#x20;training diffusion models in latent space (Vahdat et al., 2021).

# æ ¸å¿ƒè´¡çŒ®æ˜¯ä»€ä¹ˆï¼Ÿ

åŠ é€ŸDDPMçš„ç”Ÿæˆè¿‡ç¨‹ã€‚

1.  we present new parameterizations of diffusion models that provide increased stability when using few sampling steps.&#x20;
2.  we present a method to distill a trained deterministic diffusion sampler.&#x20;

# å¤§è‡´æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ

![d2a026851eebdc98f0b12efdc0b53a2c\_1\_Figure\_1.png](https://s.readpaper.com/T/2bF23cWDain)

> Figure 1: A visualization of two iterations of our proposed progressive distillation algorithm. A sampler f (z; Î·), mapping random noise  to samples x in 4 deterministic steps, is distilled into a new sampler f (z; Î¸) taking only a single step. The original sampler is derived by approximately integrating the probability flow ODE for a learned diffusion model, and distillation can thus be understood as learning to integrate in fewer steps, or amortizing this integration into the new sampler.

âœ… å‡è®¾æœ‰ä¸€ä¸ª solverï¼Œå¯ä»¥æ ¹æ®$x_t$ é¢„æµ‹$x_{t-1}$ï¼\
âœ… è°ƒç”¨ä¸¤æ¬¡ solverï¼Œå¯ä»¥ä» $x_t$ å¾—åˆ°$x_{t-2}$ï¼Œå­¦ä¹ è¿™ä¸ªè¿‡ç¨‹ï¼Œå¯ä»¥ç›´æ¥å¾—åˆ° 2 step çš„ solver.\
âœ… å‰ä¸€ä¸ª solver ç§°ä¸º teacherï¼Œåä¸€ä¸ªç§°ä¸º student.\
âœ… student æˆä¸ºæ–°çš„ teacherï¼Œè®­ç»ƒæ–°çš„ student.

# æœ‰æ•ˆæ€§

On standard image generation benchmarks like CIFAR-10, ImageNet, and LSUN, we start out with state-of-the-art samplers taking as many as 8192 steps, and are able to distill down to models taking as few as 4 steps without losing much perceptual quality; achieving, for example, a FID of 3.0 on CIFAR-10 in 4 steps.

# ç¼ºé™·

| å±€é™ | æ”¹è¿›ç‚¹ |
| :-- | :------ |
| In the current work we limited ourselves to setups where the student model has the same architecture and number of parameters as the teacher model: | in future work we hope to relax this constraint and explore settings where the student model is smaller, potentially enabling further gains in test time computational requirements.  |
| | In addition, we hope to move past the generation of images and also explore progressive distillation of diffusion models for different data modalities such as e.g. audio (Chen et al., 2021).|
| | In addition to the proposed distillation procedure, some of our progress was realized through different parameterizations of the diffusion model and its training loss. We expect to see more progress in this direction as the community further explores this model class. |

# éªŒè¯

# å¯å‘

1.  The resulting target value $\tilde x(z_t)$ is fully determined given the teacher model and starting point $z_t$, which allows the student model to make a sharp prediction when evaluated at$z_t$. In contrast, the original data point x is not fully determined given $z_t$, since multiple different data points x can produce the same noisy data $z_t$: this means that the original denoising model is predicting a weighted average of possible x values, which produces a blurry prediction.&#x20;
2.  å¯¹å™ªå£°æ±‚L2 losså¯ä»¥çœ‹ä½œæ˜¯åŠ æƒå¹³å‡çš„é‡å»ºL2 lossï¼Œæ¨å¯¼è¿‡ç¨‹è§å…¬å¼9ã€‚ä½†åœ¨distillationè¿‡ç¨‹ä¸­ï¼Œä¸é€‚åˆé¢„æµ‹å™ªå£°ï¼Œè€Œåº”è¯¥é‡å»ºã€‚
3.  In practice, the choice of loss weighting also has to take into account how Î±t, Ïƒt are sampled during training, as this sampling distribution strongly determines the weight the expected loss gives to each signal-to-noise ratio.

# é—ç•™é—®é¢˜

1.  å¾ˆå¤šç»†èŠ‚çœ‹ä¸æ‡‚ã€‚æ¯”å¦‚é¢„æµ‹xä¸é¢„æµ‹å™ªå£°çš„å…³ç³»ã€‚æ€ä¹ˆå®šä¹‰weight?parameterizations of the denoising diffusion model?DDIM?

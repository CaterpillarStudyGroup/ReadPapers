# LORA: LOW-RANK ADAPTATION OF LARGE LAN-GUAGE MODEL

## 核心问题是什么?

基于预训练的大模型进行finetune时，重新训练所有模型参数变得不太可行。

### 现有方法及存在的问题

#### 适配器层引入推理延迟

Parameter-Efficient Transfer Learning for NLP  （two adapter layers per Transformer block）  
Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning（only one per block but with an additional LayerNorm）  

虽然可以通过修剪层或利用多任务设置来减少总体延迟，但没有直接的方法可以绕过适配器层中的额外计算。  
这似乎不是问题，因为适配器层被设计为具有很少的参数（有时<原始模型的 1%），并且具有小的瓶颈尺寸，这限制了它们可以添加的 FLOP。  
然而，大型神经网络依赖硬件并行性来保持低延迟，并且适配器层必须按顺序处理。这对在线推理设置产生了影响，其中批量大小通常小至 1。在没有模型并行性的一般场景中，例如在单个 GPU 上的 GPT-2推理，我们发现使用适配器时延迟会显着增加，即使瓶颈维度非常小s.

#### 直接优化提示很难

[?]这一段没看懂

### 本文方法

我们提出了Low Rank Adapter（LoRA），它冻结了预训练的模型权重，并将可训练的rank分解矩阵注入到 Transformer 架构的每一层中，大大减少了下游任务的可训练参数的数量。与使用 Adam 微调的 GPT-3 175B 相比，LoRA 可以将可训练参数数量减少 10,000 倍，GPU 内存需求减少 3 倍。 


### 效果

LoRA 在 RoBERTa、DeBERTa、GPT-2 和 GPT-3 上的模型质量上表现与微调相当或更好，尽管可训练参数较少、训练吞吐量较高，并且与适配器不同，没有额外的推理延迟。

## 核心贡献是什么？

1.  **Low Rank Adapter（LoRA）**：这是一种新技术，通过在Transformer架构的每一层中注入可训练的低秩分解矩阵来调整预训练模型的权重，而不是重新训练所有模型参数。

6.  **易于实现和集成**：论文提供了一个便于与PyTorch模型集成的LoRA包，以及RoBERTa、DeBERTa和GPT-2的实现和模型检查点。

7.  **经验性研究**：论文还提供了对语言模型适应中的秩不足进行实证研究，这有助于理解LoRA的有效性。

8.  **模型共享与任务切换**：LoRA允许共享预训练模型，并根据不同任务通过更换低秩矩阵来高效切换。

9.  **训练和推理的一致性**：LoRA在训练和推理过程中保持了一致性，这有助于简化模型部署和应用。

## 大致方法是什么？

### LoRA

神经网络包含许多执行矩阵乘法的密集层。这些层中的权重矩阵通常具有满秩。  
我们假设**在adaption过程中权重的更新具有较低的“内在维度”。**  
定义预训练的权重矩阵为 \\(W0 ∈ R^{d×k}\\)，其更新过程为：W0 + ΔW  

我们通过用低秩分解，将权重更新过程描述为： W0 + ΔW = W0 + BA，其中 \\(B ∈ R^{d×r} , A ∈ R^{r×k}\\) ，并且秩 \\(r \ll min(d, k)\\)。  

![](./assets/95e7ad3017ad4749fbc05bc5b2e3ce8a_0_Figure_1.png)

请注意，W0 和 ΔW = BA 都与相同的输入相乘，并且它们各自的输出向量按坐标求和。

### LoRA应用到Transformer

理论上，LoRA可以应用于网络中的任意密集矩阵上，但本文仅将LoRA应用于attention层。  
对于特定adaption，使用特定的BA。换一种adaption则换一组BA。不需要adaption则将BA去掉。  

## 训练与验证

### 训练策略

#### 参数冻结

训练期间，W0 被冻结，不接收梯度更新，而 A 和 B 包含可训练参数。

#### 参数初始化

对 A 使用随机高斯初始化，对 B 使用零，因此 ΔW = BA 在训练开始时为零。  

[?] scale ∆Wx by α/r 这一部分没看懂

### 数据集

### loss

### 训练策略

## 有效

2.  **参数效率**：LoRA大幅减少了可训练参数的数量，例如，与使用Adam对GPT-3 175B进行全参数微调相比，LoRA可以将可训练参数数量减少10,000倍。

3.  **内存和计算效率**：LoRA减少了GPU内存需求，并且由于只需要优化注入的较小的低秩矩阵，它在训练时更加高效。

4.  **无损推理延迟**：与其他方法（如适配器层）不同，LoRA在部署时不会引入额外的推理延迟，因为它在推理时可以合并训练的矩阵与冻结的权重。

5.  **模型质量**：尽管可训练参数数量减少，LoRA在多个模型（如RoBERTa、DeBERTa、GPT-2和GPT-3）上的表现与全参数微调相当或更好。

1.  **参数效率**：LoRA通过在Transformer架构的每一层中引入低秩矩阵来调整预训练模型，大幅减少了可训练参数的数量，从而降低了模型对计算资源的需求。

2.  **内存和存储效率**：由于减少了可训练参数，LoRA在训练和部署时需要的内存和存储空间显著减少，使得在资源受限的环境中部署大型模型成为可能。

3.  **无损推理速度**：LoRA在推理时不会引入额外的延迟，因为它允许在部署时合并训练的低秩矩阵与冻结的权重，保持了与原始模型相同的推理速度。

4.  **模型共享与快速任务切换**：LoRA允许共享一个预训练模型，并根据不同任务快速切换低秩矩阵，这减少了存储和部署多个独立模型实例的需要。

5.  **保持模型质量**：尽管参数数量大幅减少，LoRA在多个任务上的表现与全参数微调相当或更好，保持了模型的高性能。

6.  **易于实现和集成**：论文提供了LoRA的实现和模型检查点，便于研究者和开发者将其集成到现有的PyTorch模型中。

7.  **泛化能力**：LoRA显示出良好的泛化能力，即使是在低数据环境下也能保持较好的性能。

## 局限性

1.  **特定权重的选择**：LoRA需要选择哪些权重矩阵应用低秩适应，这可能需要基于经验或额外的启发式方法来决定。

2.  **对低秩结构的依赖**：LoRA假设模型权重的更新具有低秩结构，这可能不适用于所有类型的任务或模型架构。

3.  **可能的性能瓶颈**：尽管LoRA在多个任务上表现良好，但对于某些特定任务，可能需要更多的参数来捕捉任务的复杂性，这可能限制了LoRA的性能提升空间。

4.  **适配器层的局限性**：LoRA在某些情况下可能无法完全替代传统的适配器层，特别是在需要模型并行处理或处理不同任务输入的场景中。

5.  **对预训练模型的依赖**：LoRA依赖于高质量的预训练模型，如果预训练模型在某些领域或任务上的表现不佳，LoRA的适应效果也可能受限。

6.  **超参数调整**：LoRA的性能可能受到超参数（如低秩矩阵的秩）的影响，需要仔细调整这些参数以获得最佳性能。

7.  **特定任务的适用性**：LoRA可能在某些任务上特别有效，而在其他任务上则可能需要更多的定制化或不同的适应策略。


## 启发

## 遗留问题

## 参考材料

1. 代码仓库： https://github.com/microsoft/LoRA
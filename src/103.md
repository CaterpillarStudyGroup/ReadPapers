# GENMO:AGENeralist Model for Human MOtion

NVIDIA

## 研究背景与问题

### 要解决的问题

运动生成（如从文本生成动作）和运动估计（如从视频重建3D动作）

### 现有方法及局限性

视频运动估计通常关注确定性预测以追求精度，文本/音乐到运动生成则需输出多样性以覆盖可能动作，因此将运动生成和运动估计视为独立任务。  
这导致模型冗余且知识无法共享。但实际上这两类任务本质均依赖对时空运动规律的理解，而技术路线分离阻碍了性能提升。

### 本文方法

将运动估计与生成整合至单一框架中。
1. 将运动估计重新定义为受约束的运动生成，要求输出运动必须严格满足观测到的条件信号。
2. 通过结合回归与扩散模型的协同效应，既能实现精准的全局运动估计，又能支持多样化运动生成。
3. 一种估计引导的训练目标，利用带有2D标注和文本描述的野外视频数据来增强生成多样性。
4. 支持处理可变长度运动及混合多模态条件（文本、音频、视频），在不同时间区间提供灵活控制。

这种统一方法产生了协同效益：生成先验能提升遮挡等挑战性条件下的运动估计质量，而多样视频数据则增强了生成能力。

### 主要贡献

1. 首个通用模型：GENMO 首次统一了SOTA 全局运动估计与灵活的、多模态条件（视频、音乐、文本、2D 关键点、3D 关键帧）运动生成。

2. 灵活架构设计：支持可变长度运动生成，并允许任意多模态输入组合，无需复杂后处理。

3. 双模式训练范式：探索回归与扩散的协同作用，并提出估计引导的训练目标，使模型能有效利用野外视频数据。

4. 双向提升：生成先验 提升遮挡等挑战条件下的估计鲁棒性；多样化视频数据 增强生成表现力。

## 通用人体运动模型

### 多模态输入

- **条件信号集 \(\mathcal{C}\)**：包含一种或多种模态输入：  
  - 视频特征 \(\mathbf{c}_{\text{video}} \in \mathbb{R}^{N \times d_{\text{video}}}\)  
  - 相机运动 \(\mathbf{c}_{\text{cam}} \in \mathbb{R}^{N \times d_{\text{cam}}}\)  
  - 2D骨架 \(\mathbf{c}_{\text{2d}} \in \mathbb{R}^{N \times d_{\text{2d}}}\)  
  - 音乐片段 \(\mathbf{c}_{\text{music}} \in \mathbb{R}^{N \times d_{\text{music}}}\)  
  - 2D包围框 \(\mathbf{c}_{\text{bbox}} \in \mathbb{R}^{N \times d_{\text{bbox}}}\)  
  - 自然语言描述 \(\mathbf{c}_{\text{text}} \in \mathbb{R}^{M \times d_{\text{text}}}\)（\(M\)为文本词元数）  
- **条件掩码集 \(\mathcal{M}\)**：为每个条件类型 \(\mathbf{c}_{\star}\) 定义二元掩码矩阵 \(\mathbf{m}_{\star} \in \mathbb{R}^{N \times d_{\star}}\)，当条件特征可用时掩码值为1，否则为0。  

### 运动序列 x 的表示

- **全局轨迹**（第 \(i\) 帧）：  
  - 重力视图朝向 \(\Gamma_{i}^{\text{gv}} \in \mathbb{R}^6\)  
  - 局部根节点速度 \(v_{i}^{\text{root}} \in \mathbb{R}^3\)  
- **局部运动**（第 \(i\) 帧）：  
  - SMPL模型参数[45]：  
    - 关节角 \(\theta_i \in \mathbb{R}^{24 \times 6}\)  
    - 身形参数 \(\beta_i \in \mathbb{R}^{10}\)  
    - 根节点位移 \(t_{i}^{\text{root}} \in \mathbb{R}^3\)  
- **相机位姿**（第 \(i\) 帧）：  
  - 相机到世界的变换 \(\pi_i = (\Gamma_{i}^{\text{cv}}, t_{i}^{\text{cv}})\)  
    - 相机视图朝向 \(\Gamma_{i}^{\text{cv}} \in \mathbb{R}^6\)  
    - 相机位移 \(t_{i}^{\text{cv}} \in \mathbb{R}^3\)  
- **接触标签** \(p_i \in \mathbb{R}^6\)（手足接触状态）  

## 统一估计与生成设计

[TODO] 图2

输入：带噪声的运动序列 \(x_t\) 与条件信号 \(C\)、条件掩码 \(M\)   
输出：干净运动序列 \(x_0\)  

其中Multi-Text Attention为：  

[TODO] 图3

每一个文本序列与特定窗口的x部分做cross attention。  

### 任意长度运动推理

在基于 RoPE 的 Transformer 块中引入滑动窗口注意力机制：每个令牌仅关注其前后 W 帧邻域内的令牌。  
该设计使模型能生成远超训练时长的运动序列，同时保持计算效率并确保运动过渡平滑连贯。

### 混合多模态条件处理

文本条件（无帧级对齐）→ 通过多文本注意力机制处理

时序对齐模态（视频/音乐/2D骨架）→ 通过时序掩码策略管理

## 双模式训练范式

当前的diffusion model不足以支撑与输入视频严格一致的运动重建。  
作者发现运动估计与文本生成任务存在一下差异：  
1. 运动估计结果变异性极低（确定性行为）。文本生成结果变异性显著（概率性行为）。  
2. 视频条件模型：首步预测接近最终结果，后续步变化极小。文本条件模型：各步预测方差显著。  
得出结论：首步预测精度成为关键。  
因此提出提出双模式训练范式。  

### 估计模式

对**纯噪声z**预测时间步T下的输出x0，强制模型退化为确定性映射。  
除了diffusion model本身的重建Loss意外，再增加2D/3D关节点位置的正则项约束。  

### 



# FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing

随着扩散模型的出现，文本驱动动作生成已取得显著进展。然而，现有方法在生成符合细粒度描述的复杂动作序列时仍存在困难——这些描述需要描绘精确的时空动作细节。这种精细可控性的缺失限制了动作生成技术向更广泛受众的推广。为解决这些挑战，我们提出FineMoGen，这是一个基于扩散模型的运动生成与编辑框架，能够根据用户指令进行时空组合，生成细粒度动作。具体而言，FineMoGen在扩散模型基础上构建了名为时空混合注意力（SAMI）的新型Transformer架构。SAMI从两个角度优化全局注意力模板的生成：1）显式建模时空组合的约束条件；2）利用稀疏激活的专家混合机制自适应提取细粒度特征。为促进这一新型细粒度动作生成任务的大规模研究，我们构建了HuMMan-MoGen数据集，包含2,968个视频和102,336条细粒度时空描述。大量实验证明，FineMoGen在动作生成质量上显著优于现有最优方法。值得注意的是，借助现代大语言模型（LLM），FineMoGen进一步实现了零样本动作编辑能力，能够通过细粒度指令精准操控动作序列。项目页面：https://mingyuan-zhang.github.io/projects/FineMoGen.html
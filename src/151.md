# T2LM: Long-Term 3D Human Motion Generation from Multiple Sentences

本文致力于解决长期3D人体运动生成这一挑战性难题，重点研究如何从多语句（即段落）输入中生成平滑衔接的连续动作序列。现有的长期运动生成方法多基于循环神经网络架构，以前序生成的运动片段作为后续输入。然而该方法存在双重缺陷：1）依赖成本高昂的序列化数据集；2）逐步生成的运动片段间存在失真间隙。为应对这些问题，我们提出了一种简洁高效的T2LM框架——无需序列化数据即可训练的连续长期生成系统。该框架包含两个核心组件：训练用于将运动压缩为潜在向量序列的一维卷积VQVAE，以及基于Transformer的文本编码器（可根据输入文本预测潜在序列）。在推理阶段，语句序列被转换为连续的潜在向量流，经由VQVAE解码器重构为运动序列；通过采用具有局部时间感受野的一维卷积，有效避免了训练与生成序列间的时间不一致性。这种对VQ-VAE的简易约束使其仅需短序列训练即可实现更平滑的过渡效果。T2LM不仅显著优于现有长期生成模型并突破序列数据依赖的局限，在单动作生成任务上也达到了与当前最优模型相媲美的性能。
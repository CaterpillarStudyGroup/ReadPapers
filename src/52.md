# Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models

新国立，开源     

|||
|--|--|
| LoRA | Low Rank Adaptation|
| TI | Textual Inversion |

## 核心问题是什么?

![](./assets/52-图1.png) 

### 目的

基于T2I大模型的个性化内容生成。       
.    
.    
.    
.    

### 现有方法及局限性

使用 LoRA 可以实现 Concept Customization.     
但联合多个 LoRA 实现 Multi-concept Customization 是个挑战。       
.    
.    
.    

### 本文方法

Mix-of-show 框架，可解决：   
1. 多个 LoRA 引入的 concept 冲突。   
2. 模型 fusion 引入的 ID 特征丢失。   

方法：   
1. 单个 LoRA 训练时使用 embedding-decomposed LoRA.    
2. gadient 混合时保留单个 LoRA 的 in-domain essence.   
3. 引入区域可控 sampling，解决 multi-concept sampling 中的特征绑定和ID特征丢失的问题。   

可以组合不同的 concept (角色、对象、场景)且高质量生成。   
.    
.    
.    
.    

### 效果

.    
.    
.    
.    
.    

## 核心贡献是什么？

.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    

## 大致方法是什么？

![](./assets/52-图4.png)  

Mix-of-show 框架分为两部分：Single-Clinent和 Center-Node.   
Single-Client 基于 LoRA 学习特定对象，其关键技术为layer-wise Embedding 和 multi-world 表示。    
Center-Node 用于接入各种 Single-Client 以实现定制化的效果。    

### 任务描述    

**目的：** 结台2个及以上 concept 的定制化 Diffusion 生成模型。    

**当前方法：** 多个 concept 的联合训练。    
局限性，缺少扩展性和可复用性。    

**解决方法：** 分别训练每个 concept 模型，并将它们合并。    
在本文中,单个 concept 模型用 LoRA 实现。合并的方法为把多个concept 模型集成的权重到 center-node 上(类以LoRA)。    

$$
W= f(W_0,\Delta W_1,\Delta W_2,\dots ,\Delta W_n),
$$

### ED-LoRA   

原始 LoRA 是不能做 Concept 融合的。   

[&#x2753;] embedding tuning (TI ) 与 embedding-weight tuning (LoRA) 的对比实验，没有看懂。    

[&#x2753;] in-domain 和 out-domain 分别代表什么了？   

in-domain concept：目标是预训练模型生成出来的。    

**结论1**: TI 只能学习和生成 in-domain concept，对  out-domain concept 效果不好，TI + LoRA 可以学习和生成out-domain concept.    

**结论2**: 当前的 LoRA 能对 concept ID 编码，且不同的  LoRA会把不同的外观信息映射到相似的 embedding， 因此导致了多 Concept 融合时的 Conflicts.    

针对结论2，本文将相似的 embedding 替换不同 concept 解耦的
embedding.   

$$
V=V^{+} _{rand} V^{+} _{class}
$$

\\(V^{+} _{rand}\\)：随机初始化，用于提取 concept 的外观特征。    

\\(V^{+} _{class}\\)：根据 concept 的类别初始化。    

\\(V^{+} _{rand} 、V^{+} _{class}\\). LoRA 矩阵(B、A)都是可学习参数，其中 V 学习 in-domain 特征，LoRA 提取其它特征。    

### Center-Node   

如何把 LoRA 矩阵结合到预训练模型的权重上？    

**结论3**：直接混合 LoRA 权重，会导致生成结果的 concept ID 特征丢失。   
因为，\\(n\\)个 LoRA 矩阵的简单平均，会导致每个矩阵都变为原来的\\(\frac{1}{n} \\).    
利用 “decode concepts from text prompts” 的能力，在没有数据的情况下更新预训练模型的权重。   

1. 使用每个单独的 LoRA 分别 decode 出 concept.    
2. 提取每个 LoRA 对应的 input / output feature.   
3. 各个 LoRA 的 feature Concat 到一起。   
4. fused gradients 更新 W，目标函数。   

$$
L= {\textstyle \sum_{i=1}^n{}} ||(W_0+ \Delta W_i)X_i-WXi||^2_F
$$

[&#x2753;] 与联邦学习有什么关系？      
[&#x2753;] \\(X_i\\) 代表第i个 LoRA concept 的 input activatlion      
[&#x2753;] 第3步的 concat featdare 怎么用？    

### 区域可控的 Sambling

.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.    
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  

## 训练

.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  

### 数据集

.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  

### loss

.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  

### 训练策略

.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  

## 实验与结论

.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  

## 有效

.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  


## 局限性

.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  

## 启发

.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  

## 遗留问题

.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  
.  

## 参考材料

https://showlab.github.io/Mix-of-Show

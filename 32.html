<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Artemis: Articulated Neural Pets with Appearance and Motion Synthesis - ReadPapers</title>
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="The note of Papers">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/pagetoc.css">
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><div>ReadPapers</div></li><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="179.html"><strong aria-hidden="true">2.</strong> ParticleGS: Particle-Based Dynamics Modeling of 3D Gaussians for Prior-free Motion Extrapolation</a></li><li class="chapter-item expanded "><a href="178.html"><strong aria-hidden="true">3.</strong> Animate3d: Animating any 3d model with multi-view video diffusion</a></li><li class="chapter-item expanded "><a href="177.html"><strong aria-hidden="true">4.</strong> Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos</a></li><li class="chapter-item expanded "><a href="176.html"><strong aria-hidden="true">5.</strong> HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene</a></li><li class="chapter-item expanded "><a href="175.html"><strong aria-hidden="true">6.</strong> PIG: Physically-based Multi-Material Interaction with 3D Gaussians</a></li><li class="chapter-item expanded "><a href="174.html"><strong aria-hidden="true">7.</strong> EnliveningGS: Active Locomotion of 3DGS</a></li><li class="chapter-item expanded "><a href="173.html"><strong aria-hidden="true">8.</strong> SplineGS: Learning Smooth Trajectories in Gaussian Splatting for Dynamic Scene Reconstruction</a></li><li class="chapter-item expanded "><a href="172.html"><strong aria-hidden="true">9.</strong> PAMD: Plausibility-Aware Motion Diffusion Model for Long Dance Generation</a></li><li class="chapter-item expanded "><a href="171.html"><strong aria-hidden="true">10.</strong> PMG: Progressive Motion Generation via Sparse Anchor Postures Curriculum Learning</a></li><li class="chapter-item expanded "><a href="170.html"><strong aria-hidden="true">11.</strong> LengthAware Motion Synthesis via Latent Diffusion</a></li><li class="chapter-item expanded "><a href="169.html"><strong aria-hidden="true">12.</strong> IKMo: Image-Keyframed Motion Generation with Trajectory-Pose Conditioned Motion Diffusion Model</a></li><li class="chapter-item expanded "><a href="168.html"><strong aria-hidden="true">13.</strong> UniMoGen: Universal Motion Generation</a></li><li class="chapter-item expanded "><a href="167.html"><strong aria-hidden="true">14.</strong> AMD: Anatomical Motion Diffusion with Interpretable Motion Decomposition and Fusion</a></li><li class="chapter-item expanded "><a href="166.html"><strong aria-hidden="true">15.</strong> Flame: Free-form language-based motion synthesis &amp; editing</a></li><li class="chapter-item expanded "><a href="165.html"><strong aria-hidden="true">16.</strong> Human Motion Diffusion as a Generative Prior</a></li><li class="chapter-item expanded "><a href="164.html"><strong aria-hidden="true">17.</strong> Text-driven Human Motion Generation with Motion Masked Diffusion Model</a></li><li class="chapter-item expanded "><a href="163.html"><strong aria-hidden="true">18.</strong> ReMoDiffuse: RetrievalAugmented Motion Diffusion Model</a></li><li class="chapter-item expanded "><a href="162.html"><strong aria-hidden="true">19.</strong> MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model</a></li><li class="chapter-item expanded "><a href="161.html"><strong aria-hidden="true">20.</strong> ReAlign: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment</a></li><li class="chapter-item expanded "><a href="160.html"><strong aria-hidden="true">21.</strong> Absolute Coordinates Make Motion Generation Easy</a></li><li class="chapter-item expanded "><a href="159.html"><strong aria-hidden="true">22.</strong> Seamless Human Motion Composition with Blended Positional Encodings</a></li><li class="chapter-item expanded "><a href="158.html"><strong aria-hidden="true">23.</strong> FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing</a></li><li class="chapter-item expanded "><a href="157.html"><strong aria-hidden="true">24.</strong> Fg-T2M: Fine-Grained Text-Driven Human Motion Generation via Diffusion Model</a></li><li class="chapter-item expanded "><a href="156.html"><strong aria-hidden="true">25.</strong> Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation</a></li><li class="chapter-item expanded "><a href="155.html"><strong aria-hidden="true">26.</strong> StableMoFusion: Towards Robust and Efficient Diffusion-based Motion Generation Framework</a></li><li class="chapter-item expanded "><a href="154.html"><strong aria-hidden="true">27.</strong> EMDM: Efficient Motion Diffusion Model for Fast and High-Quality Motion Generation</a></li><li class="chapter-item expanded "><a href="153.html"><strong aria-hidden="true">28.</strong> Motion Mamba: Efficient and Long Sequence Motion Generation</a></li><li class="chapter-item expanded "><a href="152.html"><strong aria-hidden="true">29.</strong> M2D2M: Multi-Motion Generation from Text with Discrete Diffusion Models</a></li><li class="chapter-item expanded "><a href="151.html"><strong aria-hidden="true">30.</strong> T2LM: Long-Term 3D Human Motion Generation from Multiple Sentences</a></li><li class="chapter-item expanded "><a href="150.html"><strong aria-hidden="true">31.</strong> AttT2M:Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism</a></li><li class="chapter-item expanded "><a href="149.html"><strong aria-hidden="true">32.</strong> BAD: Bidirectional Auto-Regressive Diffusion for Text-to-Motion Generation</a></li><li class="chapter-item expanded "><a href="148.html"><strong aria-hidden="true">33.</strong> MMM: Generative Masked Motion Model</a></li><li class="chapter-item expanded "><a href="147.html"><strong aria-hidden="true">34.</strong> Priority-Centric Human Motion Generation in Discrete Latent Space</a></li><li class="chapter-item expanded "><a href="146.html"><strong aria-hidden="true">35.</strong> AvatarGPT: All-in-One Framework for Motion Understanding, Planning, Generation and Beyond</a></li><li class="chapter-item expanded "><a href="145.html"><strong aria-hidden="true">36.</strong> MotionGPT: Human Motion as a Foreign Language</a></li><li class="chapter-item expanded "><a href="144.html"><strong aria-hidden="true">37.</strong> Action-GPT: Leveraging Large-scale Language Models for Improved and Generalized Action Generation</a></li><li class="chapter-item expanded "><a href="143.html"><strong aria-hidden="true">38.</strong> PoseGPT: Quantization-based 3D Human Motion Generation and Forecasting</a></li><li class="chapter-item expanded "><a href="142.html"><strong aria-hidden="true">39.</strong> Incorporating Physics Principles for Precise Human Motion Prediction</a></li><li class="chapter-item expanded "><a href="141.html"><strong aria-hidden="true">40.</strong> PIMNet: Physics-infused Neural Network for Human Motion Prediction</a></li><li class="chapter-item expanded "><a href="140.html"><strong aria-hidden="true">41.</strong> PhysDiff: Physics-Guided Human Motion Diffusion Model</a></li><li class="chapter-item expanded "><a href="139.html"><strong aria-hidden="true">42.</strong> NRDF: Neural Riemannian Distance Fields for Learning Articulated Pose Priors</a></li><li class="chapter-item expanded "><a href="138.html"><strong aria-hidden="true">43.</strong> Pose-NDF: Modeling Human Pose Manifolds with Neural Distance Fields</a></li><li class="chapter-item expanded "><a href="137.html"><strong aria-hidden="true">44.</strong> Geometric Neural Distance Fields for Learning Human Motion Priors</a></li><li class="chapter-item expanded "><a href="136.html"><strong aria-hidden="true">45.</strong> Character Controllers Using Motion VAEs</a></li><li class="chapter-item expanded "><a href="135.html"><strong aria-hidden="true">46.</strong> Improving Human Motion Plausibility with Body Momentum</a></li><li class="chapter-item expanded "><a href="134.html"><strong aria-hidden="true">47.</strong> MoGlow: Probabilistic and controllable motion synthesis using normalising flows</a></li><li class="chapter-item expanded "><a href="133.html"><strong aria-hidden="true">48.</strong> Modi: Unconditional motion synthesis from diverse data</a></li><li class="chapter-item expanded "><a href="132.html"><strong aria-hidden="true">49.</strong> MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model</a></li><li class="chapter-item expanded "><a href="131.html"><strong aria-hidden="true">50.</strong> A deep learning framework for character motion synthesis and editing</a></li><li class="chapter-item expanded "><a href="130.html"><strong aria-hidden="true">51.</strong> Multi-Object Sketch Animation with Grouping and Motion Trajectory Priors</a></li><li class="chapter-item expanded "><a href="129.html"><strong aria-hidden="true">52.</strong> TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos</a></li><li class="chapter-item expanded "><a href="128.html"><strong aria-hidden="true">53.</strong> X-MoGen: Unified Motion Generation across Humans and Animals</a></li><li class="chapter-item expanded "><a href="127.html"><strong aria-hidden="true">54.</strong> Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis</a></li><li class="chapter-item expanded "><a href="126.html"><strong aria-hidden="true">55.</strong> MotionShot: Adaptive Motion Transfer across Arbitrary Objects for Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="114.html"><strong aria-hidden="true">56.</strong> Drop: Dynamics responses from human motion prior and projective dynamics</a></li><li class="chapter-item expanded "><a href="112.html"><strong aria-hidden="true">57.</strong> POMP: Physics-constrainable Motion Generative Model through Phase Manifolds</a></li><li class="chapter-item expanded "><a href="111.html"><strong aria-hidden="true">58.</strong> Dreamgaussian4d: Generative 4d gaussian splatting</a></li><li class="chapter-item expanded "><a href="110.html"><strong aria-hidden="true">59.</strong> Drive Any Mesh: 4D Latent Diffusion for Mesh Deformation from Video</a></li><li class="chapter-item expanded "><a href="109.html"><strong aria-hidden="true">60.</strong> AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation</a></li><li class="chapter-item expanded "><a href="108.html"><strong aria-hidden="true">61.</strong> ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction</a></li><li class="chapter-item expanded "><a href="107.html"><strong aria-hidden="true">62.</strong> Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators</a></li><li class="chapter-item expanded "><a href="106.html"><strong aria-hidden="true">63.</strong> Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals</a></li><li class="chapter-item expanded "><a href="105.html"><strong aria-hidden="true">64.</strong> Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation</a></li><li class="chapter-item expanded "><a href="104.html"><strong aria-hidden="true">65.</strong> Generating time-consistent dynamics with discriminator-guided image diffusion models</a></li><li class="chapter-item expanded "><a href="103.html"><strong aria-hidden="true">66.</strong> GENMO:AGENeralist Model for Human MOtion</a></li><li class="chapter-item expanded "><a href="102.html"><strong aria-hidden="true">67.</strong> HGM3: HIERARCHICAL GENERATIVE MASKED MOTION MODELING WITH HARD TOKEN MINING</a></li><li class="chapter-item expanded "><a href="101.html"><strong aria-hidden="true">68.</strong> Towards Robust and Controllable Text-to-Motion via Masked Autoregressive Diffusion</a></li><li class="chapter-item expanded "><a href="100.html"><strong aria-hidden="true">69.</strong> MoCLIP: Motion-Aware Fine-Tuning and Distillation of CLIP for Human Motion Generation</a></li><li class="chapter-item expanded "><a href="99.html"><strong aria-hidden="true">70.</strong> FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance</a></li><li class="chapter-item expanded "><a href="98.html"><strong aria-hidden="true">71.</strong> VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence</a></li><li class="chapter-item expanded "><a href="97.html"><strong aria-hidden="true">72.</strong> DragAnything: Motion Control for Anything using Entity Representation</a></li><li class="chapter-item expanded "><a href="96.html"><strong aria-hidden="true">73.</strong> PhysAnimator: Physics-Guided Generative Cartoon Animation</a></li><li class="chapter-item expanded "><a href="95.html"><strong aria-hidden="true">74.</strong> SOAP: Style-Omniscient Animatable Portraits</a></li><li class="chapter-item expanded "><a href="94.html"><strong aria-hidden="true">75.</strong> Neural Discrete Representation Learning</a></li><li class="chapter-item expanded "><a href="93.html"><strong aria-hidden="true">76.</strong> TSTMotion: Training-free Scene-aware Text-to-motion Generation</a></li><li class="chapter-item expanded "><a href="92.html"><strong aria-hidden="true">77.</strong> Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis</a></li><li class="chapter-item expanded "><a href="91.html"><strong aria-hidden="true">78.</strong> A lip sync expert is all you need for speech to lip generation in the wild</a></li><li class="chapter-item expanded "><a href="90.html"><strong aria-hidden="true">79.</strong> MUSETALK: REAL-TIME HIGH QUALITY LIP SYN-CHRONIZATION WITH LATENT SPACE INPAINTING</a></li><li class="chapter-item expanded "><a href="89.html"><strong aria-hidden="true">80.</strong> LatentSync: Audio Conditioned Latent Diffusion Models for Lip Sync</a></li><li class="chapter-item expanded "><a href="88.html"><strong aria-hidden="true">81.</strong> T2m-gpt: Generating human motion from textual descriptions with discrete representations</a></li><li class="chapter-item expanded "><a href="87.html"><strong aria-hidden="true">82.</strong> Motiongpt: Finetuned llms are general-purpose motion generators</a></li><li class="chapter-item expanded "><a href="86.html"><strong aria-hidden="true">83.</strong> Guided Motion Diffusion for Controllable Human Motion Synthesis</a></li><li class="chapter-item expanded "><a href="85.html"><strong aria-hidden="true">84.</strong> OmniControl: Control Any Joint at Any Time for Human Motion Generation</a></li><li class="chapter-item expanded "><a href="84.html"><strong aria-hidden="true">85.</strong> Learning Long-form Video Prior via Generative Pre-Training</a></li><li class="chapter-item expanded "><a href="83.html"><strong aria-hidden="true">86.</strong> Instant Neural Graphics Primitives with a Multiresolution Hash Encoding</a></li><li class="chapter-item expanded "><a href="82.html"><strong aria-hidden="true">87.</strong> Magic3D: High-Resolution Text-to-3D Content Creation</a></li><li class="chapter-item expanded "><a href="81.html"><strong aria-hidden="true">88.</strong> CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers</a></li><li class="chapter-item expanded "><a href="80.html"><strong aria-hidden="true">89.</strong> One-Minute Video Generation with Test-Time Training</a></li><li class="chapter-item expanded "><a href="79.html"><strong aria-hidden="true">90.</strong> Key-Locked Rank One Editing for Text-to-Image Personalization</a></li><li class="chapter-item expanded "><a href="78.html"><strong aria-hidden="true">91.</strong> MARCHING CUBES: A HIGH RESOLUTION 3D SURFACE CONSTRUCTION ALGORITHM</a></li><li class="chapter-item expanded "><a href="77.html"><strong aria-hidden="true">92.</strong> Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation</a></li><li class="chapter-item expanded "><a href="76.html"><strong aria-hidden="true">93.</strong> NULL-text Inversion for Editing Real Images Using Guided Diffusion Models</a></li><li class="chapter-item expanded "><a href="75.html"><strong aria-hidden="true">94.</strong> simple diffusion: End-to-end diffusion for high resolution images</a></li><li class="chapter-item expanded "><a href="74.html"><strong aria-hidden="true">95.</strong> One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale</a></li><li class="chapter-item expanded "><a href="73.html"><strong aria-hidden="true">96.</strong> Scalable Diffusion Models with Transformers</a></li><li class="chapter-item expanded "><a href="72.html"><strong aria-hidden="true">97.</strong> All are Worth Words: a ViT Backbone for Score-based Diffusion Models</a></li><li class="chapter-item expanded "><a href="71.html"><strong aria-hidden="true">98.</strong> An image is worth 16x16 words: Transformers for image recognition at scale</a></li><li class="chapter-item expanded "><a href="70.html"><strong aria-hidden="true">99.</strong> eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers</a></li><li class="chapter-item expanded "><a href="69.html"><strong aria-hidden="true">100.</strong> Photorealistic text-to-image diffusion models with deep language understanding||Imagen</a></li><li class="chapter-item expanded "><a href="68.html"><strong aria-hidden="true">101.</strong> DreamFusion: Text-to-3D using 2D Diffusion</a></li><li class="chapter-item expanded "><a href="67.html"><strong aria-hidden="true">102.</strong> GLIGEN: Open-Set Grounded Text-to-Image Generation</a></li><li class="chapter-item expanded "><a href="66.html"><strong aria-hidden="true">103.</strong> Adding Conditional Control to Text-to-Image Diffusion Models</a></li><li class="chapter-item expanded "><a href="65.html"><strong aria-hidden="true">104.</strong> T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models</a></li><li class="chapter-item expanded "><a href="64.html"><strong aria-hidden="true">105.</strong> Multi-Concept Customization of Text-to-Image Diffusion</a></li><li class="chapter-item expanded "><a href="63.html"><strong aria-hidden="true">106.</strong> An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion</a></li><li class="chapter-item expanded "><a href="62.html"><strong aria-hidden="true">107.</strong> DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</a></li><li class="chapter-item expanded "><a href="61.html"><strong aria-hidden="true">108.</strong> VisorGPT: Learning Visual Prior via Generative Pre-Training</a></li><li class="chapter-item expanded "><a href="60.html"><strong aria-hidden="true">109.</strong> NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation</a></li><li class="chapter-item expanded "><a href="59.html"><strong aria-hidden="true">110.</strong> AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning</a></li><li class="chapter-item expanded "><a href="58.html"><strong aria-hidden="true">111.</strong> ModelScope Text-to-Video Technical Report</a></li><li class="chapter-item expanded "><a href="57.html"><strong aria-hidden="true">112.</strong> Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="56.html"><strong aria-hidden="true">113.</strong> Make-A-Video: Text-to-Video Generation without Text-Video Data</a></li><li class="chapter-item expanded "><a href="55.html"><strong aria-hidden="true">114.</strong> Video Diffusion Models</a></li><li class="chapter-item expanded "><a href="54.html"><strong aria-hidden="true">115.</strong> Learning Transferable Visual Models From Natural Language Supervision</a></li><li class="chapter-item expanded "><a href="53.html"><strong aria-hidden="true">116.</strong> Implicit Warping for Animation with Image Sets</a></li><li class="chapter-item expanded "><a href="52.html"><strong aria-hidden="true">117.</strong> Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models</a></li><li class="chapter-item expanded "><a href="51.html"><strong aria-hidden="true">118.</strong> Motion-Conditioned Diffusion Model for Controllable Video Synthesis</a></li><li class="chapter-item expanded "><a href="50.html"><strong aria-hidden="true">119.</strong> Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets</a></li><li class="chapter-item expanded "><a href="49.html"><strong aria-hidden="true">120.</strong> UniAnimate: Taming Unified Video Diffusion Models for Consistent Human Image Animation</a></li><li class="chapter-item expanded "><a href="48.html"><strong aria-hidden="true">121.</strong> Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models</a></li><li class="chapter-item expanded "><a href="47.html"><strong aria-hidden="true">122.</strong> Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics</a></li><li class="chapter-item expanded "><a href="46.html"><strong aria-hidden="true">123.</strong> A Recipe for Scaling up Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="45.html"><strong aria-hidden="true">124.</strong> High-Resolution Image Synthesis with Latent Diffusion Models</a></li><li class="chapter-item expanded "><a href="44.html"><strong aria-hidden="true">125.</strong> Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling</a></li><li class="chapter-item expanded "><a href="43.html"><strong aria-hidden="true">126.</strong> æ•°æ®é›†ï¼šHumanVid</a></li><li class="chapter-item expanded "><a href="42.html"><strong aria-hidden="true">127.</strong> HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation</a></li><li class="chapter-item expanded "><a href="41.html"><strong aria-hidden="true">128.</strong> StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation</a></li><li class="chapter-item expanded "><a href="40.html"><strong aria-hidden="true">129.</strong> æ•°æ®é›†ï¼šZoo-300K</a></li><li class="chapter-item expanded "><a href="39.html"><strong aria-hidden="true">130.</strong> Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion</a></li><li class="chapter-item expanded "><a href="38.html"><strong aria-hidden="true">131.</strong> LORA: LOW-RANK ADAPTATION OF LARGE LAN-GUAGE MODELS</a></li><li class="chapter-item expanded "><a href="37.html"><strong aria-hidden="true">132.</strong> TCAN: Animating Human Images with Temporally Consistent Pose Guidance using Diffusion Models</a></li><li class="chapter-item expanded "><a href="36.html"><strong aria-hidden="true">133.</strong> GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians</a></li><li class="chapter-item expanded "><a href="35.html"><strong aria-hidden="true">134.</strong> MagicPony: Learning Articulated 3D Animals in the Wild</a></li><li class="chapter-item expanded "><a href="34.html"><strong aria-hidden="true">135.</strong> Splatter a Video: Video Gaussian Representation for Versatile Processing</a></li><li class="chapter-item expanded "><a href="33.html"><strong aria-hidden="true">136.</strong> æ•°æ®é›†ï¼šDynamic Furry Animal Dataset</a></li><li class="chapter-item expanded "><a href="32.html" class="active"><strong aria-hidden="true">137.</strong> Artemis: Articulated Neural Pets with Appearance and Motion Synthesis</a></li><li class="chapter-item expanded "><a href="31.html"><strong aria-hidden="true">138.</strong> SMPLer: Taming Transformers for Monocular 3D Human Shape and Pose Estimation</a></li><li class="chapter-item expanded "><a href="30.html"><strong aria-hidden="true">139.</strong> CAT3D: Create Anything in 3D with Multi-View Diffusion Models</a></li><li class="chapter-item expanded "><a href="29.html"><strong aria-hidden="true">140.</strong> PACER+: On-Demand Pedestrian Animation Controller in Driving Scenarios</a></li><li class="chapter-item expanded "><a href="28.html"><strong aria-hidden="true">141.</strong> Humans in 4D: Reconstructing and Tracking Humans with Transformers</a></li><li class="chapter-item expanded "><a href="27.html"><strong aria-hidden="true">142.</strong> Learning Human Motion from Monocular Videos via Cross-Modal Manifold Alignment</a></li><li class="chapter-item expanded "><a href="26.html"><strong aria-hidden="true">143.</strong> PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos</a></li><li class="chapter-item expanded "><a href="25.html"><strong aria-hidden="true">144.</strong> Imagic: Text-Based Real Image Editing with Diffusion Models</a></li><li class="chapter-item expanded "><a href="24.html"><strong aria-hidden="true">145.</strong> DiffEdit: Diffusion-based semantic image editing with mask guidance</a></li><li class="chapter-item expanded "><a href="23.html"><strong aria-hidden="true">146.</strong> Dual diffusion implicit bridges for image-to-image translation</a></li><li class="chapter-item expanded "><a href="22.html"><strong aria-hidden="true">147.</strong> SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations</a></li><li class="chapter-item expanded "><a href="20.html"><strong aria-hidden="true">148.</strong> Prompt-to-Prompt Image Editing with Cross-Attention Control</a></li><li class="chapter-item expanded "><a href="19.html"><strong aria-hidden="true">149.</strong> WANDR: Intention-guided Human Motion Generation</a></li><li class="chapter-item expanded "><a href="18.html"><strong aria-hidden="true">150.</strong> TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Videos</a></li><li class="chapter-item expanded "><a href="17.html"><strong aria-hidden="true">151.</strong> 3D Gaussian Splatting for Real-Time Radiance Field Rendering</a></li><li class="chapter-item expanded "><a href="16.html"><strong aria-hidden="true">152.</strong> Decoupling Human and Camera Motion from Videos in the Wild</a></li><li class="chapter-item expanded "><a href="15.html"><strong aria-hidden="true">153.</strong> HMP: Hand Motion Priors for Pose and Shape Estimation from Video</a></li><li class="chapter-item expanded "><a href="14.html"><strong aria-hidden="true">154.</strong> HuMoR: 3D Human Motion Model for Robust Pose Estimation</a></li><li class="chapter-item expanded "><a href="13.html"><strong aria-hidden="true">155.</strong> Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video</a></li><li class="chapter-item expanded "><a href="12.html"><strong aria-hidden="true">156.</strong> Global-to-Local Modeling for Video-based 3D Human Pose and Shape Estimation</a></li><li class="chapter-item expanded "><a href="11.html"><strong aria-hidden="true">157.</strong> WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion</a></li><li class="chapter-item expanded "><a href="10.html"><strong aria-hidden="true">158.</strong> Tackling the Generative Learning Trilemma with Denoising Diffusion GANs</a></li><li class="chapter-item expanded "><a href="9.html"><strong aria-hidden="true">159.</strong> Elucidating the Design Space of Diffusion-Based Generative Models</a></li><li class="chapter-item expanded "><a href="8.html"><strong aria-hidden="true">160.</strong> SCORE-BASED GENERATIVE MODELING THROUGHSTOCHASTIC DIFFERENTIAL EQUATIONS</a></li><li class="chapter-item expanded "><a href="7.html"><strong aria-hidden="true">161.</strong> Consistency Models</a></li><li class="chapter-item expanded "><a href="6.html"><strong aria-hidden="true">162.</strong> Classifier-Free Diffusion Guidance</a></li><li class="chapter-item expanded "><a href="5.html"><strong aria-hidden="true">163.</strong> Cascaded Diffusion Models for High Fidelity Image Generation</a></li><li class="chapter-item expanded "><a href="4.html"><strong aria-hidden="true">164.</strong> LEARNING ENERGY-BASED MODELS BY DIFFUSIONRECOVERY LIKELIHOOD</a></li><li class="chapter-item expanded "><a href="3.html"><strong aria-hidden="true">165.</strong> On Distillation of Guided Diffusion Models</a></li><li class="chapter-item expanded "><a href="2.html"><strong aria-hidden="true">166.</strong> Denoising Diffusion Implicit Models</a></li><li class="chapter-item expanded "><a href="1.html"><strong aria-hidden="true">167.</strong> PROGRESSIVE DISTILLATION FOR FAST SAMPLING OF DIFFUSION MODELS</a></li><li class="chapter-item expanded "><a href="125.html"><strong aria-hidden="true">168.</strong> Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions</a></li><li class="chapter-item expanded "><a href="124.html"><strong aria-hidden="true">169.</strong> ControlVideo: Training-free Controllable Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="123.html"><strong aria-hidden="true">170.</strong> Pix2Video: Video Editing using Image Diffusion</a></li><li class="chapter-item expanded "><a href="122.html"><strong aria-hidden="true">171.</strong> Structure and Content-Guided Video Synthesis with Diffusion Models</a></li><li class="chapter-item expanded "><a href="121.html"><strong aria-hidden="true">172.</strong> MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model</a></li><li class="chapter-item expanded "><a href="120.html"><strong aria-hidden="true">173.</strong> MotionDirector: Motion Customization of Text-to-Video Diffusion Models</a></li><li class="chapter-item expanded "><a href="119.html"><strong aria-hidden="true">174.</strong> Dreamix: Video Diffusion Models are General Video Editors</a></li><li class="chapter-item expanded "><a href="118.html"><strong aria-hidden="true">175.</strong> Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="117.html"><strong aria-hidden="true">176.</strong> TokenFlow: Consistent Diffusion Features for Consistent Video Editing</a></li><li class="chapter-item expanded "><a href="116.html"><strong aria-hidden="true">177.</strong> DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing</a></li><li class="chapter-item expanded "><a href="115.html"><strong aria-hidden="true">178.</strong> Content Deformation Fields for Temporally Consistent Video Processing</a></li><li class="chapter-item expanded "><a href="113.html"><strong aria-hidden="true">179.</strong> PFNN: Phase-Functioned Neural Networks</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">ReadPapers</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/CaterpillarStudyGroup/ReadPapers" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main><div class="sidetoc"><nav class="pagetoc"></nav></div>
                        <h1 id="artemis-articulated-neural-pets-with-appearance-and-motion-synthesis"><a class="header" href="#artemis-articulated-neural-pets-with-appearance-and-motion-synthesis">Artemis: Articulated Neural Pets with Appearance and Motion Synthesis</a></h1>
<h2 id="æ ¸å¿ƒé—®é¢˜æ˜¯ä»€ä¹ˆ"><a class="header" href="#æ ¸å¿ƒé—®é¢˜æ˜¯ä»€ä¹ˆ">æ ¸å¿ƒé—®é¢˜æ˜¯ä»€ä¹ˆ?</a></h2>
<h3 id="è¦è§£å†³çš„é—®é¢˜"><a class="header" href="#è¦è§£å†³çš„é—®é¢˜">è¦è§£å†³çš„é—®é¢˜</a></h3>
<p>è®¡ç®—æœºç”Ÿæˆ (CGI) æ¯›èŒ¸èŒ¸çš„åŠ¨ç‰©å—åˆ°ç¹ççš„ç¦»çº¿æ¸²æŸ“çš„é™åˆ¶ï¼Œæ›´ä¸ç”¨è¯´äº¤äº’å¼è¿åŠ¨æ§åˆ¶äº†ã€‚</p>
<h3 id="ç°æœ‰æ–¹æ³•"><a class="header" href="#ç°æœ‰æ–¹æ³•">ç°æœ‰æ–¹æ³•</a></h3>
<h3 id="æœ¬æ–‡æ–¹æ³•"><a class="header" href="#æœ¬æ–‡æ–¹æ³•">æœ¬æ–‡æ–¹æ³•</a></h3>
<p>æˆ‘ä»¬æå‡ºäº† ARTEMISï¼Œä¸€ç§æ–°é¢–çš„ç¥ç»å»ºæ¨¡å’Œæ¸²æŸ“ç®¡é“ã€‚<br />
ARTEMIS é€šè¿‡ AppEarance å’Œ Motion SynthesIS ç”Ÿæˆ ARTiculated ç¥ç»å® ç‰©ï¼Œå¹¶å® ç‰©è¿›è¡Œäº¤äº’å¼çš„åŠ¨ä½œæ§åˆ¶ï¼Œå®æ—¶åŠ¨ç”»å’Œæ¯›èŒ¸èŒ¸åŠ¨ç‰©çš„çœŸå®æ„Ÿæ¸²æŸ“ã€‚<br />
ARTEMIS çš„æ ¸å¿ƒæ˜¯åŸºäºç¥ç»ç½‘ç»œçš„ç”Ÿæˆå¼ï¼ˆNGIï¼‰åŠ¨ç‰©å¼•æ“ï¼š</p>
<ul>
<li>é‡‡ç”¨åŸºäºå…«å‰æ ‘çš„é«˜æ•ˆè¡¨ç¤ºæ¥è¿›è¡ŒåŠ¨ç‰©åŠ¨ç”»å’Œæ¯›çš®æ¸²æŸ“ã€‚è¿™æ ·ï¼ŒåŠ¨ç”»å°±ç›¸å½“äºåŸºäºæ˜¾å¼éª¨éª¼æ‰­æ›²çš„ä½“ç´ çº§å˜å½¢ã€‚</li>
<li>ä½¿ç”¨å¿«é€Ÿå…«å‰æ ‘ç´¢å¼•å’Œé«˜æ•ˆçš„ä½“ç§¯æ¸²æŸ“æ–¹æ¡ˆæ¥ç”Ÿæˆå¤–è§‚å’Œå¯†åº¦ç‰¹å¾å›¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç€è‰²ç½‘ç»œï¼Œå¯ä»¥æ ¹æ®å¤–è§‚å’Œå¯†åº¦ç‰¹å¾å›¾ç”Ÿæˆæ–°é¢–å§¿åŠ¿ä¸‹çš„å¤–è§‚å’Œä¸é€æ˜åº¦çš„é«˜ä¿çœŸç»†èŠ‚ã€‚</li>
<li>ä½¿ç”¨åŠ¨ç‰©è¿åŠ¨æ•æ‰æ–¹æ³•ï¼Œæ¥é‡å»ºç”±å¤šè§†å›¾ RGB å’Œ Vicon ç›¸æœºé˜µåˆ—æ•è·çš„çœŸå®åŠ¨ç‰©çš„éª¨éª¼è¿åŠ¨ã€‚</li>
<li>å°†æ‰€æœ‰æ•è·çš„è¿åŠ¨è¾“å…¥ç¥ç»è§’è‰²æ§åˆ¶æ–¹æ¡ˆä¸­ï¼Œä»¥ç”Ÿæˆå…·æœ‰è¿åŠ¨é£æ ¼çš„æŠ½è±¡æ§åˆ¶ä¿¡å·ã€‚</li>
<li>å°†ARTEMISé›†æˆåˆ°æ”¯æŒVRè€³æœºçš„ç°æœ‰å¼•æ“ä¸­ï¼Œæä¾›å‰æ‰€æœªæœ‰çš„æ²‰æµ¸å¼ä½“éªŒï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ç”ŸåŠ¨çš„åŠ¨ä½œå’Œé€¼çœŸçš„å¤–è§‚ä¸å„ç§è™šæ‹ŸåŠ¨ç‰©äº²å¯†äº’åŠ¨ã€‚</li>
</ul>
<h3 id="æ•ˆæœ"><a class="header" href="#æ•ˆæœ">æ•ˆæœ</a></h3>
<ol>
<li>å®æ—¶</li>
<li>NGI åŠ¨ç‰©çš„é«˜åº¦é€¼çœŸæ¸²æŸ“æ–¹é¢</li>
<li>æä¾›äº†ä¸æ•°å­—åŠ¨ç‰©å‰æ‰€æœªè§çš„æ—¥å¸¸æ²‰æµ¸å¼äº’åŠ¨ä½“éªŒã€‚</li>
</ol>
<h2 id="æ ¸å¿ƒè´¡çŒ®æ˜¯ä»€ä¹ˆ"><a class="header" href="#æ ¸å¿ƒè´¡çŒ®æ˜¯ä»€ä¹ˆ">æ ¸å¿ƒè´¡çŒ®æ˜¯ä»€ä¹ˆï¼Ÿ</a></h2>
<ol>
<li>
<p><strong>ç¥ç»ä½“ç§¯è¡¨ç¤ºæ³•ï¼ˆNeural Volume Representationï¼‰</strong>ï¼šArtemisä½¿ç”¨ç¥ç»ä½“ç§¯æ¥è¡¨ç¤ºåŠ¨ç‰©ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥å®æ—¶æ¸²æŸ“åŠ¨ç‰©çš„å¤–è§‚å’Œæ¯›å‘ã€‚</p>
</li>
<li>
<p><strong>åŠ¨æ€åœºæ™¯å»ºæ¨¡ï¼ˆDynamic Scene Modelingï¼‰</strong>ï¼šä¸ä¼ ç»Ÿçš„åŸºäºéª¨æ¶å’Œçš®è‚¤è’™çš®æŠ€æœ¯ä¸åŒï¼ŒArtemisé‡‡ç”¨ç¥ç»ç½‘ç»œç”Ÿæˆå›¾åƒï¼ˆNGIï¼‰æ¥åŠ¨æ€æ¨¡æ‹ŸåŠ¨ç‰©çš„å¤–è§‚å’Œè¿åŠ¨ã€‚</p>
</li>
<li>
<p><strong>è¿åŠ¨åˆæˆï¼ˆMotion Synthesisï¼‰</strong>ï¼šç³»ç»Ÿåˆ©ç”¨å±€éƒ¨è¿åŠ¨ç›¸ä½ï¼ˆLocal Motion Phase, LMPï¼‰æŠ€æœ¯ï¼Œæ ¹æ®ç”¨æˆ·çš„æ§åˆ¶ä¿¡å·ç”ŸæˆåŠ¨ç‰©çš„éª¨æ¶è¿åŠ¨ã€‚</p>
</li>
<li>
<p><strong>å¤šè§†è§’åŠ¨ä½œæ•æ‰ï¼ˆMulti-view Motion Captureï¼‰</strong>ï¼šè®ºæ–‡ä¸­æå‡ºäº†ä¸€ç§ç»“åˆå¤šè§†è§’RGBæ‘„åƒå¤´å’ŒViconæ‘„åƒå¤´çš„åŠ¨ä½œæ•æ‰æ–¹æ³•ï¼Œç”¨äºæ•æ‰çœŸå®åŠ¨ç‰©çš„è¿åŠ¨ã€‚</p>
</li>
<li>
<p><strong>ä¼˜åŒ–æ–¹æ¡ˆï¼ˆOptimization Schemeï¼‰</strong>ï¼šä¸ºäº†ä»æ•æ‰åˆ°çš„åŠ¨ä½œä¸­é‡å»ºåŠ¨ç‰©çš„éª¨æ¶è¿åŠ¨ï¼Œç ”ç©¶è€…æå‡ºäº†æœ‰æ•ˆçš„ä¼˜åŒ–æ–¹æ¡ˆã€‚</p>
</li>
<li>
<p><strong>è™šæ‹Ÿç°å®é›†æˆï¼ˆVR Integrationï¼‰</strong>ï¼šArtemisè¢«é›†æˆåˆ°æ”¯æŒVRå¤´æ˜¾çš„ç°æœ‰å¼•æ“ä¸­ï¼Œä¸ºç”¨æˆ·æä¾›äº†ä¸è™šæ‹ŸåŠ¨ç‰©äº²å¯†äº’åŠ¨çš„æ²‰æµ¸å¼ä½“éªŒã€‚</p>
</li>
<li>
<p><strong>äº¤äº’å¼æ§åˆ¶ï¼ˆInteractive Controlï¼‰</strong>ï¼šç”¨æˆ·å¯ä»¥é€šè¿‡ç®€å•çš„æ§åˆ¶ä¿¡å·ï¼Œå¦‚æŒ‡å‘ç›®çš„åœ°ï¼Œæ¥å¼•å¯¼è™šæ‹ŸåŠ¨ç‰©ç§»åŠ¨ï¼Œå®ç°è‡ªç„¶çš„è¿åŠ¨æ§åˆ¶ã€‚</p>
</li>
<li>
<p><strong>æ•°æ®é›†å’Œèµ„æºåˆ†äº«ï¼ˆDataset and Resource Sharingï¼‰</strong>ï¼šè®ºæ–‡ä¸­æåˆ°äº†åŠ¨æ€æ¯›å‘åŠ¨ç‰©æ•°æ®é›†ï¼ˆDynamic Furry Animal Datasetï¼‰ï¼Œå¹¶æ‰¿è¯ºå°†è¿™äº›èµ„æºåˆ†äº«ç»™ç ”ç©¶ç¤¾åŒºï¼Œä»¥ä¿ƒè¿›æœªæ¥å…³äºé€¼çœŸåŠ¨ç‰©å»ºæ¨¡çš„ç ”ç©¶ã€‚</p>
</li>
</ol>
<h2 id="å¤§è‡´æ–¹æ³•æ˜¯ä»€ä¹ˆ"><a class="header" href="#å¤§è‡´æ–¹æ³•æ˜¯ä»€ä¹ˆ">å¤§è‡´æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ</a></h2>
<p><img src="./assets/bea83903733e7c2e0013c6f112c41aa9_2_Figure_2.png" alt="" /></p>
<p>ARTEMIS ç”±ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ç»„æˆã€‚<br />
åœ¨ç¬¬ä¸€ä¸ªæ¨¡å—ä¸­ï¼Œç»™å®š CGI åŠ¨ç‰©èµ„äº§çš„éª¨éª¼å’Œè’™çš®æƒé‡ä»¥åŠä»£è¡¨æ€§å§¿åŠ¿ä¸­ç›¸åº”çš„<strong>å¤šè§†è§’æ¸²æŸ“ RGBA å›¾åƒ</strong>ï¼Œæ„å»ºåŸºäºåŠ¨æ€å…«å‰æ ‘çš„ç¥ç»è¡¨ç¤ºï¼Œä»¥å®ç°åŠ¨æ€åŠ¨ç‰©çš„æ˜¾å¼éª¨éª¼åŠ¨ç”»å’Œå®æ—¶æ¸²æŸ“ï¼Œæ”¯æŒå®æ—¶äº¤äº’åº”ç”¨ï¼›<br />
åœ¨ç¬¬äºŒä¸ªæ¨¡å—ä¸­ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå…·æœ‰<strong>å¤šè§†è§’ RGB å’Œ VICON ç›¸æœº</strong>çš„æ··åˆåŠ¨ç‰©è¿åŠ¨æ•æ‰ç³»ç»Ÿï¼Œä»¥é‡å»ºé€¼çœŸçš„ 3D éª¨éª¼å§¿åŠ¿ï¼Œè¯¥ç³»ç»Ÿæ”¯æŒè®­ç»ƒç¥ç»è¿åŠ¨åˆæˆç½‘ç»œï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿäº¤äº’å¼åœ°å¼•å¯¼ç¥ç»åŠ¨ç‰©çš„è¿åŠ¨ã€‚<br />
ARTEMISç³»ç»Ÿè¿›ä¸€æ­¥é›†æˆåˆ°ç°æœ‰çš„æ¶ˆè´¹çº§VRè€³æœºå¹³å°ä¸­ï¼Œä¸ºç¥ç»ç”Ÿæˆçš„åŠ¨ç‰©æä¾›èº«ä¸´å…¶å¢ƒçš„VRä½“éªŒã€‚</p>
<h3 id="preliminary"><a class="header" href="#preliminary">Preliminary</a></h3>
<h4 id="ç¥ç»ä¸é€æ˜è¾å°„åœº"><a class="header" href="#ç¥ç»ä¸é€æ˜è¾å°„åœº">ç¥ç»ä¸é€æ˜è¾å°„åœº</a></h4>
<p>NeRF ç”¨æ²¿ç€å…‰çº¿çš„æ¯ä¸ªç‚¹çš„é¢œè‰²å’Œå¯†åº¦æ¥è¡¨ç¤ºåœºæ™¯ï¼Œå…¶ä¸­å¯†åº¦è‡ªç„¶åœ°åæ˜ äº†è¯¥ç‚¹çš„ä¸é€æ˜åº¦ã€‚<br />
ä¼˜ç‚¹ï¼šå¯çœŸå®åœ°è¡¨ç°æ¯›å‘æ¸²æŸ“ã€‚<br />
ç¼ºç‚¹ï¼šNeRF ç”Ÿæˆçš„ alpha æœ‰å™ªéŸ³ä¸”è¿ç»­æ€§è¾ƒå·®ã€‚</p>
<h4 id="convnerf-luo-et-al-2021"><a class="header" href="#convnerf-luo-et-al-2021">ConvNeRF [Luo et al. 2021]</a></h4>
<p>åœ¨ç‰¹å¾ç©ºé—´ä¸­å¤„ç†å›¾åƒè€Œä¸æ˜¯ç›´æ¥åœ¨ RGB é¢œè‰²ä¸­å¤„ç†å›¾åƒã€‚<br />
ä¼˜ç‚¹ï¼šè§£å†³å™ªå£°å’Œä¸è¿ç»­æ€§é—®é¢˜ã€‚<br />
ç¼ºç‚¹ï¼šåªèƒ½å¤„ç†é™æ€ç‰©ä½“ã€‚</p>
<h3 id="animatable-neural-volumes"><a class="header" href="#animatable-neural-volumes">Animatable Neural Volumes</a></h3>
<p>ç›®çš„ï¼šå€ŸåŠ©PlenOctreeçš„æƒ³æ³•ï¼Œå°†ç¥ç»ä¸é€æ˜è¾å°„åœºæ‰©å±•åˆ°åŠ¨æ€åŠ¨ç‰©ï¼Œä¸”å®ç°å®æ—¶æ¸²æŸ“ã€‚</p>
<p>è¦è§£å†³çš„é—®é¢˜ï¼š</p>
<ol>
<li>å°†ä¸é€æ˜åº¦ç‰¹å¾å­˜å‚¨åœ¨ä½“ç§¯å…«å‰æ ‘ç»“æ„ä¸­ï¼Œä½¿å¾—å¯ä»¥å¿«é€Ÿè·å–æ¸²æŸ“ç‰¹å¾ã€‚</li>
<li>åŸºäºéª¨æ¶çš„ä½“ç§¯å˜å½¢ï¼ˆç±»ä¼¼äºåŸå§‹ CGI æ¨¡å‹çš„è’™çš®æƒé‡ï¼‰ï¼Œå°†è§„èŒƒå¸§ä¸åŠ¨ç”»çš„å®æ—¶å¸§è¿æ¥èµ·æ¥ã€‚</li>
<li>è®¾è®¡ä¸€ä¸ªç¥ç»ç€è‰²ç½‘ç»œæ¥å¤„ç†åŠ¨ç”»å¯¹è±¡å»ºæ¨¡ï¼Œå¹¶é‡‡ç”¨æœ‰æ•ˆçš„å¯¹æŠ—è®­ç»ƒæ–¹æ¡ˆæ¥ä¼˜åŒ–æ¨¡å‹</li>
</ol>
<h4 id="å…«å‰æ ‘ç‰¹å¾ç´¢å¼•"><a class="header" href="#å…«å‰æ ‘ç‰¹å¾ç´¢å¼•">å…«å‰æ ‘ç‰¹å¾ç´¢å¼•</a></h4>
<p>ç”±äºæ˜¯é’ˆå¯¹ç‰¹å®šçš„CGIåŠ¨ç‰©æ¨¡å‹ï¼Œå…¶Meshæ˜¯å·²çŸ¥çš„ã€‚</p>
<ol>
<li>æˆ‘ä»¬é¦–å…ˆå°† CGI åŠ¨ç‰©è§’è‰²ï¼ˆä¾‹å¦‚è€è™æˆ–ç‹®å­ï¼‰è½¬æ¢ä¸ºåŸºäºå…«å‰æ ‘çš„è¡¨ç¤ºã€‚</li>
</ol>
<p>é‡åˆ°çš„é—®é¢˜ï¼šåŸå§‹ CGI æ¨¡å‹åŒ…å«éå¸¸è¯¦ç»†çš„æ¯›å‘ï¼Œ</p>
<ul>
<li>å¦‚æœç›´æ¥è½¬æ¢ä¸ºç¦»æ•£ä½“ç´ ï¼Œå¯èƒ½ä¼šå¯¼è‡´åç»­ç¥ç»å»ºæ¨¡ä¸­å‡ºç°å¼ºçƒˆçš„æ··å å’Œä¸¥é‡é”™è¯¯ã€‚</li>
<li>å¦‚æœå»é™¤æ¯›çš®å¹¶ä»…ä½¿ç”¨è£¸æ¨¡å‹ï¼Œåˆ™ä½“ç´ è¡¨ç¤ºå°†ä¸å®é™…çš„æ˜¾ç€åå·®ã€‚</li>
</ul>
<p>è§£å†³æ–¹æ³•ï¼šç”¨â€œdilatedâ€ä½“ç´ è¡¨ç¤ºã€‚</p>
<ul>
<li>åˆå§‹åŒ–ä¸€ä¸ªç»Ÿä¸€çš„ä½“ç§¯</li>
<li>ä»¥ä¸€ç»„å¯†é›†è§†è§’çš„æ¸²æŸ“çš„ alpha matte ä½œä¸ºè¾“å…¥ï¼Œåˆ©ç”¨dilated maskæ„é€ å…«å‰æ ‘ã€‚<br />
ç”Ÿæˆçš„å…«å‰æ ‘åŒ…å«ä½“ç´ æ„æˆçš„arrayã€‚</li>
</ul>
<ol start="2">
<li>åŸºäºè¿™ç§ä½“ç§¯è¡¨ç¤ºï¼Œåœ¨æ¯ä¸ªä½“ç´ å¤„å­˜å‚¨ä¾èµ–äºè§†å›¾çš„ç‰¹å¾ğ‘“ã€‚</li>
</ol>
<p>åˆ†é…ä¸€ä¸ªç§°ä¸ºç‰¹å¾æŸ¥æ‰¾è¡¨ (FLUT) çš„æ•°ç»„ F æ¥å­˜å‚¨ç‰¹å¾å’Œå¯†åº¦å€¼ï¼Œå¦‚åŸå§‹ PlenOctree ä¸­ä¸€æ ·ã€‚<br />
å¯¹äºä½“ç»˜åˆ¶è¿‡ç¨‹ä¸­ç©ºé—´ä¸­ç»™å®šçš„æŸ¥è¯¢ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å¸¸æ•°æ—¶é—´å†…ç´¢å¼•åˆ° FLUT ä¸­ï¼ŒæŸ¥å‡ºè¯¥ç‚¹çš„ç‰¹å¾å’Œå¯†åº¦ã€‚</p>
<p>ç±»ä¼¼äº PlenOctreeï¼Œå°†ä¸é€æ˜åº¦ç‰¹å¾ğ‘“å»ºæ¨¡ä¸ºä¸€ç»„SHï¼ˆçƒé¢è°æ³¢ï¼‰ç³»æ•°ã€‚</p>
<p>$$
S(f,d) = \sum_{h=1}^H k_h^i Y_h(d)
$$</p>
<p>kä¸ºSHç³»æ•°ï¼ŒYä¸ºSHåŸºã€‚fä¸ºä¸€ç»„SHç³»æ•°ï¼Œdä¸ºè§†è§’ã€‚</p>
<h4 id="ç»‘å®šä¸å½¢å˜"><a class="header" href="#ç»‘å®šä¸å½¢å˜">ç»‘å®šä¸å½¢å˜</a></h4>
<p>å¯¹ç”¨å…«å‰æ ‘æè¿°çš„ä½“ç´ è¿›è¡Œç»‘å®šï¼ˆæ”¾ç½®ç›®æ ‡éª¨æ¶Sï¼‰å’Œè’™çš®ï¼ˆå®šä¹‰ä½“ç´ ä¸éª¨éª¼çš„å¸¦åŠ¨å…³ç³»ï¼‰ã€‚[Huang et al. 2020]</p>
<p>ç»‘å®šï¼šå¤ç”¨CGIçš„éª¨éª¼<br />
è’™çš®ï¼šä½¿ç”¨ CGI æ¨¡å‹æä¾›çš„è’™çš®ç½‘æ ¼å°†è’™çš®æƒé‡åº”ç”¨äºä½“ç´ ã€‚å³ï¼Œé€šè¿‡æ··åˆæœ€æ¥è¿‘çš„é¡¶ç‚¹çš„æƒé‡ç”Ÿæˆæ¯ä½“ç´ è’™çš®æƒé‡ã€‚<br />
é©±åŠ¨ï¼šLBS</p>
<pre><code class="language-python">def generate_transformation_matrices(matrices, skinning_weights, joint_index):
    return svox_t.blend_transformation_matrix(matrices, skinning_weights, joint_index)
</code></pre>
<h4 id="åŠ¨æ€ä½“ç§¯æ¸²æŸ“"><a class="header" href="#åŠ¨æ€ä½“ç§¯æ¸²æŸ“">åŠ¨æ€ä½“ç§¯æ¸²æŸ“</a></h4>
<p><img src="./assets/bea83903733e7c2e0013c6f112c41aa9_4_Figure_4.png" alt="" /></p>
<ol>
<li>ä½¿ç”¨LBSé©±åŠ¨Oct Tree</li>
<li>é€šè¿‡ray marchingå¾—åˆ°æ¯ä¸ªç‚¹çš„ç‰¹å¾</li>
<li>æŠŠç‰¹å¾ç»„åˆæˆFeature F</li>
</ol>
<pre><code class="language-python">class TreeRenderer(nn.Module):
    def forward(self, tree, rays, transformation_matrices=None, fast_rendering=False):
        t = tree.to(rays.device)

        r = svox_t.VolumeRenderer(t, background_brightness=self.background_brightness, step_size=self.step_size)
        dirs = rays[..., :3].contiguous()
        origins = rays[..., 3:].contiguous()

        sh_rays = Sh_Rays(origins, dirs, dirs)

        # é€šè¿‡å°†æ–¹ç¨‹ 3.1 åº”ç”¨äºæ¯ä¸ªåƒç´ æ¥ç”Ÿæˆè§†è§’ç›¸å…³çš„ç‰¹å¾å›¾ Fï¼Œå¹¶é€šè¿‡æ²¿å…‰çº¿ç´¯ç§¯æ¥ç”Ÿæˆç²—ç•¥çš„ä¸é€æ˜åº¦å›¾ A
        res = r(self.features, sh_rays, transformation_matrices=transformation_matrices, fast=fast_rendering)

        return res
</code></pre>
<h4 id="neural-shading"><a class="header" href="#neural-shading">Neural Shading</a></h4>
<p>è¾“å…¥ï¼šè§†ç‚¹å¤„çš„ä½“ç§¯å…‰æ …åŒ–ä¸ºç¥ç»å¤–è§‚ç‰¹å¾å›¾ F å’Œä¸é€æ˜åº¦ Aã€‚<br />
è¾“å‡ºï¼šå°†å…‰æ …åŒ–ä½“ç§¯è½¬æ¢ä¸ºå…·æœ‰ç±»ä¼¼ç»å…¸ç€è‰²å™¨çš„ç›¸åº”ä¸é€æ˜åº¦è´´å›¾çš„å½©è‰²å›¾åƒã€‚</p>
<ul>
<li>è¦è§£å†³çš„é—®é¢˜ï¼š<br />
ä¸ºäº†ä¿ç•™æ¯›å‘çš„é«˜é¢‘ç»†èŠ‚ï¼Œå¿…é¡»è€ƒè™‘æœ€ç»ˆæ¸²æŸ“å›¾åƒä¸­çš„ç©ºé—´å†…å®¹ã€‚ä½†NeRF å’Œ PlenOctree éƒ½æ²¡æœ‰è€ƒè™‘ç©ºé—´ç›¸å…³æ€§ï¼Œå› ä¸ºæ‰€æœ‰åƒç´ éƒ½æ˜¯ç‹¬ç«‹æ¸²æŸ“çš„ã€‚</li>
<li>è§£å†³æ–¹æ³•ï¼š<br />
åœ¨ é‡‡ç”¨é¢å¤–çš„ U-Net æ¶æ„è¿›è¡Œå›¾åƒæ¸²æŸ“(å€Ÿé‰´ConvNeRF)ã€‚<br />
ä¼˜ç‚¹ï¼šåŸºäºray marchingçš„é‡‡æ ·ç­–ç•¥å¯ä»¥å®ç°å…¨å›¾åƒæ¸²æŸ“ã€‚</li>
</ul>
<pre><code class="language-python">class UNet(nn.Module):
    def forward(self, rgb_feature, alpha_feature):
        # ç¥ç»ç€è‰²ç½‘ç»œU-NetåŒ…å«ä¸¤ä¸ªencoder-decoderåˆ†æ”¯ï¼Œåˆ†åˆ«ç”¨äº RGB å’Œ alpha é€šé“ã€‚ 
        # RGB åˆ†æ”¯å°† F è½¬æ¢ä¸ºå…·æœ‰ä¸°å¯Œæ¯›å‘ç»†èŠ‚çš„çº¹ç†å›¾åƒ Iğ‘“ã€‚
        x1 = self.inc(rgb_feature)
        x2 = self.down1(x1)
        x3 = self.down2(x2)
        x4 = self.down3(x3)
        x5 = self.down4(x4)

        x6 = self.up1(x5, x4)
        x6 = self.up2(x6, x3)
        x6 = self.up3(x6, x2)
        x6 = self.up4(x6, x1)
        x_rgb = self.outc(x6)

        # alpha åˆ†æ”¯ç»†åŒ–ç²—ç•¥ä¸é€æ˜åº¦å›¾ A å’Œ Iğ‘“ ä»¥å½¢æˆè¶…åˆ†è¾¨ç‡ä¸é€æ˜åº¦å›¾ Aã€‚è¯¥è¿‡ç¨‹é€šè¿‡æ˜¾å¼åˆ©ç”¨ A ä¸­ç¼–ç çš„éšå¼å‡ ä½•ä¿¡æ¯æ¥å¼ºåˆ¶å¤šè§†è§’ä¸€è‡´æ€§ã€‚
        x = torch.cat([alpha_feature, x_rgb], dim=1)
        x1_2 = self.inc2(x)
        x2_2 = self.down5(x1_2)
        x3_2 = self.down6(x2_2)

        x6 = self.up5(x3_2, torch.cat([x2, x2_2], dim=1))
        x6 = self.up6(x6, torch.cat([x1, x1_2], dim=1))
        x_alpha = self.outc2(x6)

        x = torch.cat([x_rgb, x_alpha], dim=1)
        return x
</code></pre>
<h4 id="å®Œæ•´ä»£ç æµç¨‹"><a class="header" href="#å®Œæ•´ä»£ç æµç¨‹">å®Œæ•´ä»£ç æµç¨‹</a></h4>
<pre><code class="language-python">&quot;&quot;&quot;
modelï¼šNerfæ¨¡å‹
K, Tï¼šç›¸æœºå‚æ•°
treeï¼šOctree
matricesï¼šåŠ¨ä½œå‚æ•°
joint_features: B * T * dimï¼ŒåŒ…å«skeleton_init, pose_init, poseç­‰ä¿¡æ¯
bg: backgroud
&quot;&quot;&quot;
def render_image(cfg, model, K, T, img_size=(450, 800), tree=None, matrices=None, joint_features=None,
                 bg=None, skinning_weights=None, joint_index=None):
    torch.cuda.synchronize()
    s = time.time()
    h, w = img_size[0], img_size[1]
    # Sample rays from views (and images) with/without masks
    rays, _, _ = ray_sampling(K.unsqueeze(0).cuda(), T.unsqueeze(0).cuda(), img_size)

    with torch.no_grad():
        joint_features = None if not cfg.MODEL.USE_MOTION else joint_features
        # è®¡ç®—skinning matrix
        matrices = generate_transformation_matrices(matrices=matrices, skinning_weights=skinning_weights,
                                                    joint_index=joint_index)

        with torch.cuda.amp.autocast(enabled=False):
            # 1. ä½¿ç”¨LBSé©±åŠ¨Oct Tree 
            # 2. é€šè¿‡ray marchingå¾—åˆ°æ¯ä¸ªç‚¹çš„ç‰¹å¾ 
            # 3. æŠŠç‰¹å¾ç»„åˆæˆFeature F
            features = model.tree_renderer(tree, rays, matrices).reshape(1, h, w, -1).permute(0, 3, 1, 2)

        # è¿™ä¸€æ­¥æ²¡æ³¨æ„åˆ°è®ºæ–‡çš„ç›¸å…³å†…å®¹
        if cfg.MODEL.USE_MOTION:
            motion_feature = model.tree_renderer.motion_feature_render(tree, joint_features, skinning_weights,
                                                                       joint_index,
                                                                       rays)


            motion_feature = motion_feature.reshape(1, h, w, -1).permute(0, 3, 1, 2)
        else:
            motion_feature = features[:, :9, ...]

        with torch.cuda.amp.autocast(enabled=True):
            features_in = features[:, :-1, ...]
            if cfg.MODEL.USE_MOTION:
                features_in = torch.cat([features[:, :-1, ...], motion_feature], dim=1)
            # è¾“å…¥ï¼šè§†ç‚¹å¤„çš„ä½“ç§¯å…‰æ …åŒ–ä¸ºç¥ç»å¤–è§‚ç‰¹å¾å›¾ F å’Œä¸é€æ˜åº¦ Aã€‚  
            # è¾“å‡ºï¼šrgb, alphaã€‚
            rgba_out = model.render_net(features_in, features[:, -1:, ...])
            
        rgba_volume = torch.cat([features[:, :3, ...], features[:, -1:, ...]], dim=1)

        rgb = rgba_out[0, :-1, ...]
        alpha = rgba_out[0, -1:, ...]
        img_volume = rgba_volume[0, :3, ...].permute(1, 2, 0)

        # æŠŠé¢„æµ‹å‡ºçš„rgbå’Œalphaå½’ä¸€åŒ–åˆ°[0,1]åŒºé—´
        if model.use_render_net:
            rgb = torch.nn.Hardtanh()(rgb)
            rgb = (rgb + 1) / 2

            alpha = torch.nn.Hardtanh()(alpha)
            alpha = (alpha + 1) / 2
            alpha = torch.clamp(alpha, min=0, max=1.)

    # ä¸èƒŒå½±èåˆ
    if bg is not None:
        if bg.max() &gt; 1:
            bg = bg / 255
        comp_img = rgb * alpha + (1 - alpha) * bg
    else:
        comp_img = rgb * alpha + (1 - alpha)

    img_unet = comp_img.permute(1, 2, 0).float().cpu().numpy()

    return img_unet, alpha.squeeze().float().detach().cpu().numpy(), img_volume.float().detach().cpu().numpy()
</code></pre>
<h3 id="åŸºäºç¥ç»ç½‘ç»œçš„åŠ¨ç‰©è¿åŠ¨åˆæˆ"><a class="header" href="#åŸºäºç¥ç»ç½‘ç»œçš„åŠ¨ç‰©è¿åŠ¨åˆæˆ">åŸºäºç¥ç»ç½‘ç»œçš„åŠ¨ç‰©è¿åŠ¨åˆæˆ</a></h3>
<h4 id="åŠ¨ç‰©åŠ¨ä½œæ•æ‰"><a class="header" href="#åŠ¨ç‰©åŠ¨ä½œæ•æ‰">åŠ¨ç‰©åŠ¨ä½œæ•æ‰</a></h4>
<p>åŠ¨ä½œæ•æ‰ï¼šå°½ç®¡ä¸åŒç‰©ç§çš„å››è¶³åŠ¨ç‰©å…·æœ‰ç›¸ä¼¼çš„éª¨éª¼ç»“æ„ï¼Œä½†å…¶å½¢çŠ¶å’Œå°ºåº¦å´æˆªç„¶ä¸åŒã€‚**æ•è·é€‚åˆæ‰€æœ‰ç±»å‹å››è¶³åŠ¨ç‰©çš„åŠ¨ä½œæ•æ‰æ•°æ®é›†æ˜¯æ ¹æœ¬ä¸å¯èƒ½çš„ã€‚**å› æ­¤ï¼Œå…ˆå­¦ä¹ æ¸©é¡ºçš„å°å‹å® ç‰©çš„è¿åŠ¨å…ˆéªŒï¼Œå¹¶å°†å…ˆéªŒè½¬ç§»åˆ°è€è™å’Œç‹¼ç­‰å¤§å‹åŠ¨ç‰©èº«ä¸Šã€‚å¯¹äºåè€…ï¼Œä½¿ç”¨å¤šè§†å›¾ RGB çƒé¡¶è¿›ä¸€æ­¥æé«˜äº†é¢„æµ‹ç²¾åº¦ã€‚</p>
<p>åŠ¨ç‰©å§¿åŠ¿ä¼°è®¡ï¼šé‡‡ç”¨å‚æ•°åŒ– SMAL åŠ¨ç‰©å§¿åŠ¿æ¨¡å‹ã€‚ä»è§‚å¯Ÿåˆ°çš„ 2D å…³èŠ‚å’Œè½®å»“ä¸­æ¢å¤ SMAL å‚æ•° ğœƒã€ğœ™ã€ğ›¾ã€‚</p>
<h4 id="motion-synthesis"><a class="header" href="#motion-synthesis">Motion Synthesis</a></h4>
<h2 id="è®­ç»ƒä¸éªŒè¯"><a class="header" href="#è®­ç»ƒä¸éªŒè¯">è®­ç»ƒä¸éªŒè¯</a></h2>
<p>ä¼˜åŒ–å¯¹è±¡ï¼šfeature array, å‚æ•°G<br />
ä¼˜åŒ–ç›®æ ‡ï¼šå„è§†è§’ä¸‹çš„å¤–è§‚</p>
<h3 id="æ•°æ®é›†"><a class="header" href="#æ•°æ®é›†">æ•°æ®é›†</a></h3>
<p>åŠ¨æ€æ¯›èŒ¸èŒ¸åŠ¨ç‰©ï¼ˆDFAï¼‰æ•°æ®é›†ï¼š</p>
<ul>
<li>æ¥è‡ªè‰ºæœ¯å®¶çš„å»ºæ¨¡ã€‚</li>
<li>å«ä¹ç§é«˜è´¨é‡çš„ CGI åŠ¨ç‰©ï¼ŒåŒ…æ‹¬ç†ŠçŒ«ã€ç‹®å­ã€çŒ«ç­‰ã€‚</li>
<li>å®ƒä»¬å…·æœ‰åŸºäºçº¤ç»´/çº¿çš„æ¯›çš®å’Œéª¨éª¼</li>
<li>ä½¿ç”¨å•†ä¸šæ¸²æŸ“å¼•æ“ï¼ˆä¾‹å¦‚ MAYAï¼‰å°†æ‰€æœ‰è¿™äº› CGI åŠ¨ç‰©è§’è‰²æ¸²æŸ“æˆå„ç§ä»£è¡¨æ€§éª¨éª¼è¿åŠ¨ä¸‹çš„é«˜è´¨é‡å¤šè§†å›¾ 1080 Ã— 1080 RGBA è§†é¢‘ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº† 36 ä¸ªæ‘„åƒæœºè§†å›¾ï¼Œè¿™äº›æ‘„åƒæœºè§†å›¾å‡åŒ€åœ°å›´ç»•æ•è·çš„åŠ¨ç‰©æ’åˆ—æˆä¸€ä¸ªåœ†åœˆï¼Œæ¯ä¸ªåŠ¨ç‰©çš„ä»£è¡¨æ€§å§¿åŠ¿æ•°é‡ä» 700 åˆ° 1000 ä¸ªä¸ç­‰ã€‚</li>
</ul>
<h3 id="loss"><a class="header" href="#loss">loss</a></h3>
<table><thead><tr><th>loss</th><th>content</th></tr></thead><tbody>
<tr><td>ğ‘Ÿğ‘”ğ‘ğ‘<br>åœ¨è‡ªç”±è§†è§’ä¸‹æ¢å¤æ¯›èŒ¸èŒ¸åŠ¨ç‰©çš„å¤–è§‚å’Œä¸é€æ˜åº¦å€¼</td><td>æ¸²æŸ“å›¾åƒä¸åŸå§‹å›¾åƒçš„L1 Lossï¼Œæ¸²æŸ“alphaä¸çœŸå®alphaçš„L1 Loss</td></tr>
<tr><td>P<br>é¼“åŠ±äº¤å‰è§†å›¾ä¸€è‡´æ€§å¹¶ä¿æŒæ—¶é—´ä¸€è‡´æ€§</td><td>æ¸²æŸ“å›¾åƒçš„VGG lå±‚feature mapä¸çœŸå®å›¾åƒçš„VGG lå±‚feature map</td></tr>
<tr><td>A<br>é¼“åŠ±è·¨è§†å›¾ä¸€è‡´æ€§</td><td>å‡ ä½•ç‰¹å¾çš„L1 Loss</td></tr>
<tr><td>VRT<br>ä½“ç´ æ­£åˆ™åŒ–é¡¹ï¼ˆVRTï¼‰ï¼Œå¼ºåˆ¶deformåè½åœ¨åŒä¸€ç½‘æ ¼ä¸Šçš„ç‰¹å¾åº”å…·æœ‰ç›¸åŒçš„å€¼ï¼Œé¿å…ä½“ç´ ä¸Šçš„ç‰¹å¾å†²çª</td><td></td></tr>
<tr><td>GAN<br>è¿›ä¸€æ­¥æé«˜çš®æ¯›é«˜é¢‘å¤–è§‚çš„è§†è§‰è´¨é‡</td><td></td></tr>
</tbody></table>
<h3 id="è®­ç»ƒç­–ç•¥"><a class="header" href="#è®­ç»ƒç­–ç•¥">è®­ç»ƒç­–ç•¥</a></h3>
<h2 id="æœ‰æ•ˆ"><a class="header" href="#æœ‰æ•ˆ">æœ‰æ•ˆ</a></h2>
<ol start="3">
<li><strong>å®æ—¶æ¸²æŸ“ï¼ˆReal-time Renderingï¼‰</strong>ï¼šArtemisèƒ½å¤Ÿå®ç°å¯¹åŠ¨ç‰©æ¨¡å‹çš„å®æ—¶ã€é€¼çœŸæ¸²æŸ“ï¼Œè¿™å¯¹äºè™šæ‹Ÿç°å®ï¼ˆVRï¼‰ç­‰äº¤äº’å¼åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li><strong>ç³»ç»Ÿæ€§èƒ½å’Œåº”ç”¨ï¼ˆSystem Performance and Applicationsï¼‰</strong>ï¼šArtemisåœ¨å¤šè§†è§’ã€å¤šç¯å¢ƒæ¡ä»¶ä¸‹å‡å±•ç°å‡ºé«˜æ•ˆå’Œå®ç”¨çš„æ€§èƒ½ï¼Œè®ºæ–‡è¿˜è®¨è®ºäº†å…¶åœ¨åŠ¨ç‰©æ•°å­—åŒ–å’Œä¿æŠ¤ã€VR/ARã€æ¸¸æˆå’Œå¨±ä¹ç­‰é¢†åŸŸçš„æ½œåœ¨åº”ç”¨ã€‚</li>
</ol>
<h2 id="å±€é™æ€§"><a class="header" href="#å±€é™æ€§">å±€é™æ€§</a></h2>
<ul>
<li>å¯¹é¢„å®šä¹‰éª¨æ¶çš„ä¾èµ–</li>
<li>æœªè§‚å¯Ÿåˆ°çš„åŠ¨ç‰©èº«ä½“åŒºåŸŸçš„å¤–è§‚ä¼ªå½±é—®é¢˜</li>
<li>åœ¨æ–°ç¯å¢ƒä¸­å¯¹å…‰çº¿å˜åŒ–çš„é€‚åº”æ€§é—®é¢˜</li>
</ul>
<h2 id="å¯å‘"><a class="header" href="#å¯å‘">å¯å‘</a></h2>
<p>åˆ©ç”¨CGIæ¸²æŸ“ç”Ÿæˆé«˜ç²¾çš„GTå’Œå®Œç¾åŒ¹é…çš„åŠ¨ä½œæ•°æ®ã€‚</p>
<h2 id="é—ç•™é—®é¢˜"><a class="header" href="#é—ç•™é—®é¢˜">é—ç•™é—®é¢˜</a></h2>
<h2 id="å‚è€ƒææ–™"><a class="header" href="#å‚è€ƒææ–™">å‚è€ƒææ–™</a></h2>
<ol>
<li>ARTEMIS æ¨¡å‹å’ŒåŠ¨æ€æ¯›èŒ¸èŒ¸åŠ¨ç‰©æ•°æ®é›† https://haiminluo.github.io/publication/artemis/</li>
</ol>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="33.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                            <a rel="next" href="31.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="33.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                    <a rel="next" href="31.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="theme/pagetoc.js"></script>
        <script type="text/javascript" src="theme/mermaid.min.js"></script>
        <script type="text/javascript" src="theme/mermaid-init.js"></script>
    </body>
</html>
